{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtDUMteVkAAY"
      },
      "source": [
        "# Voltage Imaging Analysis Benchmark\n",
        "\n",
        "## For Empirical Code Evaluation\n",
        "\n",
        "This notebook defines a benchmark for automated optimization of voltage imaging analysis pipelines, suitable for empirical code evaluation frameworks that iteratively search for high-performing code variants.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/voltage-imaging-benchmark/blob/main/voltage_imaging_benchmark.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epn8mViAkAAa"
      },
      "source": [
        "---\n",
        "# 0. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_5ek69vkAAa",
        "outputId": "c0d4611e-fc90-4d5b-9463-7c9123c25fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/83.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m234.5/234.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies (run this cell in Colab or if packages are missing)\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q numpy scipy matplotlib scikit-image scikit-learn pandas seaborn tifffile nd2 opencv-python gdown umap-learn hdbscan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scAWy9MkkAAb"
      },
      "source": [
        "## 0.2 Download Benchmark Data\n",
        "\n",
        "The benchmark data is hosted on Google Drive. Run the cell below to download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXWTXuXTkAAb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# === CONFIGURE YOUR GOOGLE DRIVE FILE ID HERE ===\n",
        "# To get the file ID from a Google Drive sharing link:\n",
        "# https://drive.google.com/file/d/FILE_ID_HERE/view?usp=sharing\n",
        "#                                  ^^^^^^^^^^^^ copy this part\n",
        "\n",
        "GDRIVE_FILE_ID = \"156ASrvQfbyjrHwtAeCCt_dTuWcktS9Cu\"  # Replace with your actual file ID\n",
        "DATA_DIR = Path(\"./data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def download_data(file_id, output_dir):\n",
        "    \"\"\"Download and extract benchmark data from Google Drive.\"\"\"\n",
        "    import gdown\n",
        "    import zipfile\n",
        "\n",
        "    zip_path = output_dir / \"benchmark_data.zip\"\n",
        "\n",
        "    # Check if data already exists\n",
        "    if (output_dir / \"video.tif\").exists() or (output_dir / \"fish1\").exists():\n",
        "        print(\"Data already downloaded. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Download from Google Drive\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    print(f\"Downloading data from Google Drive...\")\n",
        "    gdown.download(url, str(zip_path), quiet=False)\n",
        "\n",
        "    # Extract if it's a zip file\n",
        "    if zipfile.is_zipfile(zip_path):\n",
        "        print(\"Extracting data...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(output_dir)\n",
        "        zip_path.unlink()  # Remove zip after extraction\n",
        "        print(\"Done!\")\n",
        "    else:\n",
        "        # Might be a single TIFF file\n",
        "        print(\"Downloaded file (not a zip archive)\")\n",
        "\n",
        "# Uncomment to download:\n",
        "# download_data(GDRIVE_FILE_ID, DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzqWpRq0kAAc"
      },
      "source": [
        "## 0.3 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad-_5i6TkAAc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage, signal\n",
        "from skimage import measure, morphology\n",
        "import tifffile\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# Set plotting style\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Ind1D7kAAc"
      },
      "source": [
        "---\n",
        "# 1. Problem Definition\n",
        "\n",
        "## 1.1 Scientific Context\n",
        "\n",
        "**Voltage imaging** enables recording of neuronal electrical activity at high temporal resolution (~200 Hz) across many neurons simultaneously. Unlike calcium imaging, voltage indicators directly report membrane potential changes, capturing:\n",
        "- **Action potentials (spikes)** - fast (~1-5ms) electrical events\n",
        "- **Subthreshold activity** - slower membrane potential fluctuations\n",
        "\n",
        "## 1.2 The Data\n",
        "\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| **Organism** | Zebrafish (larval) |\n",
        "| **Indicator** | Voltron-2 (soma-targeted) |\n",
        "| **Frame rate** | ~200 Hz (5 ms/frame) |\n",
        "| **File format** | ND2 (Nikon) â†’ TIFF |\n",
        "| **File size** | 2-10 GB per recording |\n",
        "| **Duration** | 1,000-10,000 frames (5-50 seconds) |\n",
        "| **Neurons per FOV** | 10-100 |\n",
        "| **Neuron size** | ~5-10 Âµm (~9 pixels) |\n",
        "| **Neuron appearance** | Ring-like (membrane labeling) |\n",
        "\n",
        "## 1.3 Signal Characteristics\n",
        "\n",
        "**Important**: Voltron-2 produces **NEGATIVE deflections** during action potentials.\n",
        "\n",
        "| Feature | Typical Value |\n",
        "|---------|---------------|\n",
        "| Spike duration | 1-5 ms (1-2 frames at 200 Hz) |\n",
        "| Spike amplitude | 2-10% Î”F/F |\n",
        "| Spike polarity | **Negative** (downward) |\n",
        "| Noise type | Broadband (camera/shot noise) |\n",
        "| SNR | Variable, typically 2-10 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDdS22vkAAd"
      },
      "source": [
        "---\n",
        "# 2. Task Specification\n",
        "\n",
        "## 2.1 Overall Goal\n",
        "\n",
        "Given a raw voltage imaging video, produce:\n",
        "1. **ROI masks** - binary masks identifying each neuron\n",
        "2. **Spike times** - timestamps of detected action potentials for each neuron\n",
        "3. **Voltage traces** - cleaned Î”F/F time series for each neuron\n",
        "\n",
        "## 2.2 Pipeline Components (Optimization Targets)\n",
        "\n",
        "The analysis pipeline consists of sequential processing stages. Each stage can be independently optimized:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    OPTIMIZATION TARGETS                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  STAGE 1: MOTION CORRECTION                                         â”‚\n",
        "â”‚  â”œâ”€â”€ Method: rigid vs non-rigid                                    â”‚\n",
        "â”‚  â”œâ”€â”€ Reference: first frame, mean, median, specific frame          â”‚\n",
        "â”‚  â”œâ”€â”€ Algorithm: phase correlation, template matching, optical flow â”‚\n",
        "â”‚  â””â”€â”€ Parameters: max_shift, upsample_factor, smoothing             â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  STAGE 2: DENOISING                                                 â”‚\n",
        "â”‚  â”œâ”€â”€ Method: none, temporal filter, spatial filter, PCA, wavelet   â”‚\n",
        "â”‚  â”œâ”€â”€ Temporal: lowpass cutoff, Savitzky-Golay window, median       â”‚\n",
        "â”‚  â”œâ”€â”€ Spatial: Gaussian sigma, bilateral filter                     â”‚\n",
        "â”‚  â””â”€â”€ PCA: local vs global, number of components, patch size        â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  STAGE 3: ROI SEGMENTATION                                          â”‚\n",
        "â”‚  â”œâ”€â”€ Method: threshold, watershed, CNN (Mask R-CNN), NMF           â”‚\n",
        "â”‚  â”œâ”€â”€ Features: std projection, correlation image, PCA components   â”‚\n",
        "â”‚  â”œâ”€â”€ Constraints: min/max area, circularity, ring-like shape       â”‚\n",
        "â”‚  â””â”€â”€ Post-processing: merge overlapping, remove duplicates         â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  STAGE 4: TRACE EXTRACTION                                          â”‚\n",
        "â”‚  â”œâ”€â”€ Aggregation: mean, median, weighted by distance               â”‚\n",
        "â”‚  â”œâ”€â”€ Background: none, annulus subtraction, neuropil coefficient   â”‚\n",
        "â”‚  â”œâ”€â”€ Baseline: percentile (which?), rolling window size            â”‚\n",
        "â”‚  â””â”€â”€ Detrending: none, linear, polynomial, exponential             â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â”‚  STAGE 5: SPIKE DETECTION                                           â”‚\n",
        "â”‚  â”œâ”€â”€ Method: threshold, template matching, deconvolution, ML       â”‚\n",
        "â”‚  â”œâ”€â”€ Threshold: fixed std, adaptive, percentile-based              â”‚\n",
        "â”‚  â”œâ”€â”€ Constraints: min spike width, refractory period               â”‚\n",
        "â”‚  â””â”€â”€ Post-processing: amplitude filter, artifact rejection         â”‚\n",
        "â”‚                                                                     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## 2.3 Input Format\n",
        "\n",
        "```python\n",
        "# Input: 3D numpy array\n",
        "video: np.ndarray  # shape: (n_frames, height, width), dtype: uint16 or float32\n",
        "fps: float  # frame rate in Hz (typically 200)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6pfF4fkAAd"
      },
      "source": [
        "---\n",
        "# 3. Challenges and Failure Modes\n",
        "\n",
        "## 3.1 Motion Artifacts\n",
        "\n",
        "**Problem**: The animal moves during imaging, causing:\n",
        "- Rigid translation (x/y shifts)\n",
        "- Non-rigid deformation (tissue stretching)\n",
        "- Focus changes (neurons going in/out of focus)\n",
        "- ROI contamination (wrong pixels included after movement)\n",
        "\n",
        "**Failure modes**:\n",
        "- Correlated \"spikes\" across many neurons (motion artifact detected as spike)\n",
        "- Lost neurons (moved out of ROI)\n",
        "- False spikes from intensity changes due to focus shifts\n",
        "\n",
        "## 3.2 Segmentation Challenges\n",
        "\n",
        "**Problem**: Neurons have irregular shapes:\n",
        "- Ring-like appearance (soma-targeted membrane labeling)\n",
        "- Variable brightness\n",
        "- Overlapping neurons\n",
        "- Similar size to noise blobs\n",
        "\n",
        "**Failure modes**:\n",
        "- Merging adjacent neurons into one ROI\n",
        "- Splitting one neuron into multiple ROIs\n",
        "- Including non-neuronal structures\n",
        "- Missing dim neurons\n",
        "\n",
        "## 3.3 Spike Detection Challenges\n",
        "\n",
        "**Problem**: Spikes are fast and small:\n",
        "- Only 1-2 frames wide at 200 Hz\n",
        "- Low SNR (amplitude similar to noise)\n",
        "- Variable amplitude across neurons and within same neuron\n",
        "- Negative polarity (opposite to calcium indicators)\n",
        "\n",
        "**Failure modes**:\n",
        "- Missing true spikes (false negatives)\n",
        "- Detecting noise as spikes (false positives)\n",
        "- Correlated false detections from global artifacts\n",
        "\n",
        "## 3.4 Known Artifacts\n",
        "\n",
        "| Artifact | Cause | Detection Method |\n",
        "|----------|-------|------------------|\n",
        "| Photobleaching | Dye degradation | Slow intensity decay |\n",
        "| Focus drift | Mechanical drift | Blur changes over time |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C65VVw2kAAd"
      },
      "source": [
        "---\n",
        "# 4. Evaluation Metrics\n",
        "\n",
        "## 4.1 Ground Truth Comparison (Supervised)\n",
        "\n",
        "When expert annotations are available\n",
        "\n",
        "## 4.2 Automated Validation (Unsupervised)\n",
        "\n",
        "Metrics that don't require ground truth annotations:\n",
        "\n",
        "### 4.2.1 Shuffled Control Test\n",
        "\n",
        "**Principle**: Spike detection on temporally shuffled data should yield far fewer spikes than real data.\n",
        "\n",
        "**Interpretation**:\n",
        "- `spike_ratio >> 1`: Good, detecting real structure\n",
        "- `spike_ratio â‰ˆ 1`: Bad, just detecting noise\n",
        "\n",
        "### 4.2.2 Spike Template Consistency\n",
        "\n",
        "**Principle**: Spikes from the same neuron should have similar waveform shapes.\n",
        "\n",
        "**Interpretation**:\n",
        "- `mean_correlation > 0.8`: Good, consistent spike shapes\n",
        "- `mean_correlation < 0.5`: Bad, likely detecting noise\n",
        "\n",
        "### 4.2.3 Inter-Neuron Correlation (Artifact Detection)\n",
        "\n",
        "**Principle**: High correlation in spike timing across many neurons suggests artifacts.\n",
        "\n",
        "**Interpretation**:\n",
        "- `mean_correlation < 0.1`: Good, neurons fire independently\n",
        "- `mean_correlation > 0.3`: Bad, likely motion artifacts\n",
        "- `synchrony_events > 0`: Suspicious, check these timepoints\n",
        "\n",
        "### 4.2.4 ROI Quality Metrics\n",
        "\n",
        "if within expected range (5-10 Âµm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx94Y_ibkAAe"
      },
      "source": [
        "---\n",
        "# 6. Baseline Implementation\n",
        "\n",
        "A simple baseline pipeline for comparison:\n",
        "\n",
        "```python\n",
        "def baseline_pipeline(video, fps=200):\n",
        "    \"\"\"\n",
        "    Simple baseline voltage imaging analysis pipeline.\n",
        "    \n",
        "    Args:\n",
        "        video: np.ndarray, shape (n_frames, height, width)\n",
        "        fps: frame rate in Hz\n",
        "    \n",
        "    Returns:\n",
        "        dict with roi_masks, traces, spike_times, spike_frames\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy import ndimage, signal\n",
        "    from skimage import measure, morphology\n",
        "    \n",
        "    n_frames, height, width = video.shape\n",
        "    \n",
        "    # === STAGE 1: No motion correction (baseline) ===\n",
        "    corrected = video\n",
        "    \n",
        "    # === STAGE 2: Simple denoising ===\n",
        "    # Temporal Gaussian smoothing\n",
        "    denoised = ndimage.gaussian_filter1d(corrected.astype(float), sigma=1, axis=0)\n",
        "    \n",
        "    # === STAGE 3: ROI segmentation ===\n",
        "    # Use std projection to find active regions\n",
        "    std_proj = np.std(denoised, axis=0)\n",
        "    \n",
        "    # Threshold\n",
        "    threshold = np.mean(std_proj) + 1.5 * np.std(std_proj)\n",
        "    binary = std_proj > threshold\n",
        "    \n",
        "    # Clean up\n",
        "    binary = morphology.remove_small_objects(binary, min_size=30)\n",
        "    binary = morphology.remove_small_holes(binary, area_threshold=30)\n",
        "    \n",
        "    # Label connected components\n",
        "    labeled = measure.label(binary)\n",
        "    regions = measure.regionprops(labeled)\n",
        "    \n",
        "    # Filter by size\n",
        "    roi_masks = []\n",
        "    for region in regions:\n",
        "        if 30 < region.area < 500:  # Expected neuron size\n",
        "            mask = labeled == region.label\n",
        "            roi_masks.append(mask)\n",
        "    \n",
        "    roi_masks = np.array(roi_masks)\n",
        "    \n",
        "    # === STAGE 4: Trace extraction ===\n",
        "    traces = []\n",
        "    for mask in roi_masks:\n",
        "        trace = np.mean(denoised[:, mask], axis=1)\n",
        "        # Compute dF/F\n",
        "        f0 = np.percentile(trace, 10)\n",
        "        dff = (trace - f0) / f0\n",
        "        # Detrend\n",
        "        dff = signal.detrend(dff)\n",
        "        traces.append(dff)\n",
        "    \n",
        "    traces = np.array(traces)\n",
        "    \n",
        "    # === STAGE 5: Spike detection ===\n",
        "    spike_times = []\n",
        "    spike_frames = []\n",
        "    \n",
        "    for trace in traces:\n",
        "        # Invert (Voltron-2 has negative spikes)\n",
        "        inverted = -trace\n",
        "        \n",
        "        # Threshold at 3 std\n",
        "        threshold = np.mean(inverted) + 3 * np.std(inverted)\n",
        "        \n",
        "        # Find peaks\n",
        "        peaks, _ = signal.find_peaks(\n",
        "            inverted,\n",
        "            height=threshold,\n",
        "            distance=int(0.005 * fps)  # 5ms refractory\n",
        "        )\n",
        "        \n",
        "        spike_frames.append(peaks)\n",
        "        spike_times.append(peaks / fps)\n",
        "    \n",
        "    return {\n",
        "        'roi_masks': roi_masks,\n",
        "        'traces': traces,\n",
        "        'spike_times': spike_times,\n",
        "        'spike_frames': spike_frames\n",
        "    }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEXaEXPkAAf"
      },
      "source": [
        "---\n",
        "# 9. References and Prior Work\n",
        "\n",
        "## 9.1 Existing Tools\n",
        "\n",
        "| Tool | What it does | Strengths | Limitations |\n",
        "|------|--------------|-----------|-------------|\n",
        "| **VolPy** (CaImAn) | Full pipeline | Mask R-CNN segmentation, SpikePursuit | Complex setup |\n",
        "| **NoRMCorre** | Motion correction | Well-validated | Part of CaImAn |\n",
        "| **Suite2p** | Calcium imaging | Fast, scalable | Not optimized for voltage |\n",
        "\n",
        "## 9.2 Key Papers\n",
        "\n",
        "1. **VolPy** - Cai et al., 2021, PLOS Comp Bio\n",
        "   - Mask R-CNN for segmentation\n",
        "   - SpikePursuit for spike detection\n",
        "   - F1 > 90% on benchmark data\n",
        "\n",
        "2. **Voltage imaging pipeline** - (First paper you shared)\n",
        "   - Camera noise correction\n",
        "   - Local PCA denoising\n",
        "   - Semi-NMF segmentation\n",
        "   - LSTM spike detection\n",
        "\n",
        "3. **Whole-brain voltage imaging** - (Second paper - Positron2)\n",
        "   - NoRMCorre motion correction\n",
        "   - UMAP+DBSCAN artifact removal\n",
        "   - SNR > 4 filtering\n",
        "\n",
        "## 9.3 Novel Ideas to Explore\n",
        "\n",
        "- Shuffled control validation\n",
        "- Template consistency scoring\n",
        "- Artifact correlation detection\n",
        "- Combined supervised + unsupervised evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCHYnmqvkAAf"
      },
      "source": [
        "---\n",
        "# 9.5 Implementation Ideas from related papers\n",
        "\n",
        "# 9.5.1 Voltron-2 analysis in small FOV paper\n",
        "\n",
        "This is from the paper\n",
        "Voltage imaging reveals circuit computations in the raphe underlying serotonin-mediated motor vigor learning\n",
        "Takashi Kawashima1,2,4 takashi.kawashima@weizmann.ac.il âˆ™ Ziqiang Wei1,4 weiz@janelia.hhmi.org âˆ™ Ravid Haruvi2 âˆ™ Inbal Shainer2,3 âˆ™ Sujatha Narayan1 âˆ™ Herwig Baier3 âˆ™ Misha B. Ahrens\n",
        "Imaging Voltron in zebrafish\n",
        "\n",
        "Processing pipeline for voltage imaging data\n",
        "We built a data processing pipeline for voltage imaging data to automatically perform camera-noise correction, motion correction, denoising, and cell segmentation (Figure 2B).\n",
        "Step #1: Camera-noise correction\n",
        "We adopted the camera noise correction algorithm from refs. Huang et al.90 and Liu et al.91 The pixel readout is computed as\n",
        "ğ‘ ğ‘–=(ğ‘ ğ‘–ğ‘Ÿâˆ’ğ‘œğ‘–)/ğ‘”ğ‘–\n",
        "where ğ‘ ğ‘–ğ‘Ÿ is the raw camera readout at pixel i, ğ‘ ğ‘– is the corrected one, ğ‘œğ‘– is the camera offset, and ğ‘”ğ‘– is the camera gain at pixel i. We estimated the offset ğ‘œğ‘– and baseline variance ğ‘£ğ‘– in the no-light condition as the mean and variance of 60k images. The gain for each pixel was calculated as\n",
        "ğ‘”ğ‘–=ğ‘ğ‘Ÿğ‘”â¡\n",
        "ğ‘šğ‘–ğ‘›\n",
        "ğ‘”\n",
        " â¡âˆ‘ğ¾\n",
        "ğ‘˜=1((ğ‘£ğ‘–ğ‘˜âˆ’ğ‘£ğ‘–)âˆ’ğ‘”â¢(ğ·ğ‘–ğ‘˜âˆ’ğ‘œğ‘–))2,\n",
        "where K is the total number of illumination levels acquired, k is the kth illumination level, ğ·ğ‘–ğ‘˜ and ğ‘£ğ‘–ğ‘˜ are the mean and variance of the 20k images acquired in the kth illumination level. In our setup, we measured them by varying the laser power from 0 mW (no light condition) to 18 mW. Empirically, we found that the camera-noise correction can improve motion correction and other later steps in the processing.91,92\n",
        "Step #2: Motion correction\n",
        "We performed the two-dimensional rigid registration of the images using a custom Python script based on the dipy package,93 to correct drifts in the sequentially recorded images at the subpixel level.\n",
        "Step #3: image-series denoising\n",
        "We performed image-series denoising for motion-corrected video Y by finding its low-rank (at K) representation\n",
        "Ì…\n",
        "ğ‘Œğ‘˜\n",
        "=âˆ‘ğ¾\n",
        "ğ‘˜=1ğ‘¢ğ‘˜â¢ğ‘£ğ‘˜\n",
        "with residual ğ‘…ğ‘˜=ğ‘Œâˆ’\n",
        "Ì…\n",
        "ğ‘Œğ‘˜\n",
        " is statistically white (within 99% confidence interval). Here, ğ‘¢ğ‘˜, ğ‘£ğ‘˜ are spatial and temporal components respectively and determined by the objective\n",
        "ğ‘¢ğ‘˜,ğ‘£ğ‘˜=ğ‘ğ‘Ÿğ‘”â¡\n",
        "ğ‘šğ‘–ğ‘›\n",
        "ğ‘¢,ğ‘£\n",
        " â¡â€–ğ‘…ğ‘˜âˆ’1â¢âˆ’ğ‘¢â¢ğ‘£â€–ğ¹2\n",
        "where F denotes the Frobenius norm. This is equivalent to performing iterative principal components analysis (PCA) on Y and stopping the iteration at the kth component as the residual is close to the white noise. Since the number of pixels is large, which creates difficulty in computing PCA, we alternatively divided the whole image into overlapped small patches (namely local PCA), performed this denoising procedure for each patch, and stitched the denoised patches back to a full denoised movie. Moreover, since video Y can be presented as a 3D tensor, we also tested the tensor-based optimization as\n",
        "Ì…\n",
        "ğ‘Œğ‘˜\n",
        "=âˆ‘ğ¾\n",
        "ğ‘˜=1ğ‘¢1,ğ‘˜âˆ˜ğ‘¢2,ğ‘˜âˆ˜ğ‘£ğ‘˜\n",
        "where ğ‘¢1,ğ‘˜ and ğ‘¢2,ğ‘˜ are two spatial components along the vertical and horizontal directions of the imaging, respectively. We found that tensor decomposition would generate stripe-like spatial correlation horizontally (which might stem from the power fluctuation of the horizontal light-sheet illumination), and thus used local PCA instead throughout the denoising step.\n",
        "Step #4: Cell segmentation\n",
        "We performed cell segmentation on the denoised movie (\n",
        "Ì…\n",
        "ğ‘Œğ‘˜\n",
        " ) using semi-nonnegative factorization, where\n",
        "ğ´,ğ¹,ğµ=ğ‘ğ‘Ÿğ‘”â¡\n",
        "ğ‘šğ‘–ğ‘›\n",
        "ğ´,ğ¹,ğµ\n",
        " â¡â€–\n",
        "Ì…\n",
        "ğ‘Œğ‘˜\n",
        "â¢âˆ’ğ´â¢ğ¹âˆ’ğµâ€–ğ¹2\n",
        "such that\n",
        "ğ´â‰¥0,ğµ=ğ‘Â·1ğ‘‡,ğ‘â‰¥0\n",
        "where A is the matrix of the nonnegative components in which the pixels were connected in space (ROIs); F is the matrix of the temporal components and presents the fluorescent dynamics of ROIs, and B is the temporally constant background. We initialized the components using super-pixels,94 which include the local pixels with neighboring correlations larger than 0.8. We then computed\n",
        "Î”â¢ğ¹/ğ¹=\n",
        "ğ¹âˆ’ğ¹0\n",
        "ğ¹0\n",
        "\n",
        "for each component (or ROI), where baseline fluorescence ğ¹0 is computed as a running 20% percentile of F within a 3-minute time window. In our hands, the denoising step does not affect the dynamics of spike generation (thus spike detection) or membrane potential fluctuation, compared to the computation of Î”F/F using hand-drawn ROIs. Since Voltron is a negative response indicator, we used the flipped signal (âˆ’Î”F/F) for subsequent analyses for spike detection and subthreshold activity estimation.\n",
        "Step #5: Spike detection\n",
        "We trained a machine-learning neural network (Figure 2C) in order to automatically extract the spikes from the time series data of Î”F/F. The network consists of two LSTM layers and a dropout layer that interconnects them. To train this neural network, we used simultaneous electrophysiology and imaging of cerebellar neurons expressing Voltron acquired in our previous work.26 The goal of the network is to determine whether there is a spike event in the middle of the time series (the length of the time series is 41 frames or 136.67 ms) by minimizing the loss function of the cross-entropy between the prediction of spike from voltage imaging with the ground truth from the electrophysiology (Figure 2C; gray dots, ground truth from electrophysiology; black, prediction of the network). We performed the 10-fold cross-validation in network training. We applied the trained network model to extract the spikes automatically thereafter.\n",
        "Step #6: Subthreshold activity estimation\n",
        "We estimated the subthreshold activity as a rolling median filter of time series data of Î”F/F with a window size of 70 ms. To avoid the nonlinearity of spikes (a depolarization followed by a repolarization), we clipped out the frames (from -1 to +1) around detected spikes while running the median filter.\n",
        "\n",
        "\n",
        "Kernel fits of membrane potential and spikes to behavioral variables\n",
        "Response kernel estimation of serotonergic neurons based on behavioral variables (Figure 3E) was performed as follows. We predicted spike events and subthreshold membrane potentials from the behavioral variables of the time series of the swim vigor\n",
        "ğ‘†ğ‘¡={ğ‘ ğ‘¡,ğ‘ ğ‘¡âˆ’1,ğ‘ ğ‘¡âˆ’2,â‹¯,ğ‘ ğ‘¡âˆ’ğœ}\n",
        "and that of the visual input\n",
        "ğ‘‰ğ‘¡={ğ‘£ğ‘¡,ğ‘£ğ‘¡âˆ’1,ğ‘£ğ‘¡âˆ’2,â‹¯,ğ‘£ğ‘¡âˆ’ğœ}\n",
        "and that of the recent spike events ğ‘†â¢ğ‘ƒğ‘¡={ğ‘ â¢ğ‘ğ‘¡,ğ‘ â¢ğ‘ğ‘¡âˆ’1,ğ‘ â¢ğ‘ğ‘¡âˆ’2,â‹¯,ğ‘ â¢ğ‘ğ‘¡âˆ’ğœ}, where ğ‘ â¢ğ‘ is 0 or 1,at a given time t, allowing history dependence (t- Ï„) up to 300 time points (1 second). Since spiking activity is sampled at 300 Hz, for each single frame, the spike event is binary, we thus modeled the spike event as\n",
        "ğ‘ƒâ¡(ğ‘ â¢ğ‘â¢ğ‘–â¢ğ‘˜â¢ğ‘’â¢ğ‘ ğ‘â¢ğ‘¡ğ‘¡â¢ğ‘–â¢ğ‘šâ¢ğ‘’ğ‘¡)=ğµâ¢ğ‘–â¢ğ‘›â¢ğ‘œâ¢ğ‘šâ¢ğ‘–â¢ğ‘â¢ğ‘™â¢(ğ‘¤ğ‘ ğ‘‡â¢ğ‘†ğ‘¡+ğ‘¤ğ‘£ğ‘‡â¢ğ‘‰ğ‘¡âˆ’ğ‘¤ğ‘ â¢ğ‘ğ‘‡â¢ğ‘†â¢ğ‘ƒğ‘¡).\n",
        "We optimized this probabilistic model from the data using a generalized linear model with logit link function and a ğ‘™2-form regularization on kernels of the swim vigor, i.e. ğ‘¤ğ‘ , the visual input, i.e. ğ‘¤ğ‘£, and the spike history, i.e. ğ‘¤ğ‘ â¢ğ‘. The model was fitted in Python using ğ‘™2 logistic regression in the Scikit-learn package95 with the following steps for data collection and cross-validation. The time series data was sampled randomly around spike events, with a balance of ğ‘ƒâ¡(ğ‘ â¢ğ‘â¢ğ‘–â¢ğ‘˜â¢ğ‘’â¢ğ‘ ğ‘â¢ğ‘¡ğ‘¡â¢ğ‘–â¢ğ‘šâ¢ğ‘’ğ‘¡)â‰ƒ0.5. We determined the strength of ğ‘™2-form regularization using 5-fold cross-validation. In each fold, we held 20% of the data for validation (completely non-overlapped with the rest of the data in the original full time series) and used the remaining 80% of the data for training. Moreover, we used the square root form of the swim vigor data in fitting. We used a model without regularization on validation data as the full model and determined the performance, using negative log likelihood (NLL), as\n",
        "1âˆ’\n",
        "ğ‘â¢ğ¿â¡ğ¿â¡(ğ‘ƒğ‘’â¢ğ‘ â¢ğ‘¡â¡(ğ‘ â¢ğ‘â¢ğ‘–â¢ğ‘˜â¢ğ‘’â¢ğ‘ ))\n",
        "ğ‘â¢ğ¿â¡ğ¿â¡(ğ‘ƒğ‘¡â¢ğ‘Ÿâ¢ğ‘¢â¢ğ‘’â¡(ğ‘ â¢ğ‘â¢ğ‘–â¢ğ‘˜â¢ğ‘’â¢ğ‘ ))\n",
        "\n",
        ".\n",
        "This is equivalent to a measure of the explained variance in a logistic regression.\n",
        "Furthermore, we assumed the change of subthreshold membrane potentials linearly as\n",
        "Î”â¢ğ¹â¡/â¢ğ¹â¡(ğ‘¡)âˆ’Î”â¢ğ¹â¡/â¢ğ¹â¡(ğ‘¡âˆ’ğœ)=ğ‘˜â¢(ğ‘¤ğ‘ ğ‘‡â¢ğ‘†ğ‘¡+ğ‘¤ğ‘£ğ‘‡â¢ğ‘‰ğ‘¡âˆ’ğ‘¤ğ‘ â¢ğ‘ğ‘‡â¢ğ‘†â¢ğ‘ƒğ‘¡)+ğ‘,\n",
        "and thus the model error is\n",
        "ğ‘’â¡(Î”â¢ğ¹â¡/â¢ğ¹â¡(ğ‘¡))=Î”â¢ğ¹â¡/â¢ğ¹â¡(ğ‘¡)âˆ’Î”â¢ğ¹â¡/â¢ğ¹â¡(ğ‘¡âˆ’ğœ)âˆ’ğ‘˜â¢(ğ‘¤ğ‘ ğ‘‡â¢ğ‘†ğ‘¡+ğ‘¤ğ‘£ğ‘‡â¢ğ‘‰ğ‘¡âˆ’ğ‘¤ğ‘ â¢ğ‘ğ‘‡â¢ğ‘†â¢ğ‘ƒğ‘¡)+ğ‘,\n",
        "where k and c are two scalars in model fits.\n",
        "We then fit the combined model of spike events and the change of subthreshold membrane potential as\n",
        "ğ‘¤ğ‘ ,ğ‘¤ğ‘£,ğ‘¤ğ‘ â¢ğ‘=ğ‘ğ‘Ÿğ‘”â¡\n",
        "ğ‘šğ‘–ğ‘›\n",
        "ğ‘¤ğ‘ ,ğ‘¤ğ‘£,ğ‘¤ğ‘ â¢ğ‘\n",
        " â¡ğ‘â¢ğ¿â¡ğ¿â¡(ğ‘ƒâ¡(ğ‘ â¢ğ‘â¢ğ‘–â¢ğ‘˜â¢ğ‘’â¢ğ‘ ))+\n",
        "1\n",
        "2\n",
        "\n",
        "â¢||ğ‘’â¢(Î”â¢ğ¹/ğ¹)|âˆ£22+\n",
        "1\n",
        "2\n",
        "\n",
        "â¢ğ›¼â¡(â€–ğ‘¤ğ‘ â€–22â¢+â€–â¢ğ‘¤ğ‘£â¢â€–22+â€–â¢ğ‘¤ğ‘ â¢ğ‘âˆ¥22)\n",
        "where ğ›¼ is ğ‘™2-form regularization of the kernels. We performed gradient descent-based minimization with the initialization of the parameters fitted from the spike-event-only model above.\n",
        "Cell selections\n",
        "Figure 3C included all segmented cells from gain adaptation tasks (Figure 1B) and memory tasks (Figure S1D; only using them in low and high Gms) and ablation tasks (Figure 6; only using those before ablation) (we also fitted them all with the behavioral variables in Figure 3E). In Figure 3C, we sorted neurons based on their center of mass of spiking rates after swimming onset and separated them into high- or low-Gms preferred categories.\n",
        "Neural population codes\n",
        "We computed the population coding of motor vigor learning by serotonergic neurons (Figures 6F and S8F) using a sparse version of linear discriminant analysis:96\n",
        "ğ‘™=ğ‘â¢ğ‘Ÿâ¢ğ‘”â¢\n",
        "ğ‘šğ‘–ğ‘›\n",
        "ğ‘™\n",
        " âˆ’\n",
        "(ğ‘™ğ‘‡â¢(ğ‘Ÿğ‘¡â„â¢ğ‘–â¢ğ‘”â¢â„âˆ’ğ‘Ÿğ‘¡ğ‘™â¢ğ‘œâ¢ğ‘¤))2\n",
        "ğ‘™ğ‘‡â¢ğ›´ğ‘Ÿâ¢ğ‘™\n",
        "\n",
        ".\n",
        "Here ğ‘Ÿğ‘¡ is ğ‘›-dim vector of the neural activity at time t; ğ‘› is the number of neurons in the population codes; ğ‘Ÿğ‘¡â„â¢ğ‘–â¢ğ‘”â¢â„ and ğ‘Ÿğ‘¡ğ‘™â¢ğ‘œâ¢ğ‘¤ are the sample average of ğ‘Ÿğ‘¡ at high or low Gms conditions, respectively; ğ‘™ is the coding direction for high Gms. We optimize the coding direction ğ‘™ with a ğ‘™2 regularizer, ğ›¾âˆˆ[0,1], which was applied to the covariance matrix of the sample data\n",
        "ğ›´ğ‘Ÿ=(1âˆ’ğ›¾)â¢(ğ‘Ÿğ‘¡âˆ’<ğ‘Ÿğ‘¡>)â¢(ğ‘Ÿğ‘¡âˆ’<ğ‘Ÿğ‘¡>)ğ‘‡+ğ›¾â¢ğ¼.\n",
        "We sampled the data from the late phase of each Gms period. The sample window for each data point to compute the firing rate is 1 second.\n",
        "For the analysis of the neural population codes for Gms in high-speed calcium imaging (Figure S5G), we used all identified neurons in each area (Figure S5H) and performed the following procedure. We first applied a Gaussian smoothing filter (Ïƒ = 3.3 ms) to all neural traces and then subtracted the baseline Î”F/F before the swimming (100 ms) from the Î”F/F values after the swimming in individual neurons for individual swim events. We then fitted a linear decoder by regressing the Î”F/F levels 500 ms after the swims, when the neural response sufficiently diverged, in all neurons to the ratio of Gms values to that at low Gms (1 = low, 2 = medium, 3 = high Gms) in individual swim events. The resulting linear decoder was then applied to the population activity (Î”F/F values of all neurons) at individual time points around the onset of swims, and the first time point that showed a significant difference of this population vector between low and high Gms conditions across swim events (p < 0.01) are designated as the response time.\n",
        "Spiking neural network model\n",
        "We simulated the neural network dynamics in the dorsal raphe nucleus (Figure 5) by using a spiking neural network model. Our model was equipped with 400 serotonergic neurons and 100 GABAergic neurons with sparse random connections with probability 0.03, i.e. ğ‘¤ğ‘—â¢ğ‘˜=1 if pc < 0.03 and otherwise ğ‘¤ğ‘—â¢ğ‘˜=0 (where pc is sampled from a uniform distribution from 0 to 1). The serotonergic neurons are modeled as adaptive integrate-and-fire neurons97:\n",
        "ğ¶ğ‘šâ¢ğ‘‘â¢ğ‘‰/ğ‘‘â¢ğ‘¡=âˆ’ğ‘”ğ¿â¢(ğ‘‰âˆ’ğ¸ğ¿)âˆ’ğ‘¢+ğ¼\n",
        "ğœğ‘¢â¢ğ‘‘â¢ğ‘¢/ğ‘‘â¢ğ‘¡=âˆ’ğ‘â¢(ğ‘‰âˆ’ğ¸ğ¿)âˆ’ğ‘¢\n",
        "V is the membrane potential; u is the adaptation variable; I is the input current; ğ¶ğ‘š=0.5 nF is the membrane capacitance; ğ‘”ğ¿=0.025 uS is the leak conductance; ğ¸ğ¿=âˆ’60 mV is the leak reversal potential; ğ‘=0.01 uS is the adaptation coupling parameter (presenting the factor of the excitatory rebound currents) and ğœğ‘¢=200 ms is the adaptation time constant for rebound currents). A spike happens when ğ‘‰>ğ‘‰ğ‘¡â¢â„=âˆ’40 mV, and V is then reset to -50 mV for a 2-ms refractory period; at the same time, a spike triggers an inhibitory afterhyperpolarization, which would add onto the adaptation variable,\n",
        "ğ‘¢â¡(ğ‘¡ğ‘ â¢ğ‘â¢ğ‘–â¢ğ‘˜â¢ğ‘’)=ğ‘¢+ğ‘,\n",
        "and b = 0.001 nA. GABAergic neurons are modeled as simple integrate-and-fire neurons:\n",
        "ğ¶ğ‘šâ¢ğ‘‘â¢ğ‘‰/ğ‘‘â¢ğ‘¡=âˆ’ğ‘”ğ¿â¢(ğ‘‰âˆ’ğ¸ğ¿)+ğ¼\n",
        "withğ¶ğ‘š=0.2nF;ğ‘”ğ¿=0.02ğœ‡â¢S;ğ¸ğ¿=âˆ’60mV.\n",
        "The synaptic inputs are modeled as\n",
        "ğ‘‘â¢ğ‘ /ğ‘‘â¢ğ‘¡=âˆ’\n",
        "1\n",
        "ğœğ‘ \n",
        "\n",
        "â¢ğ‘ +âˆ‘ğ‘–ğ›¿â¢(ğ‘¡âˆ’ğ‘¡ğ‘–)\n",
        "ğ¼ğ‘ â¢ğ‘¦â¢ğ‘›=ğ‘”ğ‘ â¢ğ‘¦â¢ğ‘›â¢ğ‘ â¢(ğ‘‰âˆ’ğ‘‰ğ‘ â¢ğ‘¦â¢ğ‘›)\n",
        "where ğ‘”ğ‘ â¢ğ‘¦â¢ğ‘› is a synaptic conductance, ğ‘‰ğ‘ â¢ğ‘¦â¢ğ‘› is the synaptic reversal potential, and s is a synaptic gating variable (that increases by one at a presynaptic spike event, then decays at a time constant ğœğ‘ ). We set ğœğ‘ =50 ms; ğ‘”5âˆ’ğ»â¢ğ‘‡=0.13 ns; ğ‘‰5âˆ’ğ»â¢ğ‘‡=0 mV; ğ‘”ğºâ¢ğ´â¢ğµâ¢ğ´=0.52 ns; ğ‘‰ğºâ¢ğ´â¢ğµâ¢ğ´=âˆ’70 mV.\n",
        "We provided glutamatic input into serotonergic and GABAergic neurons during the motor vigor learning task based on the results of neurotransmitter imaging. Both swim vigor and visual input were modeled as exponential decays from the behavioral variables' onset times:\n",
        "ğ¼ğµâ¡(ğ‘¡)=ğ¼ğµ0â¡ğ‘’ğ‘¥ğ‘â¡(âˆ’\n",
        "ğ‘¡âˆ’ğ‘¡ğµ\n",
        "ğœğµ\n",
        "\n",
        ")\n",
        "where the decay time constants were set to 50 ms, max input ğ¼ğµ0=100 pA for swim vigor, ğ¼ğµ0=50 pA for visual input. The total input to a kth serotonergic neuron was thus\n",
        "ğ¼ğ‘˜=âˆ‘ğ‘—âˆˆ5âˆ’ğ»â¢ğ‘‡ğ‘¤ğ‘—â¢ğ‘˜â¢ğ¼ğ‘—â¢ğ‘˜\n",
        "5âˆ’ğ»â¢ğ‘‡+âˆ‘ğ‘—âˆˆğºâ¢ğ´â¢ğµâ¢ğ´ğ‘¤ğ‘—â¢ğ‘˜â¢ğ¼ğ‘—â¢ğ‘˜\n",
        "ğºâ¢ğ´â¢ğµâ¢ğ´+ğ¼ğ‘£â¢ğ‘–â¢ğ‘ .\n",
        "The total input to a kth GABAergic neuron was thus\n",
        "ğ¼ğ‘˜=âˆ‘ğ‘—âˆˆ5âˆ’ğ»â¢ğ‘‡ğ‘¤ğ‘—â¢ğ‘˜â¢ğ¼ğ‘—â¢ğ‘˜\n",
        "5âˆ’ğ»â¢ğ‘‡+âˆ‘ğ‘—âˆˆğºâ¢ğ´â¢ğµâ¢ğ´ğ‘¤ğ‘—â¢ğ‘˜â¢ğ¼ğ‘—â¢ğ‘˜\n",
        "ğºâ¢ğ´â¢ğµâ¢ğ´+ğ¼ğ‘ â¢ğ‘¤â¢ğ‘–â¢ğ‘š.\n",
        "The currents ğ¼ğ‘£â¢ğ‘–â¢ğ‘  and ğ¼ğ‘ â¢ğ‘¤â¢ğ‘–â¢ğ‘š are assumed to arise from glutamatergic input targeting the serotonergic and GABAergic neurons, respectively, consistent with glutamate imaging under the assumption that the first glutamate wave arises from spillover from glutamatergic axons not targeting serotonergic neurons, which did not show a response to the first glutamate wave.\n",
        "Two-photon, plasma ablation of DRN GABAergic neurons\n",
        "We performed two-photon plasma ablation of DRN GABAergic neurons and voltage imaging of serotonergic neurons before and after the ablation (Figures 6 and S8) by using triple transgenic zebrafish, Tg(tph2:Gal4; UAS:Voltron; gad1b:loxP-RFP-loxP-GFP), and an optical setup described in our previous work.98 Briefly, we first performed a volumetric scan of red fluorescence, which represents GABAergic neurons expressing RFP, using 561-nm CW laser (Omicron, Germany) and 630/92 nm filter (FF01-630/92, Semrock), to determine the site of ablation manually. Based on this acquired stack, we chose 40-70 GABAergic cells inside the DRN for the main ablation (Figure 6) or outside the DRN for control ablation (Figures S8Câ€“S8F). The distribution of selected points is shown in Figures S8A and S8B. After the selection, we tuned the wavelength of a femtosecond laser (Camereon Ultra, Coherent) to 1050 nm and performed automatic ablation of selected cells. The lateral and depth positioning of the laser was controlled by a 2-axis galvanometer (Cambridge Technology) and a piezoelectric drive (Physik Instrument) of the objective lens. We chose the dwell time per cell to be 10-15 milliseconds because of the depth of this area. There are >100 GABAergic neurons in the larval zebrafish DRN,17 and we did not target all of them to avoid the risk of collateral damage to nearby serotonergic neurons.\n",
        "We performed voltage imaging of serotonergic neurons that express Voltron conjugated with JF525 fluorescent dye for 5 minutes during the motor vigor learning task before and after the above ablation procedure.\n",
        "\n",
        "\n",
        "# 9.5.1 Positron-2 analysis in whole brain paper\n",
        "\n",
        "'''\n",
        "Motion correction\n",
        "We applied motion correction to each z-stack layer separately using a rigid motion correction method (NoRMCorre32).\n",
        "\n",
        "ROI segmentation\n",
        "We used manual ROI labeling for Fish 0 and Fish 1. During the manual annotation process, we labeled two types of ROIs. Firstly, we identified and outlined objects that were approximately the size of a neuron (âˆ¼9 pixels or 6.6 Âµm) and had ring-like boundaries. Secondly, for objects of a similar size to neurons, which we tentatively identified as neurons, we analyzed their temporal intensity traces. We then selected those objects whose intensity traces show narrow (less than 20ms), high amplitude (e.g., >2Ã— the standard deviation of the traces), and positive-going spike signals. The annotated ROIs were reviewed by a second person. The percentage of ROIs that the second person disagreed with was âˆ¼5% of all the annotated ROIs. The disagreement concentrated on the cases when examining the intensity traces of an object was needed to decide whether to label the object.\n",
        "\n",
        "ROI temporal trace extraction\n",
        "The extracted ROIs are processed by VolPyâ€™s temporal trace extraction pipeline, which is composed of background removal, trace denoising and spike extraction. We used an adaptive threshold for spiking detection in VolPy. After VolPyâ€™s pipeline, we further remove the ROIs with low SNRVolPy ( less than 4) or ROIs with large overlap. The overlap is determined by calculating the spatial correlation between each ROI and all other ROIs. ROIs with over 0.9 correlation between itself and any of existing ROIs were rejected as they are likely selecting the same cell.\n",
        "\n",
        "Artifact removal\n",
        "After extracting putative spiking activity from each Region of Interest (ROI), we employed the Uniform Manifold Approximation and Projection (UMAP) algorithm to identify and remove ROIs that may have been influenced by recording artifacts, such as those caused by excitation intensity variation due to blood flow. To achieve this, we first applied a 250-ms moving Hanning window to each ROIâ€™s spike raster for smoothing and estimation of its firing rate. The entire recording data was structured into an N Ã— D matrix, where N represented the number of ROIs and D represented the number of temporal samples. This matrix was fed to the UMAP algorithm and transformed into an N Ã— 2 matrix, where each ROI was represented as a point within a 2-dimensional manifold. We then employed the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to identify clusters within the 2D representations of ROIs, effectively sorting ROIs with similar temporal spiking patterns into groups. We then visually identified the ROI cluster that was well-separated from the main cluster in the 2D UMAP representation. We then labeled the ROIs of the identified cluster with a unique cluster ID and removed them from the total set of ROIs. The remaining ROIs were then mapped and clustered again using UMAP-DBSCAN for the next iteration. The iteration was continued until the 2D UMAP representation became near Gaussian distributed. The remaining ROIs were assigned to a unique cluster. Following this clustering step, we scrutinized the temporal patterns and spatial distributions of ROIs within each group to detect any indications of artifacts resulting from recording imperfections, which were excluded from further analysis. The resulting ROIs were putatively treated as neurons, sorted into different groups using the same UMAP-DBSCAN algorithm, and await further analysis.\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKUKL85SkAAg"
      },
      "source": [
        "## 9.5.7 Complete Pipeline Examples from Literature\n",
        "\n",
        "### Positron2 Pipeline (Wang et al. - Whole-brain Zebrafish Voltage Imaging)\n",
        "\n",
        "This pipeline was used for imaging Positron2 across entire larval zebrafish brains.\n",
        "\n",
        "#### Manual ROI Annotation Criteria\n",
        "```python\n",
        "# ROI selection criteria from the paper:\n",
        "ROI_CRITERIA = {\n",
        "    'size_pixels': 9,           # ~6.6 Âµm diameter\n",
        "    'shape': 'ring-like',       # Membrane-targeted indicator\n",
        "    'spike_width_max_ms': 20,   # Narrow spikes\n",
        "    'spike_amplitude_std': 2.0, # >2Ã— std of trace\n",
        "    'spike_polarity': 'positive',  # Note: Positron2 has POSITIVE spikes (unlike Voltron-2)\n",
        "}\n",
        "```\n",
        "\n",
        "#### Overlap Removal (Spatial Correlation)\n",
        "```python\n",
        "def remove_overlapping_rois(roi_masks, traces, correlation_threshold=0.9):\n",
        "    \"\"\"\n",
        "    Remove ROIs that are likely duplicates based on spatial correlation.\n",
        "    From Wang et al.: ROIs with >0.9 correlation are rejected.\n",
        "    \"\"\"\n",
        "    n_rois = len(roi_masks)\n",
        "    keep_mask = np.ones(n_rois, dtype=bool)\n",
        "    \n",
        "    # Flatten masks for correlation\n",
        "    masks_flat = roi_masks.reshape(n_rois, -1).astype(float)\n",
        "    \n",
        "    # Compute pairwise spatial correlations\n",
        "    for i in range(n_rois):\n",
        "        if not keep_mask[i]:\n",
        "            continue\n",
        "        for j in range(i + 1, n_rois):\n",
        "            if not keep_mask[j]:\n",
        "                continue\n",
        "            \n",
        "            # Spatial correlation between masks\n",
        "            corr = np.corrcoef(masks_flat[i], masks_flat[j])[0, 1]\n",
        "            \n",
        "            if corr > correlation_threshold:\n",
        "                # Keep the one with higher SNR\n",
        "                snr_i = np.std(traces[i])  # Simplified SNR proxy\n",
        "                snr_j = np.std(traces[j])\n",
        "                if snr_i >= snr_j:\n",
        "                    keep_mask[j] = False\n",
        "                else:\n",
        "                    keep_mask[i] = False\n",
        "    \n",
        "    return np.where(keep_mask)[0]\n",
        "```\n",
        "\n",
        "#### Iterative UMAP-DBSCAN Artifact Removal\n",
        "```python\n",
        "def iterative_umap_dbscan_artifact_removal(spike_rasters, fps=200,\n",
        "                                            hanning_window_ms=250,\n",
        "                                            max_iterations=10):\n",
        "    \"\"\"\n",
        "    Iterative UMAP-DBSCAN clustering to remove artifact-contaminated ROIs.\n",
        "    From Wang et al.: Iterate until 2D UMAP representation is near-Gaussian.\n",
        "    \n",
        "    Args:\n",
        "        spike_rasters: list of binary spike trains per ROI\n",
        "        fps: frame rate\n",
        "        hanning_window_ms: smoothing window for firing rate estimation\n",
        "        max_iterations: maximum clustering iterations\n",
        "    \n",
        "    Returns:\n",
        "        clean_roi_indices: indices of ROIs that passed artifact removal\n",
        "        artifact_roi_indices: indices of ROIs flagged as artifacts\n",
        "        cluster_labels: final cluster assignments for clean ROIs\n",
        "    \"\"\"\n",
        "    import umap\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    from scipy.signal import convolve\n",
        "    from scipy.stats import shapiro\n",
        "    \n",
        "    n_rois = len(spike_rasters)\n",
        "    n_samples = len(spike_rasters[0])\n",
        "    \n",
        "    # Create smoothed firing rate matrix\n",
        "    window_samples = int(hanning_window_ms * fps / 1000)\n",
        "    hanning = np.hanning(window_samples)\n",
        "    hanning /= hanning.sum()\n",
        "    \n",
        "    firing_rates = np.zeros((n_rois, n_samples))\n",
        "    for i, raster in enumerate(spike_rasters):\n",
        "        firing_rates[i] = convolve(raster.astype(float), hanning, mode='same')\n",
        "    \n",
        "    # Track which ROIs are still candidates\n",
        "    candidate_mask = np.ones(n_rois, dtype=bool)\n",
        "    artifact_indices = []\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        current_indices = np.where(candidate_mask)[0]\n",
        "        if len(current_indices) < 10:\n",
        "            break\n",
        "        \n",
        "        current_rates = firing_rates[current_indices]\n",
        "        \n",
        "        # UMAP embedding\n",
        "        reducer = umap.UMAP(n_components=2, n_neighbors=min(15, len(current_indices)-1),\n",
        "                           min_dist=0.1, random_state=42)\n",
        "        embedding = reducer.fit_transform(current_rates)\n",
        "        \n",
        "        # Check if distribution is approximately Gaussian (stopping criterion)\n",
        "        # Use Shapiro-Wilk test on each dimension\n",
        "        _, p_x = shapiro(embedding[:, 0]) if len(embedding) >= 3 else (0, 1)\n",
        "        _, p_y = shapiro(embedding[:, 1]) if len(embedding) >= 3 else (0, 1)\n",
        "        \n",
        "        if p_x > 0.05 and p_y > 0.05:\n",
        "            # Distribution is approximately Gaussian - stop iterating\n",
        "            print(f\"Iteration {iteration}: UMAP distribution is Gaussian, stopping.\")\n",
        "            break\n",
        "        \n",
        "        # DBSCAN clustering\n",
        "        clusterer = DBSCAN(eps=0.5, min_samples=3)\n",
        "        labels = clusterer.fit_predict(embedding)\n",
        "        \n",
        "        # Find outlier cluster (well-separated from main cluster)\n",
        "        unique_labels = set(labels) - {-1}  # Exclude noise\n",
        "        if len(unique_labels) <= 1:\n",
        "            break\n",
        "        \n",
        "        # Main cluster = largest cluster\n",
        "        cluster_sizes = {l: np.sum(labels == l) for l in unique_labels}\n",
        "        main_cluster = max(cluster_sizes, key=cluster_sizes.get)\n",
        "        \n",
        "        # Find clusters that are well-separated (potential artifacts)\n",
        "        main_centroid = embedding[labels == main_cluster].mean(axis=0)\n",
        "        \n",
        "        for cluster_id in unique_labels:\n",
        "            if cluster_id == main_cluster:\n",
        "                continue\n",
        "            \n",
        "            cluster_centroid = embedding[labels == cluster_id].mean(axis=0)\n",
        "            distance = np.linalg.norm(cluster_centroid - main_centroid)\n",
        "            \n",
        "            # If cluster is far from main cluster, flag as artifact\n",
        "            main_std = embedding[labels == main_cluster].std()\n",
        "            if distance > 3 * main_std:\n",
        "                # Mark these ROIs as artifacts\n",
        "                artifact_mask = labels == cluster_id\n",
        "                artifact_roi_indices = current_indices[artifact_mask]\n",
        "                artifact_indices.extend(artifact_roi_indices)\n",
        "                candidate_mask[artifact_roi_indices] = False\n",
        "                print(f\"Iteration {iteration}: Removed {len(artifact_roi_indices)} artifact ROIs\")\n",
        "    \n",
        "    clean_indices = np.where(candidate_mask)[0]\n",
        "    return clean_indices, np.array(artifact_indices)\n",
        "```\n",
        "\n",
        "### SNR Calculation (VolPy-style)\n",
        "```python\n",
        "def compute_snr_volpy(trace, spike_frames, fps=200):\n",
        "    \"\"\"\n",
        "    Compute SNR as defined in VolPy.\n",
        "    SNR = median(spike_amplitudes) / noise_std\n",
        "    \n",
        "    Noise is estimated from spike-free regions.\n",
        "    \"\"\"\n",
        "    if len(spike_frames) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # For Voltron-2: invert trace (negative spikes)\n",
        "    # For Positron2: use trace directly (positive spikes)\n",
        "    \n",
        "    # Get spike amplitudes (peak - local baseline)\n",
        "    window = int(10 * fps / 1000)  # 10ms window\n",
        "    amplitudes = []\n",
        "    \n",
        "    for frame in spike_frames:\n",
        "        if frame < window or frame >= len(trace) - window:\n",
        "            continue\n",
        "        \n",
        "        # Local baseline (before spike)\n",
        "        baseline = np.median(trace[frame-window:frame-2])\n",
        "        # Peak value\n",
        "        peak = trace[frame]\n",
        "        amplitudes.append(np.abs(peak - baseline))\n",
        "    \n",
        "    if len(amplitudes) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Estimate noise from spike-free regions\n",
        "    spike_free_mask = np.ones(len(trace), dtype=bool)\n",
        "    for frame in spike_frames:\n",
        "        start = max(0, frame - window)\n",
        "        end = min(len(trace), frame + window)\n",
        "        spike_free_mask[start:end] = False\n",
        "    \n",
        "    if spike_free_mask.sum() < 100:\n",
        "        noise_std = np.std(trace)\n",
        "    else:\n",
        "        noise_std = np.std(trace[spike_free_mask])\n",
        "    \n",
        "    snr = np.median(amplitudes) / (noise_std + 1e-10)\n",
        "    return snr\n",
        "\n",
        "def filter_by_snr(traces, spike_times_list, fps=200, min_snr=4):\n",
        "    \"\"\"\n",
        "    Filter ROIs by SNR threshold.\n",
        "    Wang et al. used SNR > 4 as cutoff.\n",
        "    \"\"\"\n",
        "    keep_indices = []\n",
        "    snr_values = []\n",
        "    \n",
        "    for i, (trace, spike_times) in enumerate(zip(traces, spike_times_list)):\n",
        "        spike_frames = (np.array(spike_times) * fps).astype(int)\n",
        "        snr = compute_snr_volpy(trace, spike_frames, fps)\n",
        "        snr_values.append(snr)\n",
        "        \n",
        "        if snr >= min_snr:\n",
        "            keep_indices.append(i)\n",
        "    \n",
        "    return keep_indices, snr_values\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB5GLHdlkAAg"
      },
      "source": [
        "### Second Paper Pipeline (Camera Noise Correction, Local PCA, Semi-NMF, LSTM)\n",
        "\n",
        "If you have the second paper's methods available, please share them and I'll add detailed implementations for:\n",
        "- Camera noise correction\n",
        "- dipy-based motion correction  \n",
        "- Local PCA denoising specifics\n",
        "- Semi-NMF segmentation details\n",
        "- LSTM spike detection\n",
        "- Subthreshold activity extraction\n",
        "\n",
        "In the meantime, here are general implementations of the key techniques mentioned:\n",
        "\n",
        "#### Camera Noise Correction\n",
        "```python\n",
        "def correct_camera_noise(video, dark_frame=None, flat_field=None):\n",
        "    \"\"\"\n",
        "    Correct for camera-specific noise patterns.\n",
        "    \n",
        "    Args:\n",
        "        video: raw video (n_frames, height, width)\n",
        "        dark_frame: average of frames with no illumination (fixed pattern noise)\n",
        "        flat_field: normalized response to uniform illumination\n",
        "    \n",
        "    Returns:\n",
        "        corrected video\n",
        "    \"\"\"\n",
        "    corrected = video.astype(np.float32)\n",
        "    \n",
        "    # Subtract dark frame (fixed pattern noise)\n",
        "    if dark_frame is not None:\n",
        "        corrected -= dark_frame\n",
        "    \n",
        "    # Divide by flat field (pixel sensitivity variation)\n",
        "    if flat_field is not None:\n",
        "        # Normalize flat field\n",
        "        flat_norm = flat_field / np.mean(flat_field)\n",
        "        flat_norm = np.clip(flat_norm, 0.1, 10)  # Avoid extreme corrections\n",
        "        corrected /= flat_norm\n",
        "    \n",
        "    # Remove hot pixels (outliers in spatial domain)\n",
        "    from scipy.ndimage import median_filter\n",
        "    for i in range(len(corrected)):\n",
        "        frame = corrected[i]\n",
        "        median_frame = median_filter(frame, size=3)\n",
        "        # Replace pixels that deviate significantly from local median\n",
        "        outlier_mask = np.abs(frame - median_frame) > 5 * np.std(frame)\n",
        "        corrected[i][outlier_mask] = median_frame[outlier_mask]\n",
        "    \n",
        "    return corrected\n",
        "\n",
        "def estimate_dark_frame(dark_video):\n",
        "    \"\"\"Estimate dark frame from a recording with no illumination.\"\"\"\n",
        "    return np.median(dark_video, axis=0)\n",
        "\n",
        "def estimate_flat_field(flat_video):\n",
        "    \"\"\"Estimate flat field from uniform illumination recording.\"\"\"\n",
        "    mean_frame = np.mean(flat_video, axis=0)\n",
        "    # Normalize to mean of 1\n",
        "    return mean_frame / np.mean(mean_frame)\n",
        "```\n",
        "\n",
        "#### Background Subtraction Methods\n",
        "```python\n",
        "def subtract_background_annulus(video, roi_mask, inner_gap=2, outer_width=5):\n",
        "    \"\"\"\n",
        "    Subtract local background using annulus around ROI.\n",
        "    Common in voltage imaging to remove neuropil contamination.\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import binary_dilation, binary_erosion\n",
        "    \n",
        "    # Create annulus mask\n",
        "    dilated = binary_dilation(roi_mask, iterations=inner_gap + outer_width)\n",
        "    inner = binary_dilation(roi_mask, iterations=inner_gap)\n",
        "    annulus = dilated & ~inner\n",
        "    \n",
        "    # Extract traces\n",
        "    roi_trace = np.mean(video[:, roi_mask], axis=1)\n",
        "    bg_trace = np.mean(video[:, annulus], axis=1) if annulus.sum() > 0 else 0\n",
        "    \n",
        "    # Subtract with optional coefficient\n",
        "    return roi_trace - 0.7 * bg_trace  # 0.7 is typical neuropil coefficient\n",
        "\n",
        "def subtract_background_percentile(trace, percentile=8, window_sec=1.0, fps=200):\n",
        "    \"\"\"\n",
        "    Rolling percentile baseline subtraction.\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import percentile_filter\n",
        "    window = int(window_sec * fps)\n",
        "    baseline = percentile_filter(trace, percentile, size=window)\n",
        "    return trace - baseline\n",
        "```\n",
        "\n",
        "#### Baseline Correction and dF/F Computation\n",
        "```python\n",
        "def compute_dff(trace, method='percentile', percentile=10, window_sec=None, fps=200):\n",
        "    \"\"\"\n",
        "    Compute Î”F/F with various baseline methods.\n",
        "    \n",
        "    Methods:\n",
        "        'percentile': global percentile as F0\n",
        "        'rolling_percentile': sliding window percentile\n",
        "        'exponential': fit exponential decay (for photobleaching)\n",
        "    \"\"\"\n",
        "    if method == 'percentile':\n",
        "        f0 = np.percentile(trace, percentile)\n",
        "        dff = (trace - f0) / (f0 + 1e-10)\n",
        "        \n",
        "    elif method == 'rolling_percentile':\n",
        "        from scipy.ndimage import percentile_filter\n",
        "        window = int((window_sec or 2.0) * fps)\n",
        "        f0 = percentile_filter(trace, percentile, size=window)\n",
        "        dff = (trace - f0) / (f0 + 1e-10)\n",
        "        \n",
        "    elif method == 'exponential':\n",
        "        # Fit exponential decay for photobleaching correction\n",
        "        from scipy.optimize import curve_fit\n",
        "        \n",
        "        def exp_decay(t, a, b, c):\n",
        "            return a * np.exp(-b * t) + c\n",
        "        \n",
        "        t = np.arange(len(trace))\n",
        "        try:\n",
        "            # Use robust fitting (median of rolling windows as targets)\n",
        "            window = fps  # 1 second windows\n",
        "            n_windows = len(trace) // window\n",
        "            t_fit = np.array([i * window + window//2 for i in range(n_windows)])\n",
        "            y_fit = np.array([np.median(trace[i*window:(i+1)*window]) for i in range(n_windows)])\n",
        "            \n",
        "            popt, _ = curve_fit(exp_decay, t_fit, y_fit,\n",
        "                               p0=[trace[0]-trace[-1], 0.001, trace[-1]],\n",
        "                               maxfev=1000)\n",
        "            f0 = exp_decay(t, *popt)\n",
        "        except:\n",
        "            f0 = np.percentile(trace, percentile)\n",
        "        \n",
        "        dff = (trace - f0) / (f0 + 1e-10)\n",
        "    \n",
        "    return dff\n",
        "```\n",
        "\n",
        "#### Subthreshold Activity Extraction\n",
        "```python\n",
        "def extract_subthreshold(trace, spike_frames, fps=200,\n",
        "                         interpolation_window_ms=20,\n",
        "                         lowpass_cutoff_hz=10):\n",
        "    \"\"\"\n",
        "    Extract subthreshold membrane potential by removing spikes and lowpass filtering.\n",
        "    \n",
        "    1. Interpolate over spike regions\n",
        "    2. Lowpass filter to get slow membrane potential fluctuations\n",
        "    \"\"\"\n",
        "    from scipy.interpolate import interp1d\n",
        "    from scipy.signal import butter, filtfilt\n",
        "    \n",
        "    # Create mask of spike regions to interpolate\n",
        "    window = int(interpolation_window_ms * fps / 1000)\n",
        "    spike_mask = np.zeros(len(trace), dtype=bool)\n",
        "    for frame in spike_frames:\n",
        "        start = max(0, frame - window//2)\n",
        "        end = min(len(trace), frame + window//2)\n",
        "        spike_mask[start:end] = True\n",
        "    \n",
        "    # Interpolate over spikes\n",
        "    x = np.arange(len(trace))\n",
        "    valid_x = x[~spike_mask]\n",
        "    valid_y = trace[~spike_mask]\n",
        "    \n",
        "    if len(valid_x) < 10:\n",
        "        interpolated = trace.copy()\n",
        "    else:\n",
        "        f = interp1d(valid_x, valid_y, kind='linear',\n",
        "                     bounds_error=False, fill_value='extrapolate')\n",
        "        interpolated = f(x)\n",
        "    \n",
        "    # Lowpass filter\n",
        "    nyquist = fps / 2\n",
        "    b, a = butter(4, lowpass_cutoff_hz / nyquist, btype='low')\n",
        "    subthreshold = filtfilt(b, a, interpolated)\n",
        "    \n",
        "    return subthreshold\n",
        "```\n",
        "\n",
        "#### Complete Pipeline Function\n",
        "```python\n",
        "def voltage_imaging_pipeline_full(video, fps=200, config=None):\n",
        "    \"\"\"\n",
        "    Full voltage imaging pipeline with all stages.\n",
        "    \n",
        "    Args:\n",
        "        video: (n_frames, height, width) raw video\n",
        "        fps: frame rate\n",
        "        config: dict of parameters for each stage\n",
        "    \n",
        "    Returns:\n",
        "        dict with roi_masks, traces, spike_times, subthreshold, metadata\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = {}\n",
        "    \n",
        "    results = {'metadata': {'fps': fps, 'config': config}}\n",
        "    \n",
        "    # === Stage 0: Camera noise correction ===\n",
        "    if config.get('camera_correction', False):\n",
        "        video = correct_camera_noise(video)\n",
        "    \n",
        "    # === Stage 1: Motion correction ===\n",
        "    mc_method = config.get('motion_correction', 'template')\n",
        "    if mc_method == 'template':\n",
        "        video, shifts = motion_correct_template(video, max_shift=50)\n",
        "        results['metadata']['motion_shifts'] = shifts\n",
        "    elif mc_method == 'none':\n",
        "        pass\n",
        "    \n",
        "    # === Stage 2: Denoising ===\n",
        "    denoise_method = config.get('denoising', 'local_pca')\n",
        "    if denoise_method == 'local_pca':\n",
        "        video = local_pca_denoise(video,\n",
        "                                   patch_size=config.get('pca_patch_size', 32),\n",
        "                                   n_components=config.get('pca_components', 10))\n",
        "    elif denoise_method == 'temporal_median':\n",
        "        video = temporal_median_filter(video, window=3)\n",
        "    \n",
        "    # === Stage 3: ROI Segmentation ===\n",
        "    seg_method = config.get('segmentation', 'correlation')\n",
        "    std_proj = np.std(video, axis=0)\n",
        "    \n",
        "    if seg_method == 'correlation':\n",
        "        corr_img = compute_correlation_image(video)\n",
        "        roi_masks = segment_from_correlation(corr_img, std_proj)\n",
        "    elif seg_method == 'ring':\n",
        "        roi_masks = detect_ring_rois(std_proj)\n",
        "    elif seg_method == 'threshold':\n",
        "        # Simple threshold segmentation\n",
        "        thresh = np.mean(std_proj) + 1.5 * np.std(std_proj)\n",
        "        binary = std_proj > thresh\n",
        "        binary = morphology.remove_small_objects(binary, min_size=30)\n",
        "        labeled = measure.label(binary)\n",
        "        roi_masks = np.array([labeled == i for i in range(1, labeled.max()+1)])\n",
        "    \n",
        "    results['roi_masks'] = roi_masks\n",
        "    results['metadata']['n_rois_initial'] = len(roi_masks)\n",
        "    \n",
        "    # === Stage 4: Trace extraction ===\n",
        "    traces = []\n",
        "    for mask in roi_masks:\n",
        "        if config.get('background_subtraction', 'annulus') == 'annulus':\n",
        "            trace = subtract_background_annulus(video, mask)\n",
        "        else:\n",
        "            trace = np.mean(video[:, mask], axis=1)\n",
        "        \n",
        "        # Compute dF/F\n",
        "        dff = compute_dff(trace,\n",
        "                         method=config.get('baseline_method', 'rolling_percentile'),\n",
        "                         fps=fps)\n",
        "        traces.append(dff)\n",
        "    \n",
        "    traces = np.array(traces)\n",
        "    results['traces'] = traces\n",
        "    \n",
        "    # === Stage 5: Spike detection ===\n",
        "    spike_method = config.get('spike_detection', 'adaptive')\n",
        "    spike_times = []\n",
        "    spike_frames = []\n",
        "    \n",
        "    for trace in traces:\n",
        "        if spike_method == 'adaptive':\n",
        "            times, _ = detect_spikes_adaptive(trace, fps)\n",
        "        elif spike_method == 'mad':\n",
        "            times = detect_spikes_mad(trace, fps)\n",
        "        elif spike_method == 'template':\n",
        "            times = detect_spikes_template(trace, fps)\n",
        "        else:\n",
        "            # Simple threshold\n",
        "            inverted = -trace\n",
        "            threshold = np.mean(inverted) + 3 * np.std(inverted)\n",
        "            peaks, _ = signal.find_peaks(inverted, height=threshold)\n",
        "            times = peaks / fps\n",
        "        \n",
        "        spike_times.append(times)\n",
        "        spike_frames.append((times * fps).astype(int))\n",
        "    \n",
        "    results['spike_times'] = spike_times\n",
        "    results['spike_frames'] = spike_frames\n",
        "    \n",
        "    # === Stage 6: Quality filtering ===\n",
        "    # SNR filter\n",
        "    if config.get('snr_filter', True):\n",
        "        keep_idx, snr_values = filter_by_snr(traces, spike_times, fps,\n",
        "                                              min_snr=config.get('min_snr', 4))\n",
        "        results['metadata']['snr_values'] = snr_values\n",
        "    else:\n",
        "        keep_idx = list(range(len(roi_masks)))\n",
        "    \n",
        "    # Overlap removal\n",
        "    if config.get('overlap_removal', True):\n",
        "        keep_idx_overlap = remove_overlapping_rois(\n",
        "            roi_masks[keep_idx],\n",
        "            traces[keep_idx],\n",
        "            correlation_threshold=0.9\n",
        "        )\n",
        "        keep_idx = [keep_idx[i] for i in keep_idx_overlap]\n",
        "    \n",
        "    results['metadata']['n_rois_final'] = len(keep_idx)\n",
        "    results['keep_indices'] = keep_idx\n",
        "    \n",
        "    # === Stage 7: Artifact removal (optional) ===\n",
        "    if config.get('artifact_removal', False) and len(keep_idx) > 10:\n",
        "        # Create spike rasters\n",
        "        n_frames = video.shape[0]\n",
        "        spike_rasters = []\n",
        "        for idx in keep_idx:\n",
        "            raster = np.zeros(n_frames)\n",
        "            raster[spike_frames[idx]] = 1\n",
        "            spike_rasters.append(raster)\n",
        "        \n",
        "        clean_idx, artifact_idx = iterative_umap_dbscan_artifact_removal(spike_rasters, fps)\n",
        "        keep_idx = [keep_idx[i] for i in clean_idx]\n",
        "        results['metadata']['n_rois_after_artifact_removal'] = len(keep_idx)\n",
        "    \n",
        "    # === Stage 8: Subthreshold extraction (optional) ===\n",
        "    if config.get('extract_subthreshold', False):\n",
        "        subthreshold = []\n",
        "        for idx in keep_idx:\n",
        "            sub = extract_subthreshold(traces[idx], spike_frames[idx], fps)\n",
        "            subthreshold.append(sub)\n",
        "        results['subthreshold'] = np.array(subthreshold)\n",
        "    \n",
        "    # Filter results to kept ROIs\n",
        "    results['roi_masks_filtered'] = roi_masks[keep_idx]\n",
        "    results['traces_filtered'] = traces[keep_idx]\n",
        "    results['spike_times_filtered'] = [spike_times[i] for i in keep_idx]\n",
        "    \n",
        "    return results\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6UCqb_okAAh"
      },
      "source": [
        "### Kawashima et al. Pipeline (Voltron in Zebrafish - Raphe Serotonin Study)\n",
        "\n",
        "This pipeline was used for voltage imaging of Voltron in zebrafish raphe neurons. Notable for detailed camera noise correction, local PCA denoising with whiteness stopping criterion, semi-NMF segmentation, and LSTM spike detection.\n",
        "\n",
        "#### Step 1: Camera Noise Correction (Huang/Liu method)\n",
        "```python\n",
        "def camera_noise_correction_kawashima(video, dark_frames, calibration_data=None):\n",
        "    \"\"\"\n",
        "    Camera noise correction from Kawashima et al.\n",
        "    \n",
        "    Corrected pixel: s_i = (s_i^r - o_i) / g_i\n",
        "    \n",
        "    Where:\n",
        "    - s_i^r: raw readout at pixel i\n",
        "    - o_i: camera offset (mean of dark frames)\n",
        "    - g_i: camera gain (fitted from variance vs intensity relationship)\n",
        "    \n",
        "    Args:\n",
        "        video: raw video (n_frames, height, width)\n",
        "        dark_frames: video acquired with no illumination (for offset estimation)\n",
        "        calibration_data: dict with multiple illumination levels for gain estimation\n",
        "            {'level_k': {'mean': D_ik, 'variance': v_ik}, ...}\n",
        "    \n",
        "    Returns:\n",
        "        corrected video\n",
        "    \"\"\"\n",
        "    # Estimate offset (o_i) from dark frames\n",
        "    offset = np.mean(dark_frames, axis=0)  # Mean of ~60k dark images\n",
        "    baseline_variance = np.var(dark_frames, axis=0)  # Variance in dark condition\n",
        "    \n",
        "    # Estimate gain (g_i) from calibration data at multiple illumination levels\n",
        "    if calibration_data is not None:\n",
        "        # Fit gain: minimize sum_k ((v_ik - v_i) - g_i * (D_ik - o_i))^2\n",
        "        # This is linear regression: variance_excess = gain * intensity_excess\n",
        "        \n",
        "        h, w = offset.shape\n",
        "        gain = np.ones((h, w), dtype=np.float32)\n",
        "        \n",
        "        levels = sorted(calibration_data.keys())\n",
        "        for y in range(h):\n",
        "            for x in range(w):\n",
        "                # Collect (intensity, variance) pairs across illumination levels\n",
        "                intensities = []\n",
        "                variances = []\n",
        "                for level in levels:\n",
        "                    D_ik = calibration_data[level]['mean'][y, x]\n",
        "                    v_ik = calibration_data[level]['variance'][y, x]\n",
        "                    \n",
        "                    intensity_excess = D_ik - offset[y, x]\n",
        "                    variance_excess = v_ik - baseline_variance[y, x]\n",
        "                    \n",
        "                    if intensity_excess > 0:\n",
        "                        intensities.append(intensity_excess)\n",
        "                        variances.append(variance_excess)\n",
        "                \n",
        "                if len(intensities) >= 2:\n",
        "                    # Linear regression through origin: variance = gain * intensity\n",
        "                    intensities = np.array(intensities)\n",
        "                    variances = np.array(variances)\n",
        "                    gain[y, x] = np.sum(intensities * variances) / np.sum(intensities ** 2)\n",
        "                    gain[y, x] = np.clip(gain[y, x], 0.1, 10)  # Reasonable bounds\n",
        "    else:\n",
        "        # Without calibration data, assume uniform gain\n",
        "        gain = np.ones_like(offset)\n",
        "    \n",
        "    # Apply correction: s_i = (s_i^r - o_i) / g_i\n",
        "    corrected = (video.astype(np.float32) - offset) / gain\n",
        "    \n",
        "    return corrected, {'offset': offset, 'gain': gain, 'baseline_variance': baseline_variance}\n",
        "\n",
        "def estimate_camera_calibration(calibration_videos):\n",
        "    \"\"\"\n",
        "    Estimate camera calibration from videos at multiple illumination levels.\n",
        "    \n",
        "    Args:\n",
        "        calibration_videos: dict mapping illumination level to video array\n",
        "            e.g., {0: dark_video, 5: video_5mW, 10: video_10mW, 18: video_18mW}\n",
        "    \n",
        "    Returns:\n",
        "        calibration_data for camera_noise_correction_kawashima\n",
        "    \"\"\"\n",
        "    calibration_data = {}\n",
        "    for level, video in calibration_videos.items():\n",
        "        calibration_data[level] = {\n",
        "            'mean': np.mean(video, axis=0),\n",
        "            'variance': np.var(video, axis=0)\n",
        "        }\n",
        "    return calibration_data\n",
        "```\n",
        "\n",
        "#### Step 2: Motion Correction (dipy-based)\n",
        "```python\n",
        "def motion_correct_dipy(video, reference='mean', subpixel=True):\n",
        "    \"\"\"\n",
        "    2D rigid registration using dipy package.\n",
        "    Subpixel-level correction for sequential images.\n",
        "    \n",
        "    Args:\n",
        "        video: motion-corrected video (n_frames, height, width)\n",
        "        reference: 'mean', 'first', or frame index\n",
        "        subpixel: if True, use subpixel registration\n",
        "    \n",
        "    Returns:\n",
        "        corrected video, shifts array\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from dipy.align.imaffine import AffineRegistration, MutualInformationMetric\n",
        "        from dipy.align.transforms import TranslationTransform2D\n",
        "        USE_DIPY = True\n",
        "    except ImportError:\n",
        "        USE_DIPY = False\n",
        "        print(\"dipy not installed, falling back to skimage\")\n",
        "    \n",
        "    n_frames, h, w = video.shape\n",
        "    \n",
        "    # Create reference frame\n",
        "    if reference == 'mean':\n",
        "        ref_frame = np.mean(video, axis=0)\n",
        "    elif reference == 'first':\n",
        "        ref_frame = video[0]\n",
        "    elif isinstance(reference, int):\n",
        "        ref_frame = video[reference]\n",
        "    else:\n",
        "        ref_frame = np.mean(video, axis=0)\n",
        "    \n",
        "    corrected = np.zeros_like(video, dtype=np.float32)\n",
        "    shifts = np.zeros((n_frames, 2))\n",
        "    \n",
        "    if USE_DIPY:\n",
        "        # dipy-based registration\n",
        "        metric = MutualInformationMetric(nbins=32)\n",
        "        affreg = AffineRegistration(metric=metric)\n",
        "        transform = TranslationTransform2D()\n",
        "        \n",
        "        for i in range(n_frames):\n",
        "            frame = video[i].astype(np.float64)\n",
        "            ref = ref_frame.astype(np.float64)\n",
        "            \n",
        "            # Register\n",
        "            affine = affreg.optimize(ref, frame, transform, params0=None)\n",
        "            \n",
        "            # Extract translation\n",
        "            shifts[i] = affine.affine[:2, 2]\n",
        "            \n",
        "            # Apply transformation\n",
        "            from scipy.ndimage import affine_transform\n",
        "            corrected[i] = affine_transform(frame, affine.affine[:2, :2],\n",
        "                                            offset=affine.affine[:2, 2])\n",
        "    else:\n",
        "        # Fallback to skimage phase_cross_correlation\n",
        "        from skimage.registration import phase_cross_correlation\n",
        "        from scipy.ndimage import shift as ndi_shift\n",
        "        \n",
        "        for i in range(n_frames):\n",
        "            frame = video[i].astype(np.float64)\n",
        "            \n",
        "            shift, error, diffphase = phase_cross_correlation(\n",
        "                ref_frame, frame, upsample_factor=10 if subpixel else 1\n",
        "            )\n",
        "            shifts[i] = shift\n",
        "            corrected[i] = ndi_shift(frame, shift, mode='constant', cval=0)\n",
        "    \n",
        "    return corrected, shifts\n",
        "```\n",
        "\n",
        "#### Step 3: Local PCA Denoising (with whiteness criterion)\n",
        "```python\n",
        "def local_pca_denoise_kawashima(video, patch_size=64, overlap=32, max_components=50):\n",
        "    \"\"\"\n",
        "    Local PCA denoising with automatic rank selection.\n",
        "    \n",
        "    Stop adding components when residual is statistically white\n",
        "    (within 99% confidence interval).\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    - Find low-rank representation Y_bar = sum_k u_k * v_k\n",
        "    - Stop when residual R_k = Y - Y_bar is white noise\n",
        "    \n",
        "    Args:\n",
        "        video: (n_frames, height, width) motion-corrected video\n",
        "        patch_size: size of spatial patches\n",
        "        overlap: overlap between patches\n",
        "        max_components: maximum number of PCA components\n",
        "    \n",
        "    Returns:\n",
        "        denoised video\n",
        "    \"\"\"\n",
        "    from scipy.stats import chi2\n",
        "    \n",
        "    n_frames, h, w = video.shape\n",
        "    stride = patch_size - overlap\n",
        "    \n",
        "    # Output arrays\n",
        "    denoised = np.zeros_like(video, dtype=np.float64)\n",
        "    weights = np.zeros((h, w), dtype=np.float64)\n",
        "    \n",
        "    def is_white_noise(residual, confidence=0.99):\n",
        "        \"\"\"\n",
        "        Test if residual is statistically white using Ljung-Box test.\n",
        "        For simplicity, we check if autocorrelation at lag 1 is near zero.\n",
        "        \"\"\"\n",
        "        # Flatten spatial dimensions\n",
        "        n_frames, n_pixels = residual.shape\n",
        "        \n",
        "        # Sample some pixels for efficiency\n",
        "        sample_idx = np.random.choice(n_pixels, min(100, n_pixels), replace=False)\n",
        "        \n",
        "        autocorrs = []\n",
        "        for idx in sample_idx:\n",
        "            trace = residual[:, idx]\n",
        "            if np.std(trace) > 1e-10:\n",
        "                # Lag-1 autocorrelation\n",
        "                autocorr = np.corrcoef(trace[:-1], trace[1:])[0, 1]\n",
        "                autocorrs.append(autocorr)\n",
        "        \n",
        "        if len(autocorrs) == 0:\n",
        "            return True\n",
        "        \n",
        "        # For white noise, autocorrelation should be ~N(0, 1/n_frames)\n",
        "        # Check if mean absolute autocorrelation is small\n",
        "        mean_abs_autocorr = np.mean(np.abs(autocorrs))\n",
        "        threshold = 2.58 / np.sqrt(n_frames)  # 99% CI for autocorr of white noise\n",
        "        \n",
        "        return mean_abs_autocorr < threshold\n",
        "    \n",
        "    # Process each patch\n",
        "    for y_start in range(0, h - patch_size + 1, stride):\n",
        "        for x_start in range(0, w - patch_size + 1, stride):\n",
        "            # Extract patch: (n_frames, patch_size, patch_size)\n",
        "            patch = video[:, y_start:y_start+patch_size, x_start:x_start+patch_size]\n",
        "            patch_flat = patch.reshape(n_frames, -1)  # (n_frames, n_pixels)\n",
        "            \n",
        "            # Center the data\n",
        "            mean_vals = patch_flat.mean(axis=0)\n",
        "            centered = patch_flat - mean_vals\n",
        "            \n",
        "            # Iterative PCA with whiteness stopping criterion\n",
        "            U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
        "            \n",
        "            # Find optimal number of components\n",
        "            best_k = 1\n",
        "            for k in range(1, min(max_components, len(S))):\n",
        "                # Reconstruct with k components\n",
        "                reconstructed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
        "                residual = centered - reconstructed\n",
        "                \n",
        "                if is_white_noise(residual):\n",
        "                    best_k = k\n",
        "                    break\n",
        "                best_k = k\n",
        "            \n",
        "            # Final reconstruction with best_k components\n",
        "            reconstructed = U[:, :best_k] @ np.diag(S[:best_k]) @ Vt[:best_k, :]\n",
        "            reconstructed += mean_vals\n",
        "            reconstructed = reconstructed.reshape(n_frames, patch_size, patch_size)\n",
        "            \n",
        "            # Add to output with overlap weighting\n",
        "            denoised[:, y_start:y_start+patch_size, x_start:x_start+patch_size] += reconstructed\n",
        "            weights[y_start:y_start+patch_size, x_start:x_start+patch_size] += 1\n",
        "    \n",
        "    # Handle edges (patches that don't fit)\n",
        "    # ... (simplified: just use available data)\n",
        "    \n",
        "    # Normalize by overlap count\n",
        "    weights = np.maximum(weights, 1)  # Avoid division by zero\n",
        "    denoised /= weights[np.newaxis, :, :]\n",
        "    \n",
        "    return denoised.astype(np.float32)\n",
        "```\n",
        "\n",
        "#### Step 4: Semi-NMF Cell Segmentation\n",
        "```python\n",
        "def semi_nmf_segmentation_kawashima(video, n_components=100, correlation_threshold=0.8,\n",
        "                                     max_iter=500, min_roi_size=20):\n",
        "    \"\"\"\n",
        "    Semi-nonnegative matrix factorization for cell segmentation.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    minimize ||Y - A*F - B||^2\n",
        "    subject to: A >= 0, B = b * 1^T, b >= 0\n",
        "    \n",
        "    Where:\n",
        "    - A: spatial components (non-negative ROI masks)\n",
        "    - F: temporal components (can be negative - voltage traces)\n",
        "    - B: temporally constant background\n",
        "    \n",
        "    Initialization: super-pixels with local correlation > 0.8\n",
        "    \n",
        "    Args:\n",
        "        video: denoised video (n_frames, height, width)\n",
        "        n_components: number of components to extract\n",
        "        correlation_threshold: for super-pixel initialization\n",
        "        max_iter: maximum iterations for optimization\n",
        "        min_roi_size: minimum ROI size in pixels\n",
        "    \n",
        "    Returns:\n",
        "        roi_masks: (n_rois, height, width) boolean masks\n",
        "        temporal_components: (n_rois, n_frames) fluorescence traces\n",
        "        background: (height, width) static background\n",
        "    \"\"\"\n",
        "    n_frames, h, w = video.shape\n",
        "    n_pixels = h * w\n",
        "    \n",
        "    # Reshape video: Y is (n_pixels, n_frames)\n",
        "    Y = video.reshape(n_frames, n_pixels).T.astype(np.float64)\n",
        "    \n",
        "    # === Step 1: Initialize with super-pixels ===\n",
        "    def compute_local_correlation_map(video, radius=2):\n",
        "        \"\"\"Compute local correlation for each pixel.\"\"\"\n",
        "        corr_map = np.zeros((h, w))\n",
        "        video_norm = (video - video.mean(axis=0)) / (video.std(axis=0) + 1e-10)\n",
        "        \n",
        "        for dy in range(-radius, radius+1):\n",
        "            for dx in range(-radius, radius+1):\n",
        "                if dy == 0 and dx == 0:\n",
        "                    continue\n",
        "                shifted = np.roll(np.roll(video_norm, dy, axis=1), dx, axis=2)\n",
        "                corr = (video_norm * shifted).mean(axis=0)\n",
        "                corr_map += corr\n",
        "        \n",
        "        corr_map /= (2*radius + 1)**2 - 1\n",
        "        return corr_map\n",
        "    \n",
        "    def find_super_pixels(video, corr_threshold=0.8):\n",
        "        \"\"\"Find super-pixels: connected regions with high local correlation.\"\"\"\n",
        "        corr_map = compute_local_correlation_map(video)\n",
        "        binary = corr_map > corr_threshold\n",
        "        \n",
        "        from scipy.ndimage import label\n",
        "        labeled, n_labels = label(binary)\n",
        "        \n",
        "        super_pixels = []\n",
        "        for i in range(1, n_labels + 1):\n",
        "            mask = labeled == i\n",
        "            if mask.sum() >= min_roi_size:\n",
        "                super_pixels.append(mask)\n",
        "        \n",
        "        return super_pixels\n",
        "    \n",
        "    super_pixels = find_super_pixels(video, correlation_threshold)\n",
        "    n_init = min(len(super_pixels), n_components)\n",
        "    \n",
        "    if n_init == 0:\n",
        "        print(\"Warning: No super-pixels found, using random initialization\")\n",
        "        # Random initialization\n",
        "        n_init = n_components\n",
        "        super_pixels = []\n",
        "        for _ in range(n_init):\n",
        "            mask = np.zeros((h, w), dtype=bool)\n",
        "            cy, cx = np.random.randint(10, h-10), np.random.randint(10, w-10)\n",
        "            mask[cy-3:cy+3, cx-3:cx+3] = True\n",
        "            super_pixels.append(mask)\n",
        "    \n",
        "    # Initialize A (spatial components)\n",
        "    A = np.zeros((n_pixels, n_init))\n",
        "    for i, mask in enumerate(super_pixels[:n_init]):\n",
        "        A[:, i] = mask.flatten().astype(np.float64)\n",
        "    \n",
        "    # Initialize background b (mean of low-variance pixels)\n",
        "    pixel_var = Y.var(axis=1)\n",
        "    low_var_mask = pixel_var < np.percentile(pixel_var, 20)\n",
        "    b = Y[low_var_mask].mean(axis=1) if low_var_mask.sum() > 0 else Y.mean(axis=1)\n",
        "    b = np.maximum(b, 0)  # Non-negative background\n",
        "    \n",
        "    # B = b * 1^T\n",
        "    B = np.outer(b, np.ones(n_frames)) if len(b) == n_pixels else np.zeros((n_pixels, n_frames))\n",
        "    \n",
        "    # === Step 2: Alternating optimization ===\n",
        "    for iteration in range(max_iter):\n",
        "        # Update F (temporal components): F = (A^T A)^-1 A^T (Y - B)\n",
        "        Y_bg = Y - B\n",
        "        AtA = A.T @ A + 1e-6 * np.eye(A.shape[1])  # Regularization\n",
        "        AtY = A.T @ Y_bg\n",
        "        F = np.linalg.solve(AtA, AtY)\n",
        "        \n",
        "        # Update A (spatial components): A = (Y - B) F^T (F F^T)^-1, then clip to >= 0\n",
        "        FFt = F @ F.T + 1e-6 * np.eye(F.shape[0])\n",
        "        YFt = Y_bg @ F.T\n",
        "        A = np.linalg.solve(FFt.T, YFt.T).T\n",
        "        A = np.maximum(A, 0)  # Non-negative constraint\n",
        "        \n",
        "        # Update background b: b = mean((Y - A*F), axis=1), clipped to >= 0\n",
        "        residual = Y - A @ F\n",
        "        b = residual.mean(axis=1)\n",
        "        b = np.maximum(b, 0)\n",
        "        B = np.outer(b, np.ones(n_frames))\n",
        "        \n",
        "        # Check convergence\n",
        "        if iteration % 50 == 0:\n",
        "            reconstruction_error = np.linalg.norm(Y - A @ F - B, 'fro')\n",
        "            print(f\"Iteration {iteration}: reconstruction error = {reconstruction_error:.2f}\")\n",
        "    \n",
        "    # === Step 3: Extract ROI masks ===\n",
        "    roi_masks = []\n",
        "    temporal_components = []\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        spatial = A[:, i].reshape(h, w)\n",
        "        temporal = F[i, :]\n",
        "        \n",
        "        # Threshold spatial component to get mask\n",
        "        thresh = np.percentile(spatial[spatial > 0], 90) if (spatial > 0).sum() > 0 else 0\n",
        "        mask = spatial > thresh\n",
        "        \n",
        "        # Clean up mask\n",
        "        mask = morphology.remove_small_objects(mask, min_size=min_roi_size)\n",
        "        \n",
        "        if mask.sum() >= min_roi_size:\n",
        "            roi_masks.append(mask)\n",
        "            temporal_components.append(temporal)\n",
        "    \n",
        "    return (np.array(roi_masks),\n",
        "            np.array(temporal_components),\n",
        "            b.reshape(h, w))\n",
        "```\n",
        "\n",
        "#### Step 5: LSTM Spike Detection\n",
        "```python\n",
        "def create_lstm_spike_detector(window_size=41, hidden_size=64, dropout=0.3):\n",
        "    \"\"\"\n",
        "    Create LSTM network for spike detection.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    - Two LSTM layers with dropout between them\n",
        "    - Input: time series window (41 frames = 136.67 ms at 300 Hz)\n",
        "    - Output: probability of spike at center of window\n",
        "    - Trained on simultaneous ephys + imaging data\n",
        "    \n",
        "    Args:\n",
        "        window_size: input window size (41 frames in paper)\n",
        "        hidden_size: LSTM hidden layer size\n",
        "        dropout: dropout rate between LSTM layers\n",
        "    \n",
        "    Returns:\n",
        "        PyTorch model (or TensorFlow/Keras equivalent)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        \n",
        "        class LSTMSpikeDetector(nn.Module):\n",
        "            def __init__(self, input_size=1, hidden_size=64, dropout=0.3):\n",
        "                super().__init__()\n",
        "                self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "                self.dropout = nn.Dropout(dropout)\n",
        "                self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "                self.fc = nn.Linear(hidden_size, 1)\n",
        "                self.sigmoid = nn.Sigmoid()\n",
        "            \n",
        "            def forward(self, x):\n",
        "                # x: (batch, window_size, 1)\n",
        "                out, _ = self.lstm1(x)\n",
        "                out = self.dropout(out)\n",
        "                out, _ = self.lstm2(out)\n",
        "                # Take output at last time step\n",
        "                out = self.fc(out[:, -1, :])\n",
        "                return self.sigmoid(out)\n",
        "        \n",
        "        return LSTMSpikeDetector(hidden_size=hidden_size, dropout=dropout)\n",
        "    \n",
        "    except ImportError:\n",
        "        # Keras/TensorFlow fallback\n",
        "        try:\n",
        "            from tensorflow import keras\n",
        "            from tensorflow.keras import layers\n",
        "            \n",
        "            model = keras.Sequential([\n",
        "                layers.LSTM(hidden_size, return_sequences=True,\n",
        "                           input_shape=(window_size, 1)),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.LSTM(hidden_size),\n",
        "                layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "            return model\n",
        "        \n",
        "        except ImportError:\n",
        "            print(\"Neither PyTorch nor TensorFlow available\")\n",
        "            return None\n",
        "\n",
        "def detect_spikes_lstm(trace, model, fps=300, window_size=41, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Detect spikes using trained LSTM model.\n",
        "    \n",
        "    Args:\n",
        "        trace: Î”F/F trace (n_frames,)\n",
        "        model: trained LSTM spike detector\n",
        "        fps: frame rate (300 Hz in Kawashima paper)\n",
        "        window_size: input window size\n",
        "        threshold: probability threshold for spike detection\n",
        "    \n",
        "    Returns:\n",
        "        spike_times: array of spike times in seconds\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    \n",
        "    n_frames = len(trace)\n",
        "    half_window = window_size // 2\n",
        "    \n",
        "    # Normalize trace\n",
        "    trace_norm = (trace - np.mean(trace)) / (np.std(trace) + 1e-10)\n",
        "    \n",
        "    # Slide window across trace\n",
        "    spike_probs = np.zeros(n_frames)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(half_window, n_frames - half_window):\n",
        "            window = trace_norm[i - half_window:i + half_window + 1]\n",
        "            window_tensor = torch.FloatTensor(window).unsqueeze(0).unsqueeze(-1)\n",
        "            \n",
        "            prob = model(window_tensor).item()\n",
        "            spike_probs[i] = prob\n",
        "    \n",
        "    # Find peaks above threshold\n",
        "    from scipy.signal import find_peaks\n",
        "    peaks, _ = find_peaks(spike_probs, height=threshold, distance=int(0.002 * fps))\n",
        "    \n",
        "    spike_times = peaks / fps\n",
        "    return spike_times, spike_probs\n",
        "\n",
        "def train_lstm_spike_detector(traces, ground_truth_spikes, fps=300,\n",
        "                               window_size=41, epochs=100, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train LSTM spike detector on paired imaging + electrophysiology data.\n",
        "    \n",
        "    Args:\n",
        "        traces: list of Î”F/F traces\n",
        "        ground_truth_spikes: list of spike frame indices from electrophysiology\n",
        "        fps: frame rate\n",
        "        window_size: input window size\n",
        "        epochs: training epochs\n",
        "        batch_size: training batch size\n",
        "    \n",
        "    Returns:\n",
        "        trained model\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "    \n",
        "    # Create training data\n",
        "    X = []\n",
        "    y = []\n",
        "    half_window = window_size // 2\n",
        "    \n",
        "    for trace, spikes in zip(traces, ground_truth_spikes):\n",
        "        trace_norm = (trace - np.mean(trace)) / (np.std(trace) + 1e-10)\n",
        "        spike_set = set(spikes)\n",
        "        \n",
        "        for i in range(half_window, len(trace) - half_window):\n",
        "            window = trace_norm[i - half_window:i + half_window + 1]\n",
        "            label = 1.0 if i in spike_set else 0.0\n",
        "            X.append(window)\n",
        "            y.append(label)\n",
        "    \n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Balance classes (spike vs no-spike)\n",
        "    spike_idx = np.where(y == 1)[0]\n",
        "    no_spike_idx = np.where(y == 0)[0]\n",
        "    n_samples = min(len(spike_idx), len(no_spike_idx))\n",
        "    \n",
        "    balanced_idx = np.concatenate([\n",
        "        spike_idx[:n_samples],\n",
        "        np.random.choice(no_spike_idx, n_samples, replace=False)\n",
        "    ])\n",
        "    np.random.shuffle(balanced_idx)\n",
        "    \n",
        "    X = X[balanced_idx]\n",
        "    y = y[balanced_idx]\n",
        "    \n",
        "    # Create model and train\n",
        "    model = create_lstm_spike_detector(window_size=window_size)\n",
        "    \n",
        "    X_tensor = torch.FloatTensor(X).unsqueeze(-1)\n",
        "    y_tensor = torch.FloatTensor(y).unsqueeze(-1)\n",
        "    \n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: loss = {total_loss/len(loader):.4f}\")\n",
        "    \n",
        "    return model\n",
        "```\n",
        "\n",
        "#### Step 6: Subthreshold Activity Estimation\n",
        "```python\n",
        "def extract_subthreshold_kawashima(trace, spike_frames, fps=300,\n",
        "                                    median_window_ms=70, clip_window_frames=2):\n",
        "    \"\"\"\n",
        "    Estimate subthreshold activity using rolling median.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    - Rolling median filter with 70 ms window\n",
        "    - Clip frames around spikes (-1 to +1 frames) to avoid spike nonlinearity\n",
        "    \n",
        "    Args:\n",
        "        trace: Î”F/F trace\n",
        "        spike_frames: detected spike frame indices\n",
        "        fps: frame rate\n",
        "        median_window_ms: median filter window (70 ms in paper)\n",
        "        clip_window_frames: frames to clip around spikes (1 in paper)\n",
        "    \n",
        "    Returns:\n",
        "        subthreshold: subthreshold membrane potential estimate\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import median_filter\n",
        "    \n",
        "    n_frames = len(trace)\n",
        "    \n",
        "    # Create mask of frames to exclude (around spikes)\n",
        "    valid_mask = np.ones(n_frames, dtype=bool)\n",
        "    for spike_frame in spike_frames:\n",
        "        start = max(0, spike_frame - clip_window_frames)\n",
        "        end = min(n_frames, spike_frame + clip_window_frames + 1)\n",
        "        valid_mask[start:end] = False\n",
        "    \n",
        "    # Interpolate over clipped regions\n",
        "    trace_interpolated = trace.copy()\n",
        "    if not valid_mask.all():\n",
        "        valid_indices = np.where(valid_mask)[0]\n",
        "        invalid_indices = np.where(~valid_mask)[0]\n",
        "        \n",
        "        if len(valid_indices) > 2:\n",
        "            from scipy.interpolate import interp1d\n",
        "            f = interp1d(valid_indices, trace[valid_indices],\n",
        "                        kind='linear', bounds_error=False,\n",
        "                        fill_value='extrapolate')\n",
        "            trace_interpolated[invalid_indices] = f(invalid_indices)\n",
        "    \n",
        "    # Apply rolling median filter\n",
        "    window_samples = int(median_window_ms * fps / 1000)\n",
        "    window_samples = window_samples if window_samples % 2 == 1 else window_samples + 1\n",
        "    \n",
        "    subthreshold = median_filter(trace_interpolated, size=window_samples)\n",
        "    \n",
        "    return subthreshold\n",
        "```\n",
        "\n",
        "#### Î”F/F Computation (Kawashima method)\n",
        "```python\n",
        "def compute_dff_kawashima(trace, fps=300, baseline_percentile=20,\n",
        "                           baseline_window_sec=180):\n",
        "    \"\"\"\n",
        "    Compute Î”F/F using running percentile baseline.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    Î”F/F = (F - F0) / F0\n",
        "    where F0 is running 20th percentile within 3-minute window\n",
        "    \n",
        "    For Voltron (negative indicator): use -Î”F/F for analysis\n",
        "    \n",
        "    Args:\n",
        "        trace: raw fluorescence trace\n",
        "        fps: frame rate\n",
        "        baseline_percentile: percentile for baseline (20 in paper)\n",
        "        baseline_window_sec: window size in seconds (180 = 3 minutes)\n",
        "    \n",
        "    Returns:\n",
        "        dff: Î”F/F trace\n",
        "        dff_inverted: -Î”F/F (for Voltron analysis)\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import percentile_filter\n",
        "    \n",
        "    window_samples = int(baseline_window_sec * fps)\n",
        "    \n",
        "    # Running percentile baseline\n",
        "    F0 = percentile_filter(trace, baseline_percentile, size=window_samples)\n",
        "    \n",
        "    # Compute Î”F/F\n",
        "    dff = (trace - F0) / (F0 + 1e-10)\n",
        "    \n",
        "    # Inverted for Voltron (negative indicator)\n",
        "    dff_inverted = -dff\n",
        "    \n",
        "    return dff, dff_inverted\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4LpF8enkAAi"
      },
      "source": [
        "#### Behavioral Kernel Fitting (GLM for spike prediction)\n",
        "```python\n",
        "def fit_spike_glm_kernels(spike_times, behavior_data, fps=300,\n",
        "                          history_sec=1.0, l2_reg=0.01):\n",
        "    \"\"\"\n",
        "    Fit GLM kernels to predict spikes from behavioral variables.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    P(spike at time t) = Binomial(w_s^T * S_t + w_v^T * V_t - w_sp^T * SP_t)\n",
        "    \n",
        "    Where:\n",
        "    - S_t: swim vigor history\n",
        "    - V_t: visual input history  \n",
        "    - SP_t: recent spike history\n",
        "    - w_s, w_v, w_sp: learned kernels\n",
        "    \n",
        "    Args:\n",
        "        spike_times: array of spike times in seconds\n",
        "        behavior_data: dict with 'swim_vigor', 'visual_input' arrays\n",
        "        fps: frame rate\n",
        "        history_sec: history window for kernels (1 second in paper)\n",
        "        l2_reg: L2 regularization strength\n",
        "    \n",
        "    Returns:\n",
        "        kernels: dict with fitted w_s, w_v, w_sp\n",
        "        model: fitted sklearn LogisticRegression\n",
        "        performance: explained variance on validation set\n",
        "    \"\"\"\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    \n",
        "    n_history = int(history_sec * fps)  # 300 time points for 1 sec at 300 Hz\n",
        "    \n",
        "    swim_vigor = behavior_data.get('swim_vigor', np.zeros(10000))\n",
        "    visual_input = behavior_data.get('visual_input', np.zeros(10000))\n",
        "    n_frames = len(swim_vigor)\n",
        "    \n",
        "    # Create spike raster\n",
        "    spike_raster = np.zeros(n_frames)\n",
        "    spike_frames = (np.array(spike_times) * fps).astype(int)\n",
        "    spike_frames = spike_frames[(spike_frames >= 0) & (spike_frames < n_frames)]\n",
        "    spike_raster[spike_frames] = 1\n",
        "    \n",
        "    # Build design matrix\n",
        "    # Use sqrt of swim vigor (as in paper)\n",
        "    swim_vigor_sqrt = np.sqrt(np.abs(swim_vigor)) * np.sign(swim_vigor)\n",
        "    \n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for t in range(n_history, n_frames):\n",
        "        # Swim vigor history\n",
        "        s_t = swim_vigor_sqrt[t-n_history:t]\n",
        "        # Visual input history\n",
        "        v_t = visual_input[t-n_history:t]\n",
        "        # Spike history\n",
        "        sp_t = spike_raster[t-n_history:t]\n",
        "        \n",
        "        features = np.concatenate([s_t, v_t, sp_t])\n",
        "        X.append(features)\n",
        "        y.append(spike_raster[t])\n",
        "    \n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Balance classes (sample around spike events)\n",
        "    spike_idx = np.where(y == 1)[0]\n",
        "    no_spike_idx = np.where(y == 0)[0]\n",
        "    \n",
        "    # Sample to balance ~50/50\n",
        "    n_samples = min(len(spike_idx) * 2, len(no_spike_idx))\n",
        "    balanced_no_spike = np.random.choice(no_spike_idx, n_samples, replace=False)\n",
        "    balanced_idx = np.concatenate([spike_idx, balanced_no_spike])\n",
        "    np.random.shuffle(balanced_idx)\n",
        "    \n",
        "    X_balanced = X[balanced_idx]\n",
        "    y_balanced = y[balanced_idx]\n",
        "    \n",
        "    # Fit logistic regression with L2 regularization\n",
        "    model = LogisticRegression(penalty='l2', C=1/l2_reg, max_iter=1000)\n",
        "    model.fit(X_balanced, y_balanced)\n",
        "    \n",
        "    # Extract kernels\n",
        "    coef = model.coef_[0]\n",
        "    kernels = {\n",
        "        'swim_vigor': coef[:n_history],\n",
        "        'visual_input': coef[n_history:2*n_history],\n",
        "        'spike_history': coef[2*n_history:]\n",
        "    }\n",
        "    \n",
        "    # Cross-validation performance\n",
        "    cv_scores = cross_val_score(model, X_balanced, y_balanced, cv=5, scoring='roc_auc')\n",
        "    \n",
        "    return kernels, model, {'cv_auc': cv_scores.mean(), 'cv_std': cv_scores.std()}\n",
        "\n",
        "def plot_behavioral_kernels(kernels, fps=300):\n",
        "    \"\"\"Plot fitted behavioral kernels.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
        "    \n",
        "    time_ms = np.arange(len(kernels['swim_vigor'])) * 1000 / fps - len(kernels['swim_vigor']) * 1000 / fps\n",
        "    \n",
        "    axes[0].plot(time_ms, kernels['swim_vigor'])\n",
        "    axes[0].set_xlabel('Time before spike (ms)')\n",
        "    axes[0].set_ylabel('Weight')\n",
        "    axes[0].set_title('Swim Vigor Kernel')\n",
        "    axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    axes[1].plot(time_ms, kernels['visual_input'])\n",
        "    axes[1].set_xlabel('Time before spike (ms)')\n",
        "    axes[1].set_title('Visual Input Kernel')\n",
        "    axes[1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    axes[2].plot(time_ms, -kernels['spike_history'])  # Negative because it's subtracted\n",
        "    axes[2].set_xlabel('Time before spike (ms)')\n",
        "    axes[2].set_title('Spike History Kernel (refractory)')\n",
        "    axes[2].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "```\n",
        "\n",
        "#### Population Coding Analysis (sparse LDA)\n",
        "```python\n",
        "def compute_population_coding(neural_activity, conditions, l2_gamma=0.5):\n",
        "    \"\"\"\n",
        "    Compute population coding direction using sparse linear discriminant analysis.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    l = argmin_l -(l^T (r_high - r_low))^2 / (l^T Î£_r l)\n",
        "    \n",
        "    With regularized covariance: Î£_r = (1-Î³) * cov(r) + Î³ * I\n",
        "    \n",
        "    Args:\n",
        "        neural_activity: (n_neurons, n_timepoints) firing rates\n",
        "        conditions: array of condition labels (e.g., 'high' or 'low')\n",
        "        l2_gamma: regularization parameter [0, 1]\n",
        "    \n",
        "    Returns:\n",
        "        coding_direction: (n_neurons,) vector\n",
        "        explained_variance: fraction of variance explained\n",
        "    \"\"\"\n",
        "    unique_conditions = np.unique(conditions)\n",
        "    if len(unique_conditions) != 2:\n",
        "        raise ValueError(\"Exactly 2 conditions required\")\n",
        "    \n",
        "    cond1, cond2 = unique_conditions\n",
        "    \n",
        "    # Mean activity per condition\n",
        "    r_cond1 = neural_activity[:, conditions == cond1].mean(axis=1)\n",
        "    r_cond2 = neural_activity[:, conditions == cond2].mean(axis=1)\n",
        "    \n",
        "    # Regularized covariance\n",
        "    r_centered = neural_activity - neural_activity.mean(axis=1, keepdims=True)\n",
        "    cov_r = np.cov(r_centered)\n",
        "    n_neurons = cov_r.shape[0]\n",
        "    \n",
        "    sigma_r = (1 - l2_gamma) * cov_r + l2_gamma * np.eye(n_neurons)\n",
        "    \n",
        "    # Solve for coding direction (Fisher LDA)\n",
        "    # l = Î£_r^-1 (r_high - r_low)\n",
        "    diff = r_cond2 - r_cond1\n",
        "    coding_direction = np.linalg.solve(sigma_r, diff)\n",
        "    \n",
        "    # Normalize\n",
        "    coding_direction /= np.linalg.norm(coding_direction)\n",
        "    \n",
        "    # Compute explained variance\n",
        "    projected = coding_direction @ neural_activity\n",
        "    proj_cond1 = projected[conditions == cond1]\n",
        "    proj_cond2 = projected[conditions == cond2]\n",
        "    \n",
        "    between_var = (proj_cond1.mean() - proj_cond2.mean())**2\n",
        "    within_var = proj_cond1.var() + proj_cond2.var()\n",
        "    explained_variance = between_var / (between_var + within_var + 1e-10)\n",
        "    \n",
        "    return coding_direction, explained_variance\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}