{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtDUMteVkAAY"
      },
      "source": [
        "# Voltage Imaging Analysis Benchmark\n",
        "\n",
        "## For Empirical Code Evaluation\n",
        "\n",
        "This notebook defines a benchmark for automated optimization of voltage imaging analysis pipelines, suitable for empirical code evaluation frameworks that iteratively search for high-performing code variants.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/voltage-imaging-benchmark/blob/main/voltage_imaging_benchmark.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epn8mViAkAAa"
      },
      "source": [
        "---\n",
        "# 0. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_5ek69vkAAa",
        "outputId": "c0d4611e-fc90-4d5b-9463-7c9123c25fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.5/234.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies (run this cell in Colab or if packages are missing)\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install -q numpy scipy matplotlib scikit-image scikit-learn pandas seaborn tifffile nd2 opencv-python gdown umap-learn hdbscan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scAWy9MkkAAb"
      },
      "source": [
        "## 0.2 Download Benchmark Data\n",
        "\n",
        "The benchmark data is hosted on Google Drive. Run the cell below to download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXWTXuXTkAAb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# === CONFIGURE YOUR GOOGLE DRIVE FILE ID HERE ===\n",
        "# To get the file ID from a Google Drive sharing link:\n",
        "# https://drive.google.com/file/d/FILE_ID_HERE/view?usp=sharing\n",
        "#                                  ^^^^^^^^^^^^ copy this part\n",
        "\n",
        "GDRIVE_FILE_ID = \"156ASrvQfbyjrHwtAeCCt_dTuWcktS9Cu\"  # Replace with your actual file ID\n",
        "DATA_DIR = Path(\"./data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def download_data(file_id, output_dir):\n",
        "    \"\"\"Download and extract benchmark data from Google Drive.\"\"\"\n",
        "    import gdown\n",
        "    import zipfile\n",
        "\n",
        "    zip_path = output_dir / \"benchmark_data.zip\"\n",
        "\n",
        "    # Check if data already exists\n",
        "    if (output_dir / \"video.tif\").exists() or (output_dir / \"fish1\").exists():\n",
        "        print(\"Data already downloaded. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Download from Google Drive\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    print(f\"Downloading data from Google Drive...\")\n",
        "    gdown.download(url, str(zip_path), quiet=False)\n",
        "\n",
        "    # Extract if it's a zip file\n",
        "    if zipfile.is_zipfile(zip_path):\n",
        "        print(\"Extracting data...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(output_dir)\n",
        "        zip_path.unlink()  # Remove zip after extraction\n",
        "        print(\"Done!\")\n",
        "    else:\n",
        "        # Might be a single TIFF file\n",
        "        print(\"Downloaded file (not a zip archive)\")\n",
        "\n",
        "# Uncomment to download:\n",
        "# download_data(GDRIVE_FILE_ID, DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzqWpRq0kAAc"
      },
      "source": [
        "## 0.3 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad-_5i6TkAAc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage, signal\n",
        "from skimage import measure, morphology\n",
        "import tifffile\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# Set plotting style\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Ind1D7kAAc"
      },
      "source": [
        "---\n",
        "# 1. Problem Definition\n",
        "\n",
        "## 1.1 Scientific Context\n",
        "\n",
        "**Voltage imaging** enables recording of neuronal electrical activity at high temporal resolution (~200 Hz) across many neurons simultaneously. Unlike calcium imaging, voltage indicators directly report membrane potential changes, capturing:\n",
        "- **Action potentials (spikes)** - fast (~1-5ms) electrical events\n",
        "- **Subthreshold activity** - slower membrane potential fluctuations\n",
        "\n",
        "## 1.2 The Data\n",
        "\n",
        "| Property | Value |\n",
        "|----------|-------|\n",
        "| **Organism** | Zebrafish (larval) |\n",
        "| **Indicator** | Voltron-2 (soma-targeted) |\n",
        "| **Frame rate** | ~200 Hz (5 ms/frame) |\n",
        "| **File format** | ND2 (Nikon) → TIFF |\n",
        "| **File size** | 2-10 GB per recording |\n",
        "| **Duration** | 1,000-10,000 frames (5-50 seconds) |\n",
        "| **Neurons per FOV** | 10-100 |\n",
        "| **Neuron size** | ~5-10 µm (~9 pixels) |\n",
        "| **Neuron appearance** | Ring-like (membrane labeling) |\n",
        "\n",
        "## 1.3 Signal Characteristics\n",
        "\n",
        "**Important**: Voltron-2 produces **NEGATIVE deflections** during action potentials.\n",
        "\n",
        "| Feature | Typical Value |\n",
        "|---------|---------------|\n",
        "| Spike duration | 1-5 ms (1-2 frames at 200 Hz) |\n",
        "| Spike amplitude | 2-10% ΔF/F |\n",
        "| Spike polarity | **Negative** (downward) |\n",
        "| Noise type | Broadband (camera/shot noise) |\n",
        "| SNR | Variable, typically 2-10 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDdS22vkAAd"
      },
      "source": [
        "---\n",
        "# 2. Task Specification\n",
        "\n",
        "## 2.1 Overall Goal\n",
        "\n",
        "Given a raw voltage imaging video, produce:\n",
        "1. **ROI masks** - binary masks identifying each neuron\n",
        "2. **Spike times** - timestamps of detected action potentials for each neuron\n",
        "3. **Voltage traces** - cleaned ΔF/F time series for each neuron\n",
        "\n",
        "## 2.2 Pipeline Components (Optimization Targets)\n",
        "\n",
        "The analysis pipeline consists of sequential processing stages. Each stage can be independently optimized:\n",
        "\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────────┐\n",
        "│                    OPTIMIZATION TARGETS                             │\n",
        "├─────────────────────────────────────────────────────────────────────┤\n",
        "│                                                                     │\n",
        "│  STAGE 1: MOTION CORRECTION                                         │\n",
        "│  ├── Method: rigid vs non-rigid                                    │\n",
        "│  ├── Reference: first frame, mean, median, specific frame          │\n",
        "│  ├── Algorithm: phase correlation, template matching, optical flow │\n",
        "│  └── Parameters: max_shift, upsample_factor, smoothing             │\n",
        "│                                                                     │\n",
        "│  STAGE 2: DENOISING                                                 │\n",
        "│  ├── Method: none, temporal filter, spatial filter, PCA, wavelet   │\n",
        "│  ├── Temporal: lowpass cutoff, Savitzky-Golay window, median       │\n",
        "│  ├── Spatial: Gaussian sigma, bilateral filter                     │\n",
        "│  └── PCA: local vs global, number of components, patch size        │\n",
        "│                                                                     │\n",
        "│  STAGE 3: ROI SEGMENTATION                                          │\n",
        "│  ├── Method: threshold, watershed, CNN (Mask R-CNN), NMF           │\n",
        "│  ├── Features: std projection, correlation image, PCA components   │\n",
        "│  ├── Constraints: min/max area, circularity, ring-like shape       │\n",
        "│  └── Post-processing: merge overlapping, remove duplicates         │\n",
        "│                                                                     │\n",
        "│  STAGE 4: TRACE EXTRACTION                                          │\n",
        "│  ├── Aggregation: mean, median, weighted by distance               │\n",
        "│  ├── Background: none, annulus subtraction, neuropil coefficient   │\n",
        "│  ├── Baseline: percentile (which?), rolling window size            │\n",
        "│  └── Detrending: none, linear, polynomial, exponential             │\n",
        "│                                                                     │\n",
        "│  STAGE 5: SPIKE DETECTION                                           │\n",
        "│  ├── Method: threshold, template matching, deconvolution, ML       │\n",
        "│  ├── Threshold: fixed std, adaptive, percentile-based              │\n",
        "│  ├── Constraints: min spike width, refractory period               │\n",
        "│  └── Post-processing: amplitude filter, artifact rejection         │\n",
        "│                                                                     │\n",
        "└─────────────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "## 2.3 Input Format\n",
        "\n",
        "```python\n",
        "# Input: 3D numpy array\n",
        "video: np.ndarray  # shape: (n_frames, height, width), dtype: uint16 or float32\n",
        "fps: float  # frame rate in Hz (typically 200)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6pfF4fkAAd"
      },
      "source": [
        "---\n",
        "# 3. Challenges and Failure Modes\n",
        "\n",
        "## 3.1 Motion Artifacts\n",
        "\n",
        "**Problem**: The animal moves during imaging, causing:\n",
        "- Rigid translation (x/y shifts)\n",
        "- Non-rigid deformation (tissue stretching)\n",
        "- Focus changes (neurons going in/out of focus)\n",
        "- ROI contamination (wrong pixels included after movement)\n",
        "\n",
        "**Failure modes**:\n",
        "- Correlated \"spikes\" across many neurons (motion artifact detected as spike)\n",
        "- Lost neurons (moved out of ROI)\n",
        "- False spikes from intensity changes due to focus shifts\n",
        "\n",
        "## 3.2 Segmentation Challenges\n",
        "\n",
        "**Problem**: Neurons have irregular shapes:\n",
        "- Ring-like appearance (soma-targeted membrane labeling)\n",
        "- Variable brightness\n",
        "- Overlapping neurons\n",
        "- Similar size to noise blobs\n",
        "\n",
        "**Failure modes**:\n",
        "- Merging adjacent neurons into one ROI\n",
        "- Splitting one neuron into multiple ROIs\n",
        "- Including non-neuronal structures\n",
        "- Missing dim neurons\n",
        "\n",
        "## 3.3 Spike Detection Challenges\n",
        "\n",
        "**Problem**: Spikes are fast and small:\n",
        "- Only 1-2 frames wide at 200 Hz\n",
        "- Low SNR (amplitude similar to noise)\n",
        "- Variable amplitude across neurons and within same neuron\n",
        "- Negative polarity (opposite to calcium indicators)\n",
        "\n",
        "**Failure modes**:\n",
        "- Missing true spikes (false negatives)\n",
        "- Detecting noise as spikes (false positives)\n",
        "- Correlated false detections from global artifacts\n",
        "\n",
        "## 3.4 Known Artifacts\n",
        "\n",
        "| Artifact | Cause | Detection Method |\n",
        "|----------|-------|------------------|\n",
        "| Blood cell shadows | Blood flow | Moving dark spots |\n",
        "| Excitation variation | Laser fluctuation | Correlated intensity changes |\n",
        "| Photobleaching | Dye degradation | Slow intensity decay |\n",
        "| Focus drift | Mechanical drift | Blur changes over time |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C65VVw2kAAd"
      },
      "source": [
        "---\n",
        "# 4. Evaluation Metrics\n",
        "\n",
        "## 4.1 Ground Truth Comparison (Supervised)\n",
        "\n",
        "When expert annotations are available:\n",
        "\n",
        "### 4.1.1 ROI Segmentation Metrics\n",
        "\n",
        "```python\n",
        "def evaluate_segmentation(predicted_masks, ground_truth_masks, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compare predicted ROIs to ground truth.\n",
        "    \n",
        "    Returns:\n",
        "        precision: fraction of predicted ROIs that match ground truth\n",
        "        recall: fraction of ground truth ROIs that were found\n",
        "        f1: harmonic mean of precision and recall\n",
        "        mean_iou: mean intersection-over-union for matched ROIs\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 4.1.2 Spike Detection Metrics\n",
        "\n",
        "```python\n",
        "def evaluate_spikes(predicted_times, ground_truth_times, tolerance_ms=5):\n",
        "    \"\"\"\n",
        "    Compare predicted spike times to ground truth.\n",
        "    \n",
        "    Args:\n",
        "        tolerance_ms: maximum time difference to count as match\n",
        "    \n",
        "    Returns:\n",
        "        precision: fraction of predicted spikes that are true\n",
        "        recall: fraction of true spikes that were detected\n",
        "        f1: harmonic mean of precision and recall\n",
        "        timing_error: mean timing error for matched spikes (ms)\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "## 4.2 Automated Validation (Unsupervised)\n",
        "\n",
        "Metrics that don't require ground truth annotations:\n",
        "\n",
        "### 4.2.1 Shuffled Control Test\n",
        "\n",
        "**Principle**: Spike detection on temporally shuffled data should yield far fewer spikes than real data.\n",
        "\n",
        "```python\n",
        "def shuffled_control_score(trace, spike_detector, n_shuffles=100):\n",
        "    \"\"\"\n",
        "    Compare spike detection on real vs shuffled traces.\n",
        "    \n",
        "    Shuffling methods:\n",
        "    - Frame shuffle: random permutation of frames\n",
        "    - Circular shuffle: roll trace by random offset\n",
        "    - Block shuffle: shuffle blocks of ~100ms\n",
        "    \n",
        "    Returns:\n",
        "        spike_ratio: real_spikes / mean(shuffled_spikes)\n",
        "        false_positive_rate: estimated FP rate from shuffled data\n",
        "        p_value: probability that real spikes are due to chance\n",
        "    \"\"\"\n",
        "    real_spikes = spike_detector(trace)\n",
        "    shuffled_spikes = []\n",
        "    \n",
        "    for _ in range(n_shuffles):\n",
        "        shuffled_trace = np.random.permutation(trace)\n",
        "        shuffled_spikes.append(len(spike_detector(shuffled_trace)))\n",
        "    \n",
        "    spike_ratio = len(real_spikes) / (np.mean(shuffled_spikes) + 1)\n",
        "    return spike_ratio\n",
        "```\n",
        "\n",
        "**Interpretation**:\n",
        "- `spike_ratio >> 1`: Good, detecting real structure\n",
        "- `spike_ratio ≈ 1`: Bad, just detecting noise\n",
        "\n",
        "### 4.2.2 Spike Template Consistency\n",
        "\n",
        "**Principle**: Spikes from the same neuron should have similar waveform shapes.\n",
        "\n",
        "```python\n",
        "def template_consistency_score(trace, spike_times, window_ms=10, fps=200):\n",
        "    \"\"\"\n",
        "    Measure consistency of spike waveforms within a neuron.\n",
        "    \n",
        "    Returns:\n",
        "        mean_correlation: mean pairwise correlation between spike waveforms\n",
        "        std_correlation: variability in spike shapes\n",
        "    \"\"\"\n",
        "    window_frames = int(window_ms * fps / 1000)\n",
        "    waveforms = []\n",
        "    \n",
        "    for t in spike_times:\n",
        "        idx = int(t * fps)\n",
        "        if idx >= window_frames and idx < len(trace) - window_frames:\n",
        "            waveforms.append(trace[idx-window_frames:idx+window_frames])\n",
        "    \n",
        "    if len(waveforms) < 2:\n",
        "        return 0, 0\n",
        "    \n",
        "    correlations = []\n",
        "    for i in range(len(waveforms)):\n",
        "        for j in range(i+1, len(waveforms)):\n",
        "            corr = np.corrcoef(waveforms[i], waveforms[j])[0, 1]\n",
        "            correlations.append(corr)\n",
        "    \n",
        "    return np.mean(correlations), np.std(correlations)\n",
        "```\n",
        "\n",
        "**Interpretation**:\n",
        "- `mean_correlation > 0.8`: Good, consistent spike shapes\n",
        "- `mean_correlation < 0.5`: Bad, likely detecting noise\n",
        "\n",
        "### 4.2.3 Inter-Neuron Correlation (Artifact Detection)\n",
        "\n",
        "**Principle**: High correlation in spike timing across many neurons suggests artifacts.\n",
        "\n",
        "```python\n",
        "def artifact_correlation_score(all_spike_times, n_rois, duration, bin_size_ms=10):\n",
        "    \"\"\"\n",
        "    Detect correlated spiking that suggests motion/illumination artifacts.\n",
        "    \n",
        "    Returns:\n",
        "        mean_pairwise_correlation: should be low for real data\n",
        "        synchrony_events: number of timepoints with >50% neurons spiking\n",
        "    \"\"\"\n",
        "    # Bin spikes\n",
        "    n_bins = int(duration * 1000 / bin_size_ms)\n",
        "    spike_matrix = np.zeros((n_rois, n_bins))\n",
        "    \n",
        "    for i, times in enumerate(all_spike_times):\n",
        "        for t in times:\n",
        "            bin_idx = int(t * 1000 / bin_size_ms)\n",
        "            if bin_idx < n_bins:\n",
        "                spike_matrix[i, bin_idx] = 1\n",
        "    \n",
        "    # Compute pairwise correlations\n",
        "    corr_matrix = np.corrcoef(spike_matrix)\n",
        "    upper_tri = corr_matrix[np.triu_indices(n_rois, k=1)]\n",
        "    \n",
        "    # Count synchrony events\n",
        "    fraction_active = spike_matrix.sum(axis=0) / n_rois\n",
        "    synchrony_events = np.sum(fraction_active > 0.5)\n",
        "    \n",
        "    return np.nanmean(upper_tri), synchrony_events\n",
        "```\n",
        "\n",
        "**Interpretation**:\n",
        "- `mean_correlation < 0.1`: Good, neurons fire independently\n",
        "- `mean_correlation > 0.3`: Bad, likely motion artifacts\n",
        "- `synchrony_events > 0`: Suspicious, check these timepoints\n",
        "\n",
        "### 4.2.4 ROI Quality Metrics\n",
        "\n",
        "```python\n",
        "def roi_quality_score(roi_mask, std_projection):\n",
        "    \"\"\"\n",
        "    Assess ROI quality based on morphological and intensity features.\n",
        "    \n",
        "    Returns:\n",
        "        size_score: 1 if within expected range (5-10 µm), 0 otherwise\n",
        "        shape_score: circularity / ring-likeness\n",
        "        snr_score: signal-to-noise in ROI region\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 4.2.5 Physiological Plausibility\n",
        "\n",
        "```python\n",
        "def physiological_plausibility_score(spike_times, fps=200):\n",
        "    \"\"\"\n",
        "    Check if detected spikes are physiologically plausible.\n",
        "    \n",
        "    Returns:\n",
        "        refractory_violations: spikes closer than 2ms (should be 0)\n",
        "        firing_rate_plausible: True if rate is 0.1-50 Hz\n",
        "        isi_cv: coefficient of variation of inter-spike intervals\n",
        "    \"\"\"\n",
        "    if len(spike_times) < 2:\n",
        "        return 0, True, 0\n",
        "    \n",
        "    isis = np.diff(spike_times) * 1000  # Convert to ms\n",
        "    refractory_violations = np.sum(isis < 2)  # 2ms refractory period\n",
        "    \n",
        "    duration = spike_times[-1] - spike_times[0]\n",
        "    firing_rate = len(spike_times) / duration if duration > 0 else 0\n",
        "    firing_rate_plausible = 0.1 < firing_rate < 50\n",
        "    \n",
        "    isi_cv = np.std(isis) / np.mean(isis) if len(isis) > 0 else 0\n",
        "    \n",
        "    return refractory_violations, firing_rate_plausible, isi_cv\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPvReYLakAAe"
      },
      "source": [
        "---\n",
        "# 5. Composite Evaluation Score\n",
        "\n",
        "## 5.1 Weighted Score Function\n",
        "\n",
        "Combine all metrics into a single optimization target:\n",
        "\n",
        "```python\n",
        "def compute_benchmark_score(results, video, fps, ground_truth=None):\n",
        "    \"\"\"\n",
        "    Compute overall benchmark score for a pipeline's output.\n",
        "    \n",
        "    Args:\n",
        "        results: dict with roi_masks, traces, spike_times\n",
        "        video: original video array\n",
        "        fps: frame rate\n",
        "        ground_truth: optional dict with GT masks and spikes\n",
        "    \n",
        "    Returns:\n",
        "        score: float in [0, 1], higher is better\n",
        "        details: dict with individual metric scores\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    \n",
        "    # === UNSUPERVISED METRICS (always computed) ===\n",
        "    \n",
        "    # 1. Shuffled control (weight: 0.2)\n",
        "    shuffle_ratios = []\n",
        "    for i, trace in enumerate(results['traces']):\n",
        "        ratio = shuffled_control_score(trace, spike_detector)\n",
        "        shuffle_ratios.append(ratio)\n",
        "    scores['shuffle_ratio'] = np.median(shuffle_ratios)\n",
        "    # Normalize: ratio of 10 = perfect score\n",
        "    scores['shuffle_score'] = min(1.0, scores['shuffle_ratio'] / 10)\n",
        "    \n",
        "    # 2. Template consistency (weight: 0.15)\n",
        "    consistencies = []\n",
        "    for i, trace in enumerate(results['traces']):\n",
        "        if len(results['spike_times'][i]) >= 3:\n",
        "            corr, _ = template_consistency_score(trace, results['spike_times'][i])\n",
        "            consistencies.append(corr)\n",
        "    scores['template_consistency'] = np.mean(consistencies) if consistencies else 0\n",
        "    \n",
        "    # 3. Artifact correlation (weight: 0.15)\n",
        "    corr, sync = artifact_correlation_score(\n",
        "        results['spike_times'],\n",
        "        len(results['roi_masks']),\n",
        "        video.shape[0] / fps\n",
        "    )\n",
        "    # Lower correlation is better\n",
        "    scores['artifact_score'] = max(0, 1 - corr * 3)  # corr > 0.33 = 0 score\n",
        "    \n",
        "    # 4. Physiological plausibility (weight: 0.1)\n",
        "    violations = 0\n",
        "    plausible_count = 0\n",
        "    for times in results['spike_times']:\n",
        "        v, p, _ = physiological_plausibility_score(times, fps)\n",
        "        violations += v\n",
        "        plausible_count += int(p)\n",
        "    scores['physiology_score'] = plausible_count / len(results['spike_times'])\n",
        "    scores['refractory_violations'] = violations\n",
        "    \n",
        "    # 5. ROI count reasonableness (weight: 0.05)\n",
        "    n_rois = len(results['roi_masks'])\n",
        "    # Expect 10-100 ROIs\n",
        "    if 10 <= n_rois <= 100:\n",
        "        scores['roi_count_score'] = 1.0\n",
        "    elif 5 <= n_rois <= 150:\n",
        "        scores['roi_count_score'] = 0.5\n",
        "    else:\n",
        "        scores['roi_count_score'] = 0.0\n",
        "    \n",
        "    # === SUPERVISED METRICS (if ground truth available) ===\n",
        "    \n",
        "    if ground_truth is not None:\n",
        "        # 6. ROI segmentation F1 (weight: 0.15)\n",
        "        seg_metrics = evaluate_segmentation(\n",
        "            results['roi_masks'],\n",
        "            ground_truth['roi_masks']\n",
        "        )\n",
        "        scores['segmentation_f1'] = seg_metrics['f1']\n",
        "        \n",
        "        # 7. Spike detection F1 (weight: 0.2)\n",
        "        spike_f1s = []\n",
        "        for i in range(len(ground_truth['spike_times'])):\n",
        "            if i < len(results['spike_times']):\n",
        "                metrics = evaluate_spikes(\n",
        "                    results['spike_times'][i],\n",
        "                    ground_truth['spike_times'][i]\n",
        "                )\n",
        "                spike_f1s.append(metrics['f1'])\n",
        "        scores['spike_f1'] = np.mean(spike_f1s) if spike_f1s else 0\n",
        "        \n",
        "        # Compute weighted score with GT\n",
        "        total_score = (\n",
        "            0.15 * scores['shuffle_score'] +\n",
        "            0.10 * scores['template_consistency'] +\n",
        "            0.10 * scores['artifact_score'] +\n",
        "            0.05 * scores['physiology_score'] +\n",
        "            0.05 * scores['roi_count_score'] +\n",
        "            0.25 * scores['segmentation_f1'] +\n",
        "            0.30 * scores['spike_f1']\n",
        "        )\n",
        "    else:\n",
        "        # Compute weighted score without GT\n",
        "        total_score = (\n",
        "            0.35 * scores['shuffle_score'] +\n",
        "            0.25 * scores['template_consistency'] +\n",
        "            0.25 * scores['artifact_score'] +\n",
        "            0.10 * scores['physiology_score'] +\n",
        "            0.05 * scores['roi_count_score']\n",
        "        )\n",
        "    \n",
        "    return total_score, scores\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx94Y_ibkAAe"
      },
      "source": [
        "---\n",
        "# 6. Baseline Implementation\n",
        "\n",
        "A simple baseline pipeline for comparison:\n",
        "\n",
        "```python\n",
        "def baseline_pipeline(video, fps=200):\n",
        "    \"\"\"\n",
        "    Simple baseline voltage imaging analysis pipeline.\n",
        "    \n",
        "    Args:\n",
        "        video: np.ndarray, shape (n_frames, height, width)\n",
        "        fps: frame rate in Hz\n",
        "    \n",
        "    Returns:\n",
        "        dict with roi_masks, traces, spike_times, spike_frames\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy import ndimage, signal\n",
        "    from skimage import measure, morphology\n",
        "    \n",
        "    n_frames, height, width = video.shape\n",
        "    \n",
        "    # === STAGE 1: No motion correction (baseline) ===\n",
        "    corrected = video\n",
        "    \n",
        "    # === STAGE 2: Simple denoising ===\n",
        "    # Temporal Gaussian smoothing\n",
        "    denoised = ndimage.gaussian_filter1d(corrected.astype(float), sigma=1, axis=0)\n",
        "    \n",
        "    # === STAGE 3: ROI segmentation ===\n",
        "    # Use std projection to find active regions\n",
        "    std_proj = np.std(denoised, axis=0)\n",
        "    \n",
        "    # Threshold\n",
        "    threshold = np.mean(std_proj) + 1.5 * np.std(std_proj)\n",
        "    binary = std_proj > threshold\n",
        "    \n",
        "    # Clean up\n",
        "    binary = morphology.remove_small_objects(binary, min_size=30)\n",
        "    binary = morphology.remove_small_holes(binary, area_threshold=30)\n",
        "    \n",
        "    # Label connected components\n",
        "    labeled = measure.label(binary)\n",
        "    regions = measure.regionprops(labeled)\n",
        "    \n",
        "    # Filter by size\n",
        "    roi_masks = []\n",
        "    for region in regions:\n",
        "        if 30 < region.area < 500:  # Expected neuron size\n",
        "            mask = labeled == region.label\n",
        "            roi_masks.append(mask)\n",
        "    \n",
        "    roi_masks = np.array(roi_masks)\n",
        "    \n",
        "    # === STAGE 4: Trace extraction ===\n",
        "    traces = []\n",
        "    for mask in roi_masks:\n",
        "        trace = np.mean(denoised[:, mask], axis=1)\n",
        "        # Compute dF/F\n",
        "        f0 = np.percentile(trace, 10)\n",
        "        dff = (trace - f0) / f0\n",
        "        # Detrend\n",
        "        dff = signal.detrend(dff)\n",
        "        traces.append(dff)\n",
        "    \n",
        "    traces = np.array(traces)\n",
        "    \n",
        "    # === STAGE 5: Spike detection ===\n",
        "    spike_times = []\n",
        "    spike_frames = []\n",
        "    \n",
        "    for trace in traces:\n",
        "        # Invert (Voltron-2 has negative spikes)\n",
        "        inverted = -trace\n",
        "        \n",
        "        # Threshold at 3 std\n",
        "        threshold = np.mean(inverted) + 3 * np.std(inverted)\n",
        "        \n",
        "        # Find peaks\n",
        "        peaks, _ = signal.find_peaks(\n",
        "            inverted,\n",
        "            height=threshold,\n",
        "            distance=int(0.005 * fps)  # 5ms refractory\n",
        "        )\n",
        "        \n",
        "        spike_frames.append(peaks)\n",
        "        spike_times.append(peaks / fps)\n",
        "    \n",
        "    return {\n",
        "        'roi_masks': roi_masks,\n",
        "        'traces': traces,\n",
        "        'spike_times': spike_times,\n",
        "        'spike_frames': spike_frames\n",
        "    }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmlm990FkAAe"
      },
      "source": [
        "---\n",
        "# 7. Search Space for Optimization\n",
        "\n",
        "## 7.1 Discrete Choices\n",
        "\n",
        "```python\n",
        "SEARCH_SPACE = {\n",
        "    'motion_correction': {\n",
        "        'method': ['none', 'rigid', 'nonrigid'],\n",
        "        'reference': ['first', 'mean', 'middle'],\n",
        "        'algorithm': ['phase', 'template', 'optical_flow'],\n",
        "    },\n",
        "    'denoising': {\n",
        "        'temporal': ['none', 'gaussian', 'savgol', 'median', 'lowpass'],\n",
        "        'spatial': ['none', 'gaussian', 'bilateral'],\n",
        "        'pca': ['none', 'global', 'local'],\n",
        "    },\n",
        "    'segmentation': {\n",
        "        'method': ['threshold', 'watershed', 'nmf', 'correlation'],\n",
        "        'feature': ['std', 'max', 'mean', 'correlation'],\n",
        "    },\n",
        "    'trace_extraction': {\n",
        "        'aggregation': ['mean', 'median'],\n",
        "        'background': ['none', 'annulus', 'neuropil'],\n",
        "        'baseline': ['percentile', 'rolling_percentile'],\n",
        "        'detrend': ['none', 'linear', 'polynomial'],\n",
        "    },\n",
        "    'spike_detection': {\n",
        "        'method': ['threshold', 'template', 'adaptive'],\n",
        "        'threshold_type': ['fixed_std', 'mad', 'percentile'],\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "## 7.2 Continuous Parameters\n",
        "\n",
        "```python\n",
        "PARAMETER_RANGES = {\n",
        "    'motion_max_shift': (10, 100),  # pixels\n",
        "    'denoise_sigma': (0.5, 5.0),\n",
        "    'savgol_window': (3, 15),  # must be odd\n",
        "    'lowpass_cutoff': (20, 100),  # Hz\n",
        "    'pca_components': (5, 50),\n",
        "    'segmentation_threshold_factor': (1.0, 3.0),\n",
        "    'roi_min_area': (20, 100),  # pixels\n",
        "    'roi_max_area': (200, 1000),  # pixels\n",
        "    'baseline_percentile': (5, 20),\n",
        "    'baseline_window_seconds': (1, 10),\n",
        "    'spike_threshold_std': (2.0, 5.0),\n",
        "    'spike_min_distance_ms': (2, 20),\n",
        "    'spike_prominence_factor': (0.3, 1.0),\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UhE4K4skAAf"
      },
      "source": [
        "---\n",
        "# 8. Data and Ground Truth\n",
        "\n",
        "## 8.1 Available Data\n",
        "\n",
        "| Dataset | Description | Frames | ROIs | Has GT? |\n",
        "|---------|-------------|--------|------|--------|\n",
        "| fish1_fov1 | Example recording | ~2000 | ~20 | Yes (manual) |\n",
        "| fish1_fov2 | Different FOV | ~2000 | ~30 | Yes (manual) |\n",
        "| ... | ... | ... | ... | ... |\n",
        "\n",
        "## 8.2 Ground Truth Format\n",
        "\n",
        "```python\n",
        "ground_truth = {\n",
        "    'roi_masks': np.ndarray,  # shape: (n_rois, height, width)\n",
        "    'roi_metadata': [{\n",
        "        'id': int,\n",
        "        'quality': str,  # 'good', 'uncertain', 'overlapping'\n",
        "        'shape': str,    # 'ring', 'filled', 'irregular'\n",
        "    }],\n",
        "    'spike_times': List[np.ndarray],  # seconds, per ROI\n",
        "    'spike_confidence': List[List[str]],  # 'certain', 'probable', 'uncertain'\n",
        "}\n",
        "```\n",
        "\n",
        "## 8.3 Loading Data\n",
        "\n",
        "```python\n",
        "def load_benchmark_data(data_path):\n",
        "    \"\"\"\n",
        "    Load benchmark data and ground truth.\n",
        "    \n",
        "    Returns:\n",
        "        video: np.ndarray\n",
        "        fps: float\n",
        "        ground_truth: dict or None\n",
        "    \"\"\"\n",
        "    import tifffile\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    \n",
        "    data_path = Path(data_path)\n",
        "    \n",
        "    # Load video\n",
        "    video = tifffile.imread(data_path / 'video.tif')\n",
        "    \n",
        "    # Load metadata\n",
        "    with open(data_path / 'metadata.json') as f:\n",
        "        metadata = json.load(f)\n",
        "    fps = metadata['fps']\n",
        "    \n",
        "    # Load ground truth if available\n",
        "    gt_path = data_path / 'ground_truth.npz'\n",
        "    if gt_path.exists():\n",
        "        gt_data = np.load(gt_path, allow_pickle=True)\n",
        "        ground_truth = {\n",
        "            'roi_masks': gt_data['roi_masks'],\n",
        "            'spike_times': gt_data['spike_times'].tolist(),\n",
        "        }\n",
        "    else:\n",
        "        ground_truth = None\n",
        "    \n",
        "    return video, fps, ground_truth\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hEXaEXPkAAf"
      },
      "source": [
        "---\n",
        "# 9. References and Prior Work\n",
        "\n",
        "## 9.1 Existing Tools\n",
        "\n",
        "| Tool | What it does | Strengths | Limitations |\n",
        "|------|--------------|-----------|-------------|\n",
        "| **VolPy** (CaImAn) | Full pipeline | Mask R-CNN segmentation, SpikePursuit | Complex setup |\n",
        "| **NoRMCorre** | Motion correction | Well-validated | Part of CaImAn |\n",
        "| **Suite2p** | Calcium imaging | Fast, scalable | Not optimized for voltage |\n",
        "\n",
        "## 9.2 Key Papers\n",
        "\n",
        "1. **VolPy** - Cai et al., 2021, PLOS Comp Bio\n",
        "   - Mask R-CNN for segmentation\n",
        "   - SpikePursuit for spike detection\n",
        "   - F1 > 90% on benchmark data\n",
        "\n",
        "2. **Voltage imaging pipeline** - (First paper you shared)\n",
        "   - Camera noise correction\n",
        "   - Local PCA denoising\n",
        "   - Semi-NMF segmentation\n",
        "   - LSTM spike detection\n",
        "\n",
        "3. **Whole-brain voltage imaging** - (Second paper - Positron2)\n",
        "   - NoRMCorre motion correction\n",
        "   - UMAP+DBSCAN artifact removal\n",
        "   - SNR > 4 filtering\n",
        "\n",
        "## 9.3 Novel Ideas to Explore\n",
        "\n",
        "- Shuffled control validation\n",
        "- Template consistency scoring\n",
        "- Artifact correlation detection\n",
        "- Combined supervised + unsupervised evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCHYnmqvkAAf"
      },
      "source": [
        "---\n",
        "# 9.5 Implementation Ideas and Advanced Techniques\n",
        "\n",
        "## 9.5.1 Motion Correction Approaches\n",
        "\n",
        "### Template Matching (Recommended for Rigid Motion)\n",
        "```python\n",
        "import cv2\n",
        "\n",
        "def motion_correct_template(video, max_shift=50, reference='mean'):\n",
        "    \"\"\"\n",
        "    Rigid motion correction using OpenCV template matching.\n",
        "    Most robust for small translations.\n",
        "    \"\"\"\n",
        "    if reference == 'mean':\n",
        "        ref_frame = np.mean(video, axis=0).astype(np.float32)\n",
        "    elif reference == 'first':\n",
        "        ref_frame = video[0].astype(np.float32)\n",
        "    else:\n",
        "        ref_frame = video[len(video)//2].astype(np.float32)\n",
        "    \n",
        "    h, w = ref_frame.shape\n",
        "    margin = max_shift\n",
        "    template = ref_frame[margin:h-margin, margin:w-margin]\n",
        "    \n",
        "    corrected = np.zeros_like(video)\n",
        "    shifts = []\n",
        "    \n",
        "    for i, frame in enumerate(video):\n",
        "        result = cv2.matchTemplate(frame.astype(np.float32), template, cv2.TM_CCORR_NORMED)\n",
        "        _, _, _, max_loc = cv2.minMaxLoc(result)\n",
        "        shift_x = margin - max_loc[0]\n",
        "        shift_y = margin - max_loc[1]\n",
        "        shifts.append((shift_x, shift_y))\n",
        "        \n",
        "        M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
        "        corrected[i] = cv2.warpAffine(frame, M, (w, h))\n",
        "    \n",
        "    return corrected, np.array(shifts)\n",
        "```\n",
        "\n",
        "### NoRMCorre-style Piecewise Rigid\n",
        "```python\n",
        "def motion_correct_piecewise(video, patch_size=128, overlap=32, max_shift=20):\n",
        "    \"\"\"\n",
        "    Piecewise rigid correction - divides FOV into patches.\n",
        "    Better for non-uniform motion (e.g., tissue deformation).\n",
        "    \"\"\"\n",
        "    # Divide into overlapping patches\n",
        "    # Compute shift per patch\n",
        "    # Interpolate shifts across boundaries\n",
        "    # Apply smooth deformation field\n",
        "    pass\n",
        "```\n",
        "\n",
        "### Optical Flow (Non-rigid)\n",
        "```python\n",
        "def motion_correct_optical_flow(video, reference_frame):\n",
        "    \"\"\"\n",
        "    Dense optical flow for non-rigid deformation.\n",
        "    Use sparingly - can introduce artifacts.\n",
        "    \"\"\"\n",
        "    flow_params = dict(\n",
        "        pyr_scale=0.5, levels=3, winsize=15,\n",
        "        iterations=3, poly_n=5, poly_sigma=1.2, flags=0\n",
        "    )\n",
        "    # cv2.calcOpticalFlowFarneback for each frame\n",
        "    pass\n",
        "```\n",
        "\n",
        "## 9.5.2 Denoising Strategies\n",
        "\n",
        "### Local PCA Denoising (from paper methods)\n",
        "```python\n",
        "def local_pca_denoise(video, patch_size=32, n_components=10, stride=16):\n",
        "    \"\"\"\n",
        "    Local PCA denoising - preserves local structure while removing noise.\n",
        "    \n",
        "    For each spatial patch:\n",
        "    1. Extract time series for all pixels in patch\n",
        "    2. Perform PCA, keep top n_components\n",
        "    3. Reconstruct patch from low-rank approximation\n",
        "    4. Average overlapping regions\n",
        "    \"\"\"\n",
        "    n_frames, h, w = video.shape\n",
        "    denoised = np.zeros_like(video, dtype=np.float32)\n",
        "    weights = np.zeros((h, w), dtype=np.float32)\n",
        "    \n",
        "    for y in range(0, h - patch_size + 1, stride):\n",
        "        for x in range(0, w - patch_size + 1, stride):\n",
        "            # Extract patch time series: (n_frames, patch_size^2)\n",
        "            patch = video[:, y:y+patch_size, x:x+patch_size]\n",
        "            patch_flat = patch.reshape(n_frames, -1)\n",
        "            \n",
        "            # Center\n",
        "            mean_vals = patch_flat.mean(axis=0)\n",
        "            centered = patch_flat - mean_vals\n",
        "            \n",
        "            # SVD (more stable than PCA for this)\n",
        "            U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
        "            \n",
        "            # Reconstruct with top components\n",
        "            reconstructed = U[:, :n_components] @ np.diag(S[:n_components]) @ Vt[:n_components, :]\n",
        "            reconstructed += mean_vals\n",
        "            \n",
        "            # Add to output with averaging\n",
        "            denoised[:, y:y+patch_size, x:x+patch_size] += reconstructed.reshape(n_frames, patch_size, patch_size)\n",
        "            weights[y:y+patch_size, x:x+patch_size] += 1\n",
        "    \n",
        "    # Normalize by overlap count\n",
        "    denoised /= weights[np.newaxis, :, :]\n",
        "    return denoised\n",
        "```\n",
        "\n",
        "### Temporal Median Filter (Spike-Preserving)\n",
        "```python\n",
        "def temporal_median_filter(video, window=3):\n",
        "    \"\"\"\n",
        "    Median filter preserves sharp edges (spikes) better than Gaussian.\n",
        "    Window must be small (3-5) to not blur 1-2 frame spikes.\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import median_filter\n",
        "    return median_filter(video, size=(window, 1, 1))\n",
        "```\n",
        "\n",
        "### Wavelet Denoising\n",
        "```python\n",
        "def wavelet_denoise(trace, wavelet='db4', level=3, threshold_mode='soft'):\n",
        "    \"\"\"\n",
        "    Wavelet denoising for individual traces.\n",
        "    Good for separating spike frequencies from noise.\n",
        "    \"\"\"\n",
        "    import pywt\n",
        "    coeffs = pywt.wavedec(trace, wavelet, level=level)\n",
        "    \n",
        "    # Estimate noise from finest detail coefficients\n",
        "    sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "    threshold = sigma * np.sqrt(2 * np.log(len(trace)))\n",
        "    \n",
        "    # Threshold detail coefficients\n",
        "    denoised_coeffs = [coeffs[0]]  # Keep approximation\n",
        "    for c in coeffs[1:]:\n",
        "        denoised_coeffs.append(pywt.threshold(c, threshold, mode=threshold_mode))\n",
        "    \n",
        "    return pywt.waverec(denoised_coeffs, wavelet)[:len(trace)]\n",
        "```\n",
        "\n",
        "## 9.5.3 ROI Segmentation Methods\n",
        "\n",
        "### Correlation Image Segmentation\n",
        "```python\n",
        "def compute_correlation_image(video, radius=4):\n",
        "    \"\"\"\n",
        "    For each pixel, compute mean correlation with neighbors.\n",
        "    Neurons show high local correlation; noise does not.\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import uniform_filter\n",
        "    \n",
        "    n_frames, h, w = video.shape\n",
        "    video_flat = video.reshape(n_frames, -1)\n",
        "    \n",
        "    # Normalize each pixel's time series\n",
        "    video_norm = (video_flat - video_flat.mean(axis=0)) / (video_flat.std(axis=0) + 1e-10)\n",
        "    \n",
        "    # Compute local correlation via convolution trick\n",
        "    corr_img = np.zeros((h, w))\n",
        "    \n",
        "    for dy in range(-radius, radius+1):\n",
        "        for dx in range(-radius, radius+1):\n",
        "            if dy == 0 and dx == 0:\n",
        "                continue\n",
        "            # Shift and correlate\n",
        "            shifted = np.roll(np.roll(video, dy, axis=1), dx, axis=2)\n",
        "            shifted_flat = shifted.reshape(n_frames, -1)\n",
        "            shifted_norm = (shifted_flat - shifted_flat.mean(axis=0)) / (shifted_flat.std(axis=0) + 1e-10)\n",
        "            \n",
        "            corr = (video_norm * shifted_norm).mean(axis=0).reshape(h, w)\n",
        "            corr_img += corr\n",
        "    \n",
        "    corr_img /= (2*radius + 1)**2 - 1\n",
        "    return corr_img\n",
        "\n",
        "def segment_from_correlation(corr_img, std_proj, min_area=30, max_area=500):\n",
        "    \"\"\"Segment ROIs using correlation image.\"\"\"\n",
        "    # Combine correlation and std projection\n",
        "    combined = corr_img * std_proj\n",
        "    combined = (combined - combined.min()) / (combined.max() - combined.min())\n",
        "    \n",
        "    # Adaptive threshold\n",
        "    from skimage.filters import threshold_otsu\n",
        "    thresh = threshold_otsu(combined)\n",
        "    binary = combined > thresh\n",
        "    \n",
        "    # Watershed for touching neurons\n",
        "    from skimage.segmentation import watershed\n",
        "    from skimage.feature import peak_local_max\n",
        "    from scipy import ndimage\n",
        "    \n",
        "    distance = ndimage.distance_transform_edt(binary)\n",
        "    local_max = peak_local_max(distance, min_distance=5, labels=binary)\n",
        "    markers = np.zeros_like(binary, dtype=int)\n",
        "    markers[tuple(local_max.T)] = np.arange(len(local_max)) + 1\n",
        "    labels = watershed(-distance, markers, mask=binary)\n",
        "    \n",
        "    # Filter by size\n",
        "    roi_masks = []\n",
        "    for region in measure.regionprops(labels):\n",
        "        if min_area < region.area < max_area:\n",
        "            mask = labels == region.label\n",
        "            roi_masks.append(mask)\n",
        "    \n",
        "    return np.array(roi_masks)\n",
        "```\n",
        "\n",
        "### Ring-Detection for Membrane-Labeled Neurons\n",
        "```python\n",
        "def detect_ring_rois(std_proj, inner_radius=2, outer_radius=6):\n",
        "    \"\"\"\n",
        "    Detect ring-shaped ROIs typical of soma-targeted membrane indicators.\n",
        "    Uses ring-shaped matched filter.\n",
        "    \"\"\"\n",
        "    from skimage.draw import disk\n",
        "    \n",
        "    # Create ring template\n",
        "    size = outer_radius * 2 + 1\n",
        "    template = np.zeros((size, size))\n",
        "    rr_outer, cc_outer = disk((outer_radius, outer_radius), outer_radius)\n",
        "    rr_inner, cc_inner = disk((outer_radius, outer_radius), inner_radius)\n",
        "    template[rr_outer, cc_outer] = 1\n",
        "    template[rr_inner, cc_inner] = -1  # Hollow center\n",
        "    template /= np.abs(template).sum()\n",
        "    \n",
        "    # Convolve\n",
        "    from scipy.signal import convolve2d\n",
        "    response = convolve2d(std_proj, template, mode='same')\n",
        "    \n",
        "    # Find peaks\n",
        "    from skimage.feature import peak_local_max\n",
        "    peaks = peak_local_max(response, min_distance=outer_radius, threshold_rel=0.3)\n",
        "    \n",
        "    # Create circular masks around peaks\n",
        "    roi_masks = []\n",
        "    h, w = std_proj.shape\n",
        "    for y, x in peaks:\n",
        "        mask = np.zeros((h, w), dtype=bool)\n",
        "        rr, cc = disk((y, x), outer_radius, shape=(h, w))\n",
        "        mask[rr, cc] = True\n",
        "        roi_masks.append(mask)\n",
        "    \n",
        "    return np.array(roi_masks)\n",
        "```\n",
        "\n",
        "### Semi-NMF Segmentation\n",
        "```python\n",
        "def seminmf_segmentation(video, n_components=50, n_rois=30):\n",
        "    \"\"\"\n",
        "    Semi-NMF: Spatial components are non-negative (masks),\n",
        "    temporal components can be negative (voltage traces).\n",
        "    \"\"\"\n",
        "    from sklearn.decomposition import NMF\n",
        "    \n",
        "    n_frames, h, w = video.shape\n",
        "    video_flat = video.reshape(n_frames, h*w).T  # (pixels, frames)\n",
        "    \n",
        "    # Standard NMF on absolute values to initialize\n",
        "    nmf = NMF(n_components=n_components, max_iter=500)\n",
        "    W = nmf.fit_transform(np.abs(video_flat))  # Spatial (pixels, components)\n",
        "    H = nmf.components_  # Temporal (components, frames)\n",
        "    \n",
        "    # Threshold spatial components to get masks\n",
        "    roi_masks = []\n",
        "    for i in range(n_components):\n",
        "        spatial = W[:, i].reshape(h, w)\n",
        "        thresh = np.percentile(spatial, 95)\n",
        "        mask = spatial > thresh\n",
        "        \n",
        "        # Filter small regions\n",
        "        mask = morphology.remove_small_objects(mask, min_size=30)\n",
        "        if mask.sum() > 0:\n",
        "            roi_masks.append(mask)\n",
        "    \n",
        "    return np.array(roi_masks[:n_rois])\n",
        "```\n",
        "\n",
        "## 9.5.4 Advanced Spike Detection\n",
        "\n",
        "### Adaptive Threshold Detection\n",
        "```python\n",
        "def detect_spikes_adaptive(trace, fps=200, window_sec=1.0, n_std=3.5):\n",
        "    \"\"\"\n",
        "    Adaptive threshold that adjusts to local noise level.\n",
        "    Better for traces with non-stationary noise.\n",
        "    \"\"\"\n",
        "    window = int(window_sec * fps)\n",
        "    inverted = -trace  # Voltron-2 has negative spikes\n",
        "    \n",
        "    # Rolling statistics\n",
        "    from scipy.ndimage import uniform_filter1d\n",
        "    local_mean = uniform_filter1d(inverted, window)\n",
        "    local_std = np.sqrt(uniform_filter1d((inverted - local_mean)**2, window))\n",
        "    \n",
        "    # Adaptive threshold\n",
        "    threshold = local_mean + n_std * local_std\n",
        "    \n",
        "    # Find peaks above threshold\n",
        "    peaks, properties = signal.find_peaks(\n",
        "        inverted,\n",
        "        height=threshold,\n",
        "        distance=int(0.003 * fps),  # 3ms refractory\n",
        "        prominence=0.5 * local_std.mean()\n",
        "    )\n",
        "    \n",
        "    return peaks / fps, properties\n",
        "\n",
        "```\n",
        "\n",
        "### Template Matching Spike Detection\n",
        "```python\n",
        "def detect_spikes_template(trace, fps=200, template_width_ms=10):\n",
        "    \"\"\"\n",
        "    Use average spike waveform as template for matched filtering.\n",
        "    Two-pass: first detect rough spikes, then refine with template.\n",
        "    \"\"\"\n",
        "    inverted = -trace\n",
        "    template_samples = int(template_width_ms * fps / 1000)\n",
        "    \n",
        "    # Pass 1: Simple threshold to get initial spikes\n",
        "    threshold = np.mean(inverted) + 4 * np.std(inverted)\n",
        "    initial_peaks, _ = signal.find_peaks(inverted, height=threshold, distance=template_samples//2)\n",
        "    \n",
        "    if len(initial_peaks) < 3:\n",
        "        return initial_peaks / fps\n",
        "    \n",
        "    # Extract waveforms and compute average template\n",
        "    waveforms = []\n",
        "    for peak in initial_peaks:\n",
        "        if peak >= template_samples and peak < len(trace) - template_samples:\n",
        "            waveforms.append(inverted[peak-template_samples:peak+template_samples])\n",
        "    \n",
        "    template = np.mean(waveforms, axis=0)\n",
        "    template = (template - template.mean()) / template.std()\n",
        "    \n",
        "    # Pass 2: Matched filter\n",
        "    filtered = np.correlate(inverted, template, mode='same')\n",
        "    \n",
        "    # Detect on filtered trace\n",
        "    refined_peaks, _ = signal.find_peaks(\n",
        "        filtered,\n",
        "        height=np.percentile(filtered, 95),\n",
        "        distance=int(0.003 * fps)\n",
        "    )\n",
        "    \n",
        "    return refined_peaks / fps\n",
        "```\n",
        "\n",
        "### MAD-based Robust Threshold\n",
        "```python\n",
        "def detect_spikes_mad(trace, fps=200, n_mad=5):\n",
        "    \"\"\"\n",
        "    Use Median Absolute Deviation instead of std.\n",
        "    More robust to outliers and non-Gaussian noise.\n",
        "    \"\"\"\n",
        "    inverted = -trace\n",
        "    median = np.median(inverted)\n",
        "    mad = np.median(np.abs(inverted - median))\n",
        "    \n",
        "    # MAD to std conversion for Gaussian: std ≈ 1.4826 * MAD\n",
        "    threshold = median + n_mad * 1.4826 * mad\n",
        "    \n",
        "    peaks, _ = signal.find_peaks(\n",
        "        inverted,\n",
        "        height=threshold,\n",
        "        distance=int(0.003 * fps)\n",
        "    )\n",
        "    \n",
        "    return peaks / fps\n",
        "```\n",
        "\n",
        "## 9.5.5 Artifact Detection and Removal\n",
        "\n",
        "### UMAP + DBSCAN Artifact Clustering (from Bharioke et al.)\n",
        "```python\n",
        "def remove_artifact_spikes_umap(traces, spike_times_list, fps=200, window_ms=15):\n",
        "    \"\"\"\n",
        "    Cluster spike waveforms using UMAP + DBSCAN.\n",
        "    Remove clusters that appear across many neurons (likely artifacts).\n",
        "    \"\"\"\n",
        "    import umap\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    \n",
        "    window = int(window_ms * fps / 1000)\n",
        "    all_waveforms = []\n",
        "    waveform_info = []  # (roi_idx, spike_idx)\n",
        "    \n",
        "    # Collect all waveforms\n",
        "    for roi_idx, (trace, spike_times) in enumerate(zip(traces, spike_times_list)):\n",
        "        for spike_idx, t in enumerate(spike_times):\n",
        "            frame = int(t * fps)\n",
        "            if window <= frame < len(trace) - window:\n",
        "                wf = trace[frame-window:frame+window]\n",
        "                wf = (wf - wf.mean()) / (wf.std() + 1e-10)\n",
        "                all_waveforms.append(wf)\n",
        "                waveform_info.append((roi_idx, spike_idx))\n",
        "    \n",
        "    if len(all_waveforms) < 10:\n",
        "        return spike_times_list\n",
        "    \n",
        "    all_waveforms = np.array(all_waveforms)\n",
        "    \n",
        "    # UMAP embedding\n",
        "    reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)\n",
        "    embedding = reducer.fit_transform(all_waveforms)\n",
        "    \n",
        "    # DBSCAN clustering\n",
        "    clusterer = DBSCAN(eps=0.5, min_samples=5)\n",
        "    labels = clusterer.fit_predict(embedding)\n",
        "    \n",
        "    # Find artifact clusters (present in >50% of ROIs)\n",
        "    n_rois = len(traces)\n",
        "    artifact_clusters = set()\n",
        "    \n",
        "    for cluster_id in set(labels):\n",
        "        if cluster_id == -1:  # Noise\n",
        "            continue\n",
        "        cluster_mask = labels == cluster_id\n",
        "        rois_in_cluster = set(waveform_info[i][0] for i in np.where(cluster_mask)[0])\n",
        "        if len(rois_in_cluster) > 0.5 * n_rois:\n",
        "            artifact_clusters.add(cluster_id)\n",
        "    \n",
        "    # Remove artifact spikes\n",
        "    cleaned_spike_times = [list(st) for st in spike_times_list]\n",
        "    for i, (roi_idx, spike_idx) in enumerate(waveform_info):\n",
        "        if labels[i] in artifact_clusters:\n",
        "            cleaned_spike_times[roi_idx][spike_idx] = None\n",
        "    \n",
        "    # Filter out None values\n",
        "    cleaned_spike_times = [\n",
        "        np.array([t for t in times if t is not None])\n",
        "        for times in cleaned_spike_times\n",
        "    ]\n",
        "    \n",
        "    return cleaned_spike_times\n",
        "```\n",
        "\n",
        "### Global Signal Regression\n",
        "```python\n",
        "def remove_global_signal(traces):\n",
        "    \"\"\"\n",
        "    Remove global signal (first PC) that likely represents motion/illumination artifacts.\n",
        "    \"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "    \n",
        "    # Compute global signal (first PC)\n",
        "    pca = PCA(n_components=1)\n",
        "    global_signal = pca.fit_transform(traces.T).flatten()\n",
        "    \n",
        "    # Regress out from each trace\n",
        "    cleaned = np.zeros_like(traces)\n",
        "    for i, trace in enumerate(traces):\n",
        "        # Linear regression\n",
        "        coef = np.dot(trace, global_signal) / np.dot(global_signal, global_signal)\n",
        "        cleaned[i] = trace - coef * global_signal\n",
        "    \n",
        "    return cleaned\n",
        "```\n",
        "\n",
        "## 9.5.6 Quality Control Metrics\n",
        "\n",
        "### SNR Estimation per ROI\n",
        "```python\n",
        "def estimate_snr(trace, spike_frames, fps=200, noise_window_ms=50):\n",
        "    \"\"\"\n",
        "    Estimate SNR as spike amplitude / baseline noise.\n",
        "    \"\"\"\n",
        "    inverted = -trace\n",
        "    noise_window = int(noise_window_ms * fps / 1000)\n",
        "    \n",
        "    # Get spike amplitudes\n",
        "    amplitudes = inverted[spike_frames] if len(spike_frames) > 0 else []\n",
        "    \n",
        "    # Estimate noise from spike-free regions\n",
        "    spike_free = np.ones(len(trace), dtype=bool)\n",
        "    for f in spike_frames:\n",
        "        spike_free[max(0,f-noise_window):min(len(trace),f+noise_window)] = False\n",
        "    \n",
        "    if spike_free.sum() > 100:\n",
        "        noise_std = np.std(inverted[spike_free])\n",
        "    else:\n",
        "        noise_std = np.std(inverted)\n",
        "    \n",
        "    if len(amplitudes) > 0 and noise_std > 0:\n",
        "        snr = np.median(amplitudes) / noise_std\n",
        "    else:\n",
        "        snr = 0\n",
        "    \n",
        "    return snr\n",
        "\n",
        "def filter_rois_by_snr(traces, spike_times_list, min_snr=4):\n",
        "    \"\"\"Filter ROIs with SNR below threshold.\"\"\"\n",
        "    keep_idx = []\n",
        "    for i, (trace, spikes) in enumerate(zip(traces, spike_times_list)):\n",
        "        spike_frames = (np.array(spikes) * 200).astype(int)\n",
        "        snr = estimate_snr(trace, spike_frames)\n",
        "        if snr >= min_snr:\n",
        "            keep_idx.append(i)\n",
        "    return keep_idx\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKUKL85SkAAg"
      },
      "source": [
        "## 9.5.7 Complete Pipeline Examples from Literature\n",
        "\n",
        "### Positron2 Pipeline (Wang et al. - Whole-brain Zebrafish Voltage Imaging)\n",
        "\n",
        "This pipeline was used for imaging Positron2 across entire larval zebrafish brains.\n",
        "\n",
        "#### Manual ROI Annotation Criteria\n",
        "```python\n",
        "# ROI selection criteria from the paper:\n",
        "ROI_CRITERIA = {\n",
        "    'size_pixels': 9,           # ~6.6 µm diameter\n",
        "    'shape': 'ring-like',       # Membrane-targeted indicator\n",
        "    'spike_width_max_ms': 20,   # Narrow spikes\n",
        "    'spike_amplitude_std': 2.0, # >2× std of trace\n",
        "    'spike_polarity': 'positive',  # Note: Positron2 has POSITIVE spikes (unlike Voltron-2)\n",
        "}\n",
        "```\n",
        "\n",
        "#### Overlap Removal (Spatial Correlation)\n",
        "```python\n",
        "def remove_overlapping_rois(roi_masks, traces, correlation_threshold=0.9):\n",
        "    \"\"\"\n",
        "    Remove ROIs that are likely duplicates based on spatial correlation.\n",
        "    From Wang et al.: ROIs with >0.9 correlation are rejected.\n",
        "    \"\"\"\n",
        "    n_rois = len(roi_masks)\n",
        "    keep_mask = np.ones(n_rois, dtype=bool)\n",
        "    \n",
        "    # Flatten masks for correlation\n",
        "    masks_flat = roi_masks.reshape(n_rois, -1).astype(float)\n",
        "    \n",
        "    # Compute pairwise spatial correlations\n",
        "    for i in range(n_rois):\n",
        "        if not keep_mask[i]:\n",
        "            continue\n",
        "        for j in range(i + 1, n_rois):\n",
        "            if not keep_mask[j]:\n",
        "                continue\n",
        "            \n",
        "            # Spatial correlation between masks\n",
        "            corr = np.corrcoef(masks_flat[i], masks_flat[j])[0, 1]\n",
        "            \n",
        "            if corr > correlation_threshold:\n",
        "                # Keep the one with higher SNR\n",
        "                snr_i = np.std(traces[i])  # Simplified SNR proxy\n",
        "                snr_j = np.std(traces[j])\n",
        "                if snr_i >= snr_j:\n",
        "                    keep_mask[j] = False\n",
        "                else:\n",
        "                    keep_mask[i] = False\n",
        "    \n",
        "    return np.where(keep_mask)[0]\n",
        "```\n",
        "\n",
        "#### Iterative UMAP-DBSCAN Artifact Removal\n",
        "```python\n",
        "def iterative_umap_dbscan_artifact_removal(spike_rasters, fps=200,\n",
        "                                            hanning_window_ms=250,\n",
        "                                            max_iterations=10):\n",
        "    \"\"\"\n",
        "    Iterative UMAP-DBSCAN clustering to remove artifact-contaminated ROIs.\n",
        "    From Wang et al.: Iterate until 2D UMAP representation is near-Gaussian.\n",
        "    \n",
        "    Args:\n",
        "        spike_rasters: list of binary spike trains per ROI\n",
        "        fps: frame rate\n",
        "        hanning_window_ms: smoothing window for firing rate estimation\n",
        "        max_iterations: maximum clustering iterations\n",
        "    \n",
        "    Returns:\n",
        "        clean_roi_indices: indices of ROIs that passed artifact removal\n",
        "        artifact_roi_indices: indices of ROIs flagged as artifacts\n",
        "        cluster_labels: final cluster assignments for clean ROIs\n",
        "    \"\"\"\n",
        "    import umap\n",
        "    from sklearn.cluster import DBSCAN\n",
        "    from scipy.signal import convolve\n",
        "    from scipy.stats import shapiro\n",
        "    \n",
        "    n_rois = len(spike_rasters)\n",
        "    n_samples = len(spike_rasters[0])\n",
        "    \n",
        "    # Create smoothed firing rate matrix\n",
        "    window_samples = int(hanning_window_ms * fps / 1000)\n",
        "    hanning = np.hanning(window_samples)\n",
        "    hanning /= hanning.sum()\n",
        "    \n",
        "    firing_rates = np.zeros((n_rois, n_samples))\n",
        "    for i, raster in enumerate(spike_rasters):\n",
        "        firing_rates[i] = convolve(raster.astype(float), hanning, mode='same')\n",
        "    \n",
        "    # Track which ROIs are still candidates\n",
        "    candidate_mask = np.ones(n_rois, dtype=bool)\n",
        "    artifact_indices = []\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        current_indices = np.where(candidate_mask)[0]\n",
        "        if len(current_indices) < 10:\n",
        "            break\n",
        "        \n",
        "        current_rates = firing_rates[current_indices]\n",
        "        \n",
        "        # UMAP embedding\n",
        "        reducer = umap.UMAP(n_components=2, n_neighbors=min(15, len(current_indices)-1),\n",
        "                           min_dist=0.1, random_state=42)\n",
        "        embedding = reducer.fit_transform(current_rates)\n",
        "        \n",
        "        # Check if distribution is approximately Gaussian (stopping criterion)\n",
        "        # Use Shapiro-Wilk test on each dimension\n",
        "        _, p_x = shapiro(embedding[:, 0]) if len(embedding) >= 3 else (0, 1)\n",
        "        _, p_y = shapiro(embedding[:, 1]) if len(embedding) >= 3 else (0, 1)\n",
        "        \n",
        "        if p_x > 0.05 and p_y > 0.05:\n",
        "            # Distribution is approximately Gaussian - stop iterating\n",
        "            print(f\"Iteration {iteration}: UMAP distribution is Gaussian, stopping.\")\n",
        "            break\n",
        "        \n",
        "        # DBSCAN clustering\n",
        "        clusterer = DBSCAN(eps=0.5, min_samples=3)\n",
        "        labels = clusterer.fit_predict(embedding)\n",
        "        \n",
        "        # Find outlier cluster (well-separated from main cluster)\n",
        "        unique_labels = set(labels) - {-1}  # Exclude noise\n",
        "        if len(unique_labels) <= 1:\n",
        "            break\n",
        "        \n",
        "        # Main cluster = largest cluster\n",
        "        cluster_sizes = {l: np.sum(labels == l) for l in unique_labels}\n",
        "        main_cluster = max(cluster_sizes, key=cluster_sizes.get)\n",
        "        \n",
        "        # Find clusters that are well-separated (potential artifacts)\n",
        "        main_centroid = embedding[labels == main_cluster].mean(axis=0)\n",
        "        \n",
        "        for cluster_id in unique_labels:\n",
        "            if cluster_id == main_cluster:\n",
        "                continue\n",
        "            \n",
        "            cluster_centroid = embedding[labels == cluster_id].mean(axis=0)\n",
        "            distance = np.linalg.norm(cluster_centroid - main_centroid)\n",
        "            \n",
        "            # If cluster is far from main cluster, flag as artifact\n",
        "            main_std = embedding[labels == main_cluster].std()\n",
        "            if distance > 3 * main_std:\n",
        "                # Mark these ROIs as artifacts\n",
        "                artifact_mask = labels == cluster_id\n",
        "                artifact_roi_indices = current_indices[artifact_mask]\n",
        "                artifact_indices.extend(artifact_roi_indices)\n",
        "                candidate_mask[artifact_roi_indices] = False\n",
        "                print(f\"Iteration {iteration}: Removed {len(artifact_roi_indices)} artifact ROIs\")\n",
        "    \n",
        "    clean_indices = np.where(candidate_mask)[0]\n",
        "    return clean_indices, np.array(artifact_indices)\n",
        "```\n",
        "\n",
        "### SNR Calculation (VolPy-style)\n",
        "```python\n",
        "def compute_snr_volpy(trace, spike_frames, fps=200):\n",
        "    \"\"\"\n",
        "    Compute SNR as defined in VolPy.\n",
        "    SNR = median(spike_amplitudes) / noise_std\n",
        "    \n",
        "    Noise is estimated from spike-free regions.\n",
        "    \"\"\"\n",
        "    if len(spike_frames) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # For Voltron-2: invert trace (negative spikes)\n",
        "    # For Positron2: use trace directly (positive spikes)\n",
        "    \n",
        "    # Get spike amplitudes (peak - local baseline)\n",
        "    window = int(10 * fps / 1000)  # 10ms window\n",
        "    amplitudes = []\n",
        "    \n",
        "    for frame in spike_frames:\n",
        "        if frame < window or frame >= len(trace) - window:\n",
        "            continue\n",
        "        \n",
        "        # Local baseline (before spike)\n",
        "        baseline = np.median(trace[frame-window:frame-2])\n",
        "        # Peak value\n",
        "        peak = trace[frame]\n",
        "        amplitudes.append(np.abs(peak - baseline))\n",
        "    \n",
        "    if len(amplitudes) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Estimate noise from spike-free regions\n",
        "    spike_free_mask = np.ones(len(trace), dtype=bool)\n",
        "    for frame in spike_frames:\n",
        "        start = max(0, frame - window)\n",
        "        end = min(len(trace), frame + window)\n",
        "        spike_free_mask[start:end] = False\n",
        "    \n",
        "    if spike_free_mask.sum() < 100:\n",
        "        noise_std = np.std(trace)\n",
        "    else:\n",
        "        noise_std = np.std(trace[spike_free_mask])\n",
        "    \n",
        "    snr = np.median(amplitudes) / (noise_std + 1e-10)\n",
        "    return snr\n",
        "\n",
        "def filter_by_snr(traces, spike_times_list, fps=200, min_snr=4):\n",
        "    \"\"\"\n",
        "    Filter ROIs by SNR threshold.\n",
        "    Wang et al. used SNR > 4 as cutoff.\n",
        "    \"\"\"\n",
        "    keep_indices = []\n",
        "    snr_values = []\n",
        "    \n",
        "    for i, (trace, spike_times) in enumerate(zip(traces, spike_times_list)):\n",
        "        spike_frames = (np.array(spike_times) * fps).astype(int)\n",
        "        snr = compute_snr_volpy(trace, spike_frames, fps)\n",
        "        snr_values.append(snr)\n",
        "        \n",
        "        if snr >= min_snr:\n",
        "            keep_indices.append(i)\n",
        "    \n",
        "    return keep_indices, snr_values\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB5GLHdlkAAg"
      },
      "source": [
        "### Second Paper Pipeline (Camera Noise Correction, Local PCA, Semi-NMF, LSTM)\n",
        "\n",
        "If you have the second paper's methods available, please share them and I'll add detailed implementations for:\n",
        "- Camera noise correction\n",
        "- dipy-based motion correction  \n",
        "- Local PCA denoising specifics\n",
        "- Semi-NMF segmentation details\n",
        "- LSTM spike detection\n",
        "- Subthreshold activity extraction\n",
        "\n",
        "In the meantime, here are general implementations of the key techniques mentioned:\n",
        "\n",
        "#### Camera Noise Correction\n",
        "```python\n",
        "def correct_camera_noise(video, dark_frame=None, flat_field=None):\n",
        "    \"\"\"\n",
        "    Correct for camera-specific noise patterns.\n",
        "    \n",
        "    Args:\n",
        "        video: raw video (n_frames, height, width)\n",
        "        dark_frame: average of frames with no illumination (fixed pattern noise)\n",
        "        flat_field: normalized response to uniform illumination\n",
        "    \n",
        "    Returns:\n",
        "        corrected video\n",
        "    \"\"\"\n",
        "    corrected = video.astype(np.float32)\n",
        "    \n",
        "    # Subtract dark frame (fixed pattern noise)\n",
        "    if dark_frame is not None:\n",
        "        corrected -= dark_frame\n",
        "    \n",
        "    # Divide by flat field (pixel sensitivity variation)\n",
        "    if flat_field is not None:\n",
        "        # Normalize flat field\n",
        "        flat_norm = flat_field / np.mean(flat_field)\n",
        "        flat_norm = np.clip(flat_norm, 0.1, 10)  # Avoid extreme corrections\n",
        "        corrected /= flat_norm\n",
        "    \n",
        "    # Remove hot pixels (outliers in spatial domain)\n",
        "    from scipy.ndimage import median_filter\n",
        "    for i in range(len(corrected)):\n",
        "        frame = corrected[i]\n",
        "        median_frame = median_filter(frame, size=3)\n",
        "        # Replace pixels that deviate significantly from local median\n",
        "        outlier_mask = np.abs(frame - median_frame) > 5 * np.std(frame)\n",
        "        corrected[i][outlier_mask] = median_frame[outlier_mask]\n",
        "    \n",
        "    return corrected\n",
        "\n",
        "def estimate_dark_frame(dark_video):\n",
        "    \"\"\"Estimate dark frame from a recording with no illumination.\"\"\"\n",
        "    return np.median(dark_video, axis=0)\n",
        "\n",
        "def estimate_flat_field(flat_video):\n",
        "    \"\"\"Estimate flat field from uniform illumination recording.\"\"\"\n",
        "    mean_frame = np.mean(flat_video, axis=0)\n",
        "    # Normalize to mean of 1\n",
        "    return mean_frame / np.mean(mean_frame)\n",
        "```\n",
        "\n",
        "#### Background Subtraction Methods\n",
        "```python\n",
        "def subtract_background_annulus(video, roi_mask, inner_gap=2, outer_width=5):\n",
        "    \"\"\"\n",
        "    Subtract local background using annulus around ROI.\n",
        "    Common in voltage imaging to remove neuropil contamination.\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import binary_dilation, binary_erosion\n",
        "    \n",
        "    # Create annulus mask\n",
        "    dilated = binary_dilation(roi_mask, iterations=inner_gap + outer_width)\n",
        "    inner = binary_dilation(roi_mask, iterations=inner_gap)\n",
        "    annulus = dilated & ~inner\n",
        "    \n",
        "    # Extract traces\n",
        "    roi_trace = np.mean(video[:, roi_mask], axis=1)\n",
        "    bg_trace = np.mean(video[:, annulus], axis=1) if annulus.sum() > 0 else 0\n",
        "    \n",
        "    # Subtract with optional coefficient\n",
        "    return roi_trace - 0.7 * bg_trace  # 0.7 is typical neuropil coefficient\n",
        "\n",
        "def subtract_background_percentile(trace, percentile=8, window_sec=1.0, fps=200):\n",
        "    \"\"\"\n",
        "    Rolling percentile baseline subtraction.\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import percentile_filter\n",
        "    window = int(window_sec * fps)\n",
        "    baseline = percentile_filter(trace, percentile, size=window)\n",
        "    return trace - baseline\n",
        "```\n",
        "\n",
        "#### Baseline Correction and dF/F Computation\n",
        "```python\n",
        "def compute_dff(trace, method='percentile', percentile=10, window_sec=None, fps=200):\n",
        "    \"\"\"\n",
        "    Compute ΔF/F with various baseline methods.\n",
        "    \n",
        "    Methods:\n",
        "        'percentile': global percentile as F0\n",
        "        'rolling_percentile': sliding window percentile\n",
        "        'exponential': fit exponential decay (for photobleaching)\n",
        "    \"\"\"\n",
        "    if method == 'percentile':\n",
        "        f0 = np.percentile(trace, percentile)\n",
        "        dff = (trace - f0) / (f0 + 1e-10)\n",
        "        \n",
        "    elif method == 'rolling_percentile':\n",
        "        from scipy.ndimage import percentile_filter\n",
        "        window = int((window_sec or 2.0) * fps)\n",
        "        f0 = percentile_filter(trace, percentile, size=window)\n",
        "        dff = (trace - f0) / (f0 + 1e-10)\n",
        "        \n",
        "    elif method == 'exponential':\n",
        "        # Fit exponential decay for photobleaching correction\n",
        "        from scipy.optimize import curve_fit\n",
        "        \n",
        "        def exp_decay(t, a, b, c):\n",
        "            return a * np.exp(-b * t) + c\n",
        "        \n",
        "        t = np.arange(len(trace))\n",
        "        try:\n",
        "            # Use robust fitting (median of rolling windows as targets)\n",
        "            window = fps  # 1 second windows\n",
        "            n_windows = len(trace) // window\n",
        "            t_fit = np.array([i * window + window//2 for i in range(n_windows)])\n",
        "            y_fit = np.array([np.median(trace[i*window:(i+1)*window]) for i in range(n_windows)])\n",
        "            \n",
        "            popt, _ = curve_fit(exp_decay, t_fit, y_fit,\n",
        "                               p0=[trace[0]-trace[-1], 0.001, trace[-1]],\n",
        "                               maxfev=1000)\n",
        "            f0 = exp_decay(t, *popt)\n",
        "        except:\n",
        "            f0 = np.percentile(trace, percentile)\n",
        "        \n",
        "        dff = (trace - f0) / (f0 + 1e-10)\n",
        "    \n",
        "    return dff\n",
        "```\n",
        "\n",
        "#### Subthreshold Activity Extraction\n",
        "```python\n",
        "def extract_subthreshold(trace, spike_frames, fps=200,\n",
        "                         interpolation_window_ms=20,\n",
        "                         lowpass_cutoff_hz=10):\n",
        "    \"\"\"\n",
        "    Extract subthreshold membrane potential by removing spikes and lowpass filtering.\n",
        "    \n",
        "    1. Interpolate over spike regions\n",
        "    2. Lowpass filter to get slow membrane potential fluctuations\n",
        "    \"\"\"\n",
        "    from scipy.interpolate import interp1d\n",
        "    from scipy.signal import butter, filtfilt\n",
        "    \n",
        "    # Create mask of spike regions to interpolate\n",
        "    window = int(interpolation_window_ms * fps / 1000)\n",
        "    spike_mask = np.zeros(len(trace), dtype=bool)\n",
        "    for frame in spike_frames:\n",
        "        start = max(0, frame - window//2)\n",
        "        end = min(len(trace), frame + window//2)\n",
        "        spike_mask[start:end] = True\n",
        "    \n",
        "    # Interpolate over spikes\n",
        "    x = np.arange(len(trace))\n",
        "    valid_x = x[~spike_mask]\n",
        "    valid_y = trace[~spike_mask]\n",
        "    \n",
        "    if len(valid_x) < 10:\n",
        "        interpolated = trace.copy()\n",
        "    else:\n",
        "        f = interp1d(valid_x, valid_y, kind='linear',\n",
        "                     bounds_error=False, fill_value='extrapolate')\n",
        "        interpolated = f(x)\n",
        "    \n",
        "    # Lowpass filter\n",
        "    nyquist = fps / 2\n",
        "    b, a = butter(4, lowpass_cutoff_hz / nyquist, btype='low')\n",
        "    subthreshold = filtfilt(b, a, interpolated)\n",
        "    \n",
        "    return subthreshold\n",
        "```\n",
        "\n",
        "#### Complete Pipeline Function\n",
        "```python\n",
        "def voltage_imaging_pipeline_full(video, fps=200, config=None):\n",
        "    \"\"\"\n",
        "    Full voltage imaging pipeline with all stages.\n",
        "    \n",
        "    Args:\n",
        "        video: (n_frames, height, width) raw video\n",
        "        fps: frame rate\n",
        "        config: dict of parameters for each stage\n",
        "    \n",
        "    Returns:\n",
        "        dict with roi_masks, traces, spike_times, subthreshold, metadata\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = {}\n",
        "    \n",
        "    results = {'metadata': {'fps': fps, 'config': config}}\n",
        "    \n",
        "    # === Stage 0: Camera noise correction ===\n",
        "    if config.get('camera_correction', False):\n",
        "        video = correct_camera_noise(video)\n",
        "    \n",
        "    # === Stage 1: Motion correction ===\n",
        "    mc_method = config.get('motion_correction', 'template')\n",
        "    if mc_method == 'template':\n",
        "        video, shifts = motion_correct_template(video, max_shift=50)\n",
        "        results['metadata']['motion_shifts'] = shifts\n",
        "    elif mc_method == 'none':\n",
        "        pass\n",
        "    \n",
        "    # === Stage 2: Denoising ===\n",
        "    denoise_method = config.get('denoising', 'local_pca')\n",
        "    if denoise_method == 'local_pca':\n",
        "        video = local_pca_denoise(video,\n",
        "                                   patch_size=config.get('pca_patch_size', 32),\n",
        "                                   n_components=config.get('pca_components', 10))\n",
        "    elif denoise_method == 'temporal_median':\n",
        "        video = temporal_median_filter(video, window=3)\n",
        "    \n",
        "    # === Stage 3: ROI Segmentation ===\n",
        "    seg_method = config.get('segmentation', 'correlation')\n",
        "    std_proj = np.std(video, axis=0)\n",
        "    \n",
        "    if seg_method == 'correlation':\n",
        "        corr_img = compute_correlation_image(video)\n",
        "        roi_masks = segment_from_correlation(corr_img, std_proj)\n",
        "    elif seg_method == 'ring':\n",
        "        roi_masks = detect_ring_rois(std_proj)\n",
        "    elif seg_method == 'threshold':\n",
        "        # Simple threshold segmentation\n",
        "        thresh = np.mean(std_proj) + 1.5 * np.std(std_proj)\n",
        "        binary = std_proj > thresh\n",
        "        binary = morphology.remove_small_objects(binary, min_size=30)\n",
        "        labeled = measure.label(binary)\n",
        "        roi_masks = np.array([labeled == i for i in range(1, labeled.max()+1)])\n",
        "    \n",
        "    results['roi_masks'] = roi_masks\n",
        "    results['metadata']['n_rois_initial'] = len(roi_masks)\n",
        "    \n",
        "    # === Stage 4: Trace extraction ===\n",
        "    traces = []\n",
        "    for mask in roi_masks:\n",
        "        if config.get('background_subtraction', 'annulus') == 'annulus':\n",
        "            trace = subtract_background_annulus(video, mask)\n",
        "        else:\n",
        "            trace = np.mean(video[:, mask], axis=1)\n",
        "        \n",
        "        # Compute dF/F\n",
        "        dff = compute_dff(trace,\n",
        "                         method=config.get('baseline_method', 'rolling_percentile'),\n",
        "                         fps=fps)\n",
        "        traces.append(dff)\n",
        "    \n",
        "    traces = np.array(traces)\n",
        "    results['traces'] = traces\n",
        "    \n",
        "    # === Stage 5: Spike detection ===\n",
        "    spike_method = config.get('spike_detection', 'adaptive')\n",
        "    spike_times = []\n",
        "    spike_frames = []\n",
        "    \n",
        "    for trace in traces:\n",
        "        if spike_method == 'adaptive':\n",
        "            times, _ = detect_spikes_adaptive(trace, fps)\n",
        "        elif spike_method == 'mad':\n",
        "            times = detect_spikes_mad(trace, fps)\n",
        "        elif spike_method == 'template':\n",
        "            times = detect_spikes_template(trace, fps)\n",
        "        else:\n",
        "            # Simple threshold\n",
        "            inverted = -trace\n",
        "            threshold = np.mean(inverted) + 3 * np.std(inverted)\n",
        "            peaks, _ = signal.find_peaks(inverted, height=threshold)\n",
        "            times = peaks / fps\n",
        "        \n",
        "        spike_times.append(times)\n",
        "        spike_frames.append((times * fps).astype(int))\n",
        "    \n",
        "    results['spike_times'] = spike_times\n",
        "    results['spike_frames'] = spike_frames\n",
        "    \n",
        "    # === Stage 6: Quality filtering ===\n",
        "    # SNR filter\n",
        "    if config.get('snr_filter', True):\n",
        "        keep_idx, snr_values = filter_by_snr(traces, spike_times, fps,\n",
        "                                              min_snr=config.get('min_snr', 4))\n",
        "        results['metadata']['snr_values'] = snr_values\n",
        "    else:\n",
        "        keep_idx = list(range(len(roi_masks)))\n",
        "    \n",
        "    # Overlap removal\n",
        "    if config.get('overlap_removal', True):\n",
        "        keep_idx_overlap = remove_overlapping_rois(\n",
        "            roi_masks[keep_idx],\n",
        "            traces[keep_idx],\n",
        "            correlation_threshold=0.9\n",
        "        )\n",
        "        keep_idx = [keep_idx[i] for i in keep_idx_overlap]\n",
        "    \n",
        "    results['metadata']['n_rois_final'] = len(keep_idx)\n",
        "    results['keep_indices'] = keep_idx\n",
        "    \n",
        "    # === Stage 7: Artifact removal (optional) ===\n",
        "    if config.get('artifact_removal', False) and len(keep_idx) > 10:\n",
        "        # Create spike rasters\n",
        "        n_frames = video.shape[0]\n",
        "        spike_rasters = []\n",
        "        for idx in keep_idx:\n",
        "            raster = np.zeros(n_frames)\n",
        "            raster[spike_frames[idx]] = 1\n",
        "            spike_rasters.append(raster)\n",
        "        \n",
        "        clean_idx, artifact_idx = iterative_umap_dbscan_artifact_removal(spike_rasters, fps)\n",
        "        keep_idx = [keep_idx[i] for i in clean_idx]\n",
        "        results['metadata']['n_rois_after_artifact_removal'] = len(keep_idx)\n",
        "    \n",
        "    # === Stage 8: Subthreshold extraction (optional) ===\n",
        "    if config.get('extract_subthreshold', False):\n",
        "        subthreshold = []\n",
        "        for idx in keep_idx:\n",
        "            sub = extract_subthreshold(traces[idx], spike_frames[idx], fps)\n",
        "            subthreshold.append(sub)\n",
        "        results['subthreshold'] = np.array(subthreshold)\n",
        "    \n",
        "    # Filter results to kept ROIs\n",
        "    results['roi_masks_filtered'] = roi_masks[keep_idx]\n",
        "    results['traces_filtered'] = traces[keep_idx]\n",
        "    results['spike_times_filtered'] = [spike_times[i] for i in keep_idx]\n",
        "    \n",
        "    return results\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6UCqb_okAAh"
      },
      "source": [
        "### Kawashima et al. Pipeline (Voltron in Zebrafish - Raphe Serotonin Study)\n",
        "\n",
        "This pipeline was used for voltage imaging of Voltron in zebrafish raphe neurons. Notable for detailed camera noise correction, local PCA denoising with whiteness stopping criterion, semi-NMF segmentation, and LSTM spike detection.\n",
        "\n",
        "#### Step 1: Camera Noise Correction (Huang/Liu method)\n",
        "```python\n",
        "def camera_noise_correction_kawashima(video, dark_frames, calibration_data=None):\n",
        "    \"\"\"\n",
        "    Camera noise correction from Kawashima et al.\n",
        "    \n",
        "    Corrected pixel: s_i = (s_i^r - o_i) / g_i\n",
        "    \n",
        "    Where:\n",
        "    - s_i^r: raw readout at pixel i\n",
        "    - o_i: camera offset (mean of dark frames)\n",
        "    - g_i: camera gain (fitted from variance vs intensity relationship)\n",
        "    \n",
        "    Args:\n",
        "        video: raw video (n_frames, height, width)\n",
        "        dark_frames: video acquired with no illumination (for offset estimation)\n",
        "        calibration_data: dict with multiple illumination levels for gain estimation\n",
        "            {'level_k': {'mean': D_ik, 'variance': v_ik}, ...}\n",
        "    \n",
        "    Returns:\n",
        "        corrected video\n",
        "    \"\"\"\n",
        "    # Estimate offset (o_i) from dark frames\n",
        "    offset = np.mean(dark_frames, axis=0)  # Mean of ~60k dark images\n",
        "    baseline_variance = np.var(dark_frames, axis=0)  # Variance in dark condition\n",
        "    \n",
        "    # Estimate gain (g_i) from calibration data at multiple illumination levels\n",
        "    if calibration_data is not None:\n",
        "        # Fit gain: minimize sum_k ((v_ik - v_i) - g_i * (D_ik - o_i))^2\n",
        "        # This is linear regression: variance_excess = gain * intensity_excess\n",
        "        \n",
        "        h, w = offset.shape\n",
        "        gain = np.ones((h, w), dtype=np.float32)\n",
        "        \n",
        "        levels = sorted(calibration_data.keys())\n",
        "        for y in range(h):\n",
        "            for x in range(w):\n",
        "                # Collect (intensity, variance) pairs across illumination levels\n",
        "                intensities = []\n",
        "                variances = []\n",
        "                for level in levels:\n",
        "                    D_ik = calibration_data[level]['mean'][y, x]\n",
        "                    v_ik = calibration_data[level]['variance'][y, x]\n",
        "                    \n",
        "                    intensity_excess = D_ik - offset[y, x]\n",
        "                    variance_excess = v_ik - baseline_variance[y, x]\n",
        "                    \n",
        "                    if intensity_excess > 0:\n",
        "                        intensities.append(intensity_excess)\n",
        "                        variances.append(variance_excess)\n",
        "                \n",
        "                if len(intensities) >= 2:\n",
        "                    # Linear regression through origin: variance = gain * intensity\n",
        "                    intensities = np.array(intensities)\n",
        "                    variances = np.array(variances)\n",
        "                    gain[y, x] = np.sum(intensities * variances) / np.sum(intensities ** 2)\n",
        "                    gain[y, x] = np.clip(gain[y, x], 0.1, 10)  # Reasonable bounds\n",
        "    else:\n",
        "        # Without calibration data, assume uniform gain\n",
        "        gain = np.ones_like(offset)\n",
        "    \n",
        "    # Apply correction: s_i = (s_i^r - o_i) / g_i\n",
        "    corrected = (video.astype(np.float32) - offset) / gain\n",
        "    \n",
        "    return corrected, {'offset': offset, 'gain': gain, 'baseline_variance': baseline_variance}\n",
        "\n",
        "def estimate_camera_calibration(calibration_videos):\n",
        "    \"\"\"\n",
        "    Estimate camera calibration from videos at multiple illumination levels.\n",
        "    \n",
        "    Args:\n",
        "        calibration_videos: dict mapping illumination level to video array\n",
        "            e.g., {0: dark_video, 5: video_5mW, 10: video_10mW, 18: video_18mW}\n",
        "    \n",
        "    Returns:\n",
        "        calibration_data for camera_noise_correction_kawashima\n",
        "    \"\"\"\n",
        "    calibration_data = {}\n",
        "    for level, video in calibration_videos.items():\n",
        "        calibration_data[level] = {\n",
        "            'mean': np.mean(video, axis=0),\n",
        "            'variance': np.var(video, axis=0)\n",
        "        }\n",
        "    return calibration_data\n",
        "```\n",
        "\n",
        "#### Step 2: Motion Correction (dipy-based)\n",
        "```python\n",
        "def motion_correct_dipy(video, reference='mean', subpixel=True):\n",
        "    \"\"\"\n",
        "    2D rigid registration using dipy package.\n",
        "    Subpixel-level correction for sequential images.\n",
        "    \n",
        "    Args:\n",
        "        video: motion-corrected video (n_frames, height, width)\n",
        "        reference: 'mean', 'first', or frame index\n",
        "        subpixel: if True, use subpixel registration\n",
        "    \n",
        "    Returns:\n",
        "        corrected video, shifts array\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from dipy.align.imaffine import AffineRegistration, MutualInformationMetric\n",
        "        from dipy.align.transforms import TranslationTransform2D\n",
        "        USE_DIPY = True\n",
        "    except ImportError:\n",
        "        USE_DIPY = False\n",
        "        print(\"dipy not installed, falling back to skimage\")\n",
        "    \n",
        "    n_frames, h, w = video.shape\n",
        "    \n",
        "    # Create reference frame\n",
        "    if reference == 'mean':\n",
        "        ref_frame = np.mean(video, axis=0)\n",
        "    elif reference == 'first':\n",
        "        ref_frame = video[0]\n",
        "    elif isinstance(reference, int):\n",
        "        ref_frame = video[reference]\n",
        "    else:\n",
        "        ref_frame = np.mean(video, axis=0)\n",
        "    \n",
        "    corrected = np.zeros_like(video, dtype=np.float32)\n",
        "    shifts = np.zeros((n_frames, 2))\n",
        "    \n",
        "    if USE_DIPY:\n",
        "        # dipy-based registration\n",
        "        metric = MutualInformationMetric(nbins=32)\n",
        "        affreg = AffineRegistration(metric=metric)\n",
        "        transform = TranslationTransform2D()\n",
        "        \n",
        "        for i in range(n_frames):\n",
        "            frame = video[i].astype(np.float64)\n",
        "            ref = ref_frame.astype(np.float64)\n",
        "            \n",
        "            # Register\n",
        "            affine = affreg.optimize(ref, frame, transform, params0=None)\n",
        "            \n",
        "            # Extract translation\n",
        "            shifts[i] = affine.affine[:2, 2]\n",
        "            \n",
        "            # Apply transformation\n",
        "            from scipy.ndimage import affine_transform\n",
        "            corrected[i] = affine_transform(frame, affine.affine[:2, :2],\n",
        "                                            offset=affine.affine[:2, 2])\n",
        "    else:\n",
        "        # Fallback to skimage phase_cross_correlation\n",
        "        from skimage.registration import phase_cross_correlation\n",
        "        from scipy.ndimage import shift as ndi_shift\n",
        "        \n",
        "        for i in range(n_frames):\n",
        "            frame = video[i].astype(np.float64)\n",
        "            \n",
        "            shift, error, diffphase = phase_cross_correlation(\n",
        "                ref_frame, frame, upsample_factor=10 if subpixel else 1\n",
        "            )\n",
        "            shifts[i] = shift\n",
        "            corrected[i] = ndi_shift(frame, shift, mode='constant', cval=0)\n",
        "    \n",
        "    return corrected, shifts\n",
        "```\n",
        "\n",
        "#### Step 3: Local PCA Denoising (with whiteness criterion)\n",
        "```python\n",
        "def local_pca_denoise_kawashima(video, patch_size=64, overlap=32, max_components=50):\n",
        "    \"\"\"\n",
        "    Local PCA denoising with automatic rank selection.\n",
        "    \n",
        "    Stop adding components when residual is statistically white\n",
        "    (within 99% confidence interval).\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    - Find low-rank representation Y_bar = sum_k u_k * v_k\n",
        "    - Stop when residual R_k = Y - Y_bar is white noise\n",
        "    \n",
        "    Args:\n",
        "        video: (n_frames, height, width) motion-corrected video\n",
        "        patch_size: size of spatial patches\n",
        "        overlap: overlap between patches\n",
        "        max_components: maximum number of PCA components\n",
        "    \n",
        "    Returns:\n",
        "        denoised video\n",
        "    \"\"\"\n",
        "    from scipy.stats import chi2\n",
        "    \n",
        "    n_frames, h, w = video.shape\n",
        "    stride = patch_size - overlap\n",
        "    \n",
        "    # Output arrays\n",
        "    denoised = np.zeros_like(video, dtype=np.float64)\n",
        "    weights = np.zeros((h, w), dtype=np.float64)\n",
        "    \n",
        "    def is_white_noise(residual, confidence=0.99):\n",
        "        \"\"\"\n",
        "        Test if residual is statistically white using Ljung-Box test.\n",
        "        For simplicity, we check if autocorrelation at lag 1 is near zero.\n",
        "        \"\"\"\n",
        "        # Flatten spatial dimensions\n",
        "        n_frames, n_pixels = residual.shape\n",
        "        \n",
        "        # Sample some pixels for efficiency\n",
        "        sample_idx = np.random.choice(n_pixels, min(100, n_pixels), replace=False)\n",
        "        \n",
        "        autocorrs = []\n",
        "        for idx in sample_idx:\n",
        "            trace = residual[:, idx]\n",
        "            if np.std(trace) > 1e-10:\n",
        "                # Lag-1 autocorrelation\n",
        "                autocorr = np.corrcoef(trace[:-1], trace[1:])[0, 1]\n",
        "                autocorrs.append(autocorr)\n",
        "        \n",
        "        if len(autocorrs) == 0:\n",
        "            return True\n",
        "        \n",
        "        # For white noise, autocorrelation should be ~N(0, 1/n_frames)\n",
        "        # Check if mean absolute autocorrelation is small\n",
        "        mean_abs_autocorr = np.mean(np.abs(autocorrs))\n",
        "        threshold = 2.58 / np.sqrt(n_frames)  # 99% CI for autocorr of white noise\n",
        "        \n",
        "        return mean_abs_autocorr < threshold\n",
        "    \n",
        "    # Process each patch\n",
        "    for y_start in range(0, h - patch_size + 1, stride):\n",
        "        for x_start in range(0, w - patch_size + 1, stride):\n",
        "            # Extract patch: (n_frames, patch_size, patch_size)\n",
        "            patch = video[:, y_start:y_start+patch_size, x_start:x_start+patch_size]\n",
        "            patch_flat = patch.reshape(n_frames, -1)  # (n_frames, n_pixels)\n",
        "            \n",
        "            # Center the data\n",
        "            mean_vals = patch_flat.mean(axis=0)\n",
        "            centered = patch_flat - mean_vals\n",
        "            \n",
        "            # Iterative PCA with whiteness stopping criterion\n",
        "            U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
        "            \n",
        "            # Find optimal number of components\n",
        "            best_k = 1\n",
        "            for k in range(1, min(max_components, len(S))):\n",
        "                # Reconstruct with k components\n",
        "                reconstructed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
        "                residual = centered - reconstructed\n",
        "                \n",
        "                if is_white_noise(residual):\n",
        "                    best_k = k\n",
        "                    break\n",
        "                best_k = k\n",
        "            \n",
        "            # Final reconstruction with best_k components\n",
        "            reconstructed = U[:, :best_k] @ np.diag(S[:best_k]) @ Vt[:best_k, :]\n",
        "            reconstructed += mean_vals\n",
        "            reconstructed = reconstructed.reshape(n_frames, patch_size, patch_size)\n",
        "            \n",
        "            # Add to output with overlap weighting\n",
        "            denoised[:, y_start:y_start+patch_size, x_start:x_start+patch_size] += reconstructed\n",
        "            weights[y_start:y_start+patch_size, x_start:x_start+patch_size] += 1\n",
        "    \n",
        "    # Handle edges (patches that don't fit)\n",
        "    # ... (simplified: just use available data)\n",
        "    \n",
        "    # Normalize by overlap count\n",
        "    weights = np.maximum(weights, 1)  # Avoid division by zero\n",
        "    denoised /= weights[np.newaxis, :, :]\n",
        "    \n",
        "    return denoised.astype(np.float32)\n",
        "```\n",
        "\n",
        "#### Step 4: Semi-NMF Cell Segmentation\n",
        "```python\n",
        "def semi_nmf_segmentation_kawashima(video, n_components=100, correlation_threshold=0.8,\n",
        "                                     max_iter=500, min_roi_size=20):\n",
        "    \"\"\"\n",
        "    Semi-nonnegative matrix factorization for cell segmentation.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    minimize ||Y - A*F - B||^2\n",
        "    subject to: A >= 0, B = b * 1^T, b >= 0\n",
        "    \n",
        "    Where:\n",
        "    - A: spatial components (non-negative ROI masks)\n",
        "    - F: temporal components (can be negative - voltage traces)\n",
        "    - B: temporally constant background\n",
        "    \n",
        "    Initialization: super-pixels with local correlation > 0.8\n",
        "    \n",
        "    Args:\n",
        "        video: denoised video (n_frames, height, width)\n",
        "        n_components: number of components to extract\n",
        "        correlation_threshold: for super-pixel initialization\n",
        "        max_iter: maximum iterations for optimization\n",
        "        min_roi_size: minimum ROI size in pixels\n",
        "    \n",
        "    Returns:\n",
        "        roi_masks: (n_rois, height, width) boolean masks\n",
        "        temporal_components: (n_rois, n_frames) fluorescence traces\n",
        "        background: (height, width) static background\n",
        "    \"\"\"\n",
        "    n_frames, h, w = video.shape\n",
        "    n_pixels = h * w\n",
        "    \n",
        "    # Reshape video: Y is (n_pixels, n_frames)\n",
        "    Y = video.reshape(n_frames, n_pixels).T.astype(np.float64)\n",
        "    \n",
        "    # === Step 1: Initialize with super-pixels ===\n",
        "    def compute_local_correlation_map(video, radius=2):\n",
        "        \"\"\"Compute local correlation for each pixel.\"\"\"\n",
        "        corr_map = np.zeros((h, w))\n",
        "        video_norm = (video - video.mean(axis=0)) / (video.std(axis=0) + 1e-10)\n",
        "        \n",
        "        for dy in range(-radius, radius+1):\n",
        "            for dx in range(-radius, radius+1):\n",
        "                if dy == 0 and dx == 0:\n",
        "                    continue\n",
        "                shifted = np.roll(np.roll(video_norm, dy, axis=1), dx, axis=2)\n",
        "                corr = (video_norm * shifted).mean(axis=0)\n",
        "                corr_map += corr\n",
        "        \n",
        "        corr_map /= (2*radius + 1)**2 - 1\n",
        "        return corr_map\n",
        "    \n",
        "    def find_super_pixels(video, corr_threshold=0.8):\n",
        "        \"\"\"Find super-pixels: connected regions with high local correlation.\"\"\"\n",
        "        corr_map = compute_local_correlation_map(video)\n",
        "        binary = corr_map > corr_threshold\n",
        "        \n",
        "        from scipy.ndimage import label\n",
        "        labeled, n_labels = label(binary)\n",
        "        \n",
        "        super_pixels = []\n",
        "        for i in range(1, n_labels + 1):\n",
        "            mask = labeled == i\n",
        "            if mask.sum() >= min_roi_size:\n",
        "                super_pixels.append(mask)\n",
        "        \n",
        "        return super_pixels\n",
        "    \n",
        "    super_pixels = find_super_pixels(video, correlation_threshold)\n",
        "    n_init = min(len(super_pixels), n_components)\n",
        "    \n",
        "    if n_init == 0:\n",
        "        print(\"Warning: No super-pixels found, using random initialization\")\n",
        "        # Random initialization\n",
        "        n_init = n_components\n",
        "        super_pixels = []\n",
        "        for _ in range(n_init):\n",
        "            mask = np.zeros((h, w), dtype=bool)\n",
        "            cy, cx = np.random.randint(10, h-10), np.random.randint(10, w-10)\n",
        "            mask[cy-3:cy+3, cx-3:cx+3] = True\n",
        "            super_pixels.append(mask)\n",
        "    \n",
        "    # Initialize A (spatial components)\n",
        "    A = np.zeros((n_pixels, n_init))\n",
        "    for i, mask in enumerate(super_pixels[:n_init]):\n",
        "        A[:, i] = mask.flatten().astype(np.float64)\n",
        "    \n",
        "    # Initialize background b (mean of low-variance pixels)\n",
        "    pixel_var = Y.var(axis=1)\n",
        "    low_var_mask = pixel_var < np.percentile(pixel_var, 20)\n",
        "    b = Y[low_var_mask].mean(axis=1) if low_var_mask.sum() > 0 else Y.mean(axis=1)\n",
        "    b = np.maximum(b, 0)  # Non-negative background\n",
        "    \n",
        "    # B = b * 1^T\n",
        "    B = np.outer(b, np.ones(n_frames)) if len(b) == n_pixels else np.zeros((n_pixels, n_frames))\n",
        "    \n",
        "    # === Step 2: Alternating optimization ===\n",
        "    for iteration in range(max_iter):\n",
        "        # Update F (temporal components): F = (A^T A)^-1 A^T (Y - B)\n",
        "        Y_bg = Y - B\n",
        "        AtA = A.T @ A + 1e-6 * np.eye(A.shape[1])  # Regularization\n",
        "        AtY = A.T @ Y_bg\n",
        "        F = np.linalg.solve(AtA, AtY)\n",
        "        \n",
        "        # Update A (spatial components): A = (Y - B) F^T (F F^T)^-1, then clip to >= 0\n",
        "        FFt = F @ F.T + 1e-6 * np.eye(F.shape[0])\n",
        "        YFt = Y_bg @ F.T\n",
        "        A = np.linalg.solve(FFt.T, YFt.T).T\n",
        "        A = np.maximum(A, 0)  # Non-negative constraint\n",
        "        \n",
        "        # Update background b: b = mean((Y - A*F), axis=1), clipped to >= 0\n",
        "        residual = Y - A @ F\n",
        "        b = residual.mean(axis=1)\n",
        "        b = np.maximum(b, 0)\n",
        "        B = np.outer(b, np.ones(n_frames))\n",
        "        \n",
        "        # Check convergence\n",
        "        if iteration % 50 == 0:\n",
        "            reconstruction_error = np.linalg.norm(Y - A @ F - B, 'fro')\n",
        "            print(f\"Iteration {iteration}: reconstruction error = {reconstruction_error:.2f}\")\n",
        "    \n",
        "    # === Step 3: Extract ROI masks ===\n",
        "    roi_masks = []\n",
        "    temporal_components = []\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "        spatial = A[:, i].reshape(h, w)\n",
        "        temporal = F[i, :]\n",
        "        \n",
        "        # Threshold spatial component to get mask\n",
        "        thresh = np.percentile(spatial[spatial > 0], 90) if (spatial > 0).sum() > 0 else 0\n",
        "        mask = spatial > thresh\n",
        "        \n",
        "        # Clean up mask\n",
        "        mask = morphology.remove_small_objects(mask, min_size=min_roi_size)\n",
        "        \n",
        "        if mask.sum() >= min_roi_size:\n",
        "            roi_masks.append(mask)\n",
        "            temporal_components.append(temporal)\n",
        "    \n",
        "    return (np.array(roi_masks),\n",
        "            np.array(temporal_components),\n",
        "            b.reshape(h, w))\n",
        "```\n",
        "\n",
        "#### Step 5: LSTM Spike Detection\n",
        "```python\n",
        "def create_lstm_spike_detector(window_size=41, hidden_size=64, dropout=0.3):\n",
        "    \"\"\"\n",
        "    Create LSTM network for spike detection.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    - Two LSTM layers with dropout between them\n",
        "    - Input: time series window (41 frames = 136.67 ms at 300 Hz)\n",
        "    - Output: probability of spike at center of window\n",
        "    - Trained on simultaneous ephys + imaging data\n",
        "    \n",
        "    Args:\n",
        "        window_size: input window size (41 frames in paper)\n",
        "        hidden_size: LSTM hidden layer size\n",
        "        dropout: dropout rate between LSTM layers\n",
        "    \n",
        "    Returns:\n",
        "        PyTorch model (or TensorFlow/Keras equivalent)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        \n",
        "        class LSTMSpikeDetector(nn.Module):\n",
        "            def __init__(self, input_size=1, hidden_size=64, dropout=0.3):\n",
        "                super().__init__()\n",
        "                self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "                self.dropout = nn.Dropout(dropout)\n",
        "                self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "                self.fc = nn.Linear(hidden_size, 1)\n",
        "                self.sigmoid = nn.Sigmoid()\n",
        "            \n",
        "            def forward(self, x):\n",
        "                # x: (batch, window_size, 1)\n",
        "                out, _ = self.lstm1(x)\n",
        "                out = self.dropout(out)\n",
        "                out, _ = self.lstm2(out)\n",
        "                # Take output at last time step\n",
        "                out = self.fc(out[:, -1, :])\n",
        "                return self.sigmoid(out)\n",
        "        \n",
        "        return LSTMSpikeDetector(hidden_size=hidden_size, dropout=dropout)\n",
        "    \n",
        "    except ImportError:\n",
        "        # Keras/TensorFlow fallback\n",
        "        try:\n",
        "            from tensorflow import keras\n",
        "            from tensorflow.keras import layers\n",
        "            \n",
        "            model = keras.Sequential([\n",
        "                layers.LSTM(hidden_size, return_sequences=True,\n",
        "                           input_shape=(window_size, 1)),\n",
        "                layers.Dropout(dropout),\n",
        "                layers.LSTM(hidden_size),\n",
        "                layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "            return model\n",
        "        \n",
        "        except ImportError:\n",
        "            print(\"Neither PyTorch nor TensorFlow available\")\n",
        "            return None\n",
        "\n",
        "def detect_spikes_lstm(trace, model, fps=300, window_size=41, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Detect spikes using trained LSTM model.\n",
        "    \n",
        "    Args:\n",
        "        trace: ΔF/F trace (n_frames,)\n",
        "        model: trained LSTM spike detector\n",
        "        fps: frame rate (300 Hz in Kawashima paper)\n",
        "        window_size: input window size\n",
        "        threshold: probability threshold for spike detection\n",
        "    \n",
        "    Returns:\n",
        "        spike_times: array of spike times in seconds\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    \n",
        "    n_frames = len(trace)\n",
        "    half_window = window_size // 2\n",
        "    \n",
        "    # Normalize trace\n",
        "    trace_norm = (trace - np.mean(trace)) / (np.std(trace) + 1e-10)\n",
        "    \n",
        "    # Slide window across trace\n",
        "    spike_probs = np.zeros(n_frames)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(half_window, n_frames - half_window):\n",
        "            window = trace_norm[i - half_window:i + half_window + 1]\n",
        "            window_tensor = torch.FloatTensor(window).unsqueeze(0).unsqueeze(-1)\n",
        "            \n",
        "            prob = model(window_tensor).item()\n",
        "            spike_probs[i] = prob\n",
        "    \n",
        "    # Find peaks above threshold\n",
        "    from scipy.signal import find_peaks\n",
        "    peaks, _ = find_peaks(spike_probs, height=threshold, distance=int(0.002 * fps))\n",
        "    \n",
        "    spike_times = peaks / fps\n",
        "    return spike_times, spike_probs\n",
        "\n",
        "def train_lstm_spike_detector(traces, ground_truth_spikes, fps=300,\n",
        "                               window_size=41, epochs=100, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train LSTM spike detector on paired imaging + electrophysiology data.\n",
        "    \n",
        "    Args:\n",
        "        traces: list of ΔF/F traces\n",
        "        ground_truth_spikes: list of spike frame indices from electrophysiology\n",
        "        fps: frame rate\n",
        "        window_size: input window size\n",
        "        epochs: training epochs\n",
        "        batch_size: training batch size\n",
        "    \n",
        "    Returns:\n",
        "        trained model\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader, TensorDataset\n",
        "    \n",
        "    # Create training data\n",
        "    X = []\n",
        "    y = []\n",
        "    half_window = window_size // 2\n",
        "    \n",
        "    for trace, spikes in zip(traces, ground_truth_spikes):\n",
        "        trace_norm = (trace - np.mean(trace)) / (np.std(trace) + 1e-10)\n",
        "        spike_set = set(spikes)\n",
        "        \n",
        "        for i in range(half_window, len(trace) - half_window):\n",
        "            window = trace_norm[i - half_window:i + half_window + 1]\n",
        "            label = 1.0 if i in spike_set else 0.0\n",
        "            X.append(window)\n",
        "            y.append(label)\n",
        "    \n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Balance classes (spike vs no-spike)\n",
        "    spike_idx = np.where(y == 1)[0]\n",
        "    no_spike_idx = np.where(y == 0)[0]\n",
        "    n_samples = min(len(spike_idx), len(no_spike_idx))\n",
        "    \n",
        "    balanced_idx = np.concatenate([\n",
        "        spike_idx[:n_samples],\n",
        "        np.random.choice(no_spike_idx, n_samples, replace=False)\n",
        "    ])\n",
        "    np.random.shuffle(balanced_idx)\n",
        "    \n",
        "    X = X[balanced_idx]\n",
        "    y = y[balanced_idx]\n",
        "    \n",
        "    # Create model and train\n",
        "    model = create_lstm_spike_detector(window_size=window_size)\n",
        "    \n",
        "    X_tensor = torch.FloatTensor(X).unsqueeze(-1)\n",
        "    y_tensor = torch.FloatTensor(y).unsqueeze(-1)\n",
        "    \n",
        "    dataset = TensorDataset(X_tensor, y_tensor)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_X)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: loss = {total_loss/len(loader):.4f}\")\n",
        "    \n",
        "    return model\n",
        "```\n",
        "\n",
        "#### Step 6: Subthreshold Activity Estimation\n",
        "```python\n",
        "def extract_subthreshold_kawashima(trace, spike_frames, fps=300,\n",
        "                                    median_window_ms=70, clip_window_frames=2):\n",
        "    \"\"\"\n",
        "    Estimate subthreshold activity using rolling median.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    - Rolling median filter with 70 ms window\n",
        "    - Clip frames around spikes (-1 to +1 frames) to avoid spike nonlinearity\n",
        "    \n",
        "    Args:\n",
        "        trace: ΔF/F trace\n",
        "        spike_frames: detected spike frame indices\n",
        "        fps: frame rate\n",
        "        median_window_ms: median filter window (70 ms in paper)\n",
        "        clip_window_frames: frames to clip around spikes (1 in paper)\n",
        "    \n",
        "    Returns:\n",
        "        subthreshold: subthreshold membrane potential estimate\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import median_filter\n",
        "    \n",
        "    n_frames = len(trace)\n",
        "    \n",
        "    # Create mask of frames to exclude (around spikes)\n",
        "    valid_mask = np.ones(n_frames, dtype=bool)\n",
        "    for spike_frame in spike_frames:\n",
        "        start = max(0, spike_frame - clip_window_frames)\n",
        "        end = min(n_frames, spike_frame + clip_window_frames + 1)\n",
        "        valid_mask[start:end] = False\n",
        "    \n",
        "    # Interpolate over clipped regions\n",
        "    trace_interpolated = trace.copy()\n",
        "    if not valid_mask.all():\n",
        "        valid_indices = np.where(valid_mask)[0]\n",
        "        invalid_indices = np.where(~valid_mask)[0]\n",
        "        \n",
        "        if len(valid_indices) > 2:\n",
        "            from scipy.interpolate import interp1d\n",
        "            f = interp1d(valid_indices, trace[valid_indices],\n",
        "                        kind='linear', bounds_error=False,\n",
        "                        fill_value='extrapolate')\n",
        "            trace_interpolated[invalid_indices] = f(invalid_indices)\n",
        "    \n",
        "    # Apply rolling median filter\n",
        "    window_samples = int(median_window_ms * fps / 1000)\n",
        "    window_samples = window_samples if window_samples % 2 == 1 else window_samples + 1\n",
        "    \n",
        "    subthreshold = median_filter(trace_interpolated, size=window_samples)\n",
        "    \n",
        "    return subthreshold\n",
        "```\n",
        "\n",
        "#### ΔF/F Computation (Kawashima method)\n",
        "```python\n",
        "def compute_dff_kawashima(trace, fps=300, baseline_percentile=20,\n",
        "                           baseline_window_sec=180):\n",
        "    \"\"\"\n",
        "    Compute ΔF/F using running percentile baseline.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    ΔF/F = (F - F0) / F0\n",
        "    where F0 is running 20th percentile within 3-minute window\n",
        "    \n",
        "    For Voltron (negative indicator): use -ΔF/F for analysis\n",
        "    \n",
        "    Args:\n",
        "        trace: raw fluorescence trace\n",
        "        fps: frame rate\n",
        "        baseline_percentile: percentile for baseline (20 in paper)\n",
        "        baseline_window_sec: window size in seconds (180 = 3 minutes)\n",
        "    \n",
        "    Returns:\n",
        "        dff: ΔF/F trace\n",
        "        dff_inverted: -ΔF/F (for Voltron analysis)\n",
        "    \"\"\"\n",
        "    from scipy.ndimage import percentile_filter\n",
        "    \n",
        "    window_samples = int(baseline_window_sec * fps)\n",
        "    \n",
        "    # Running percentile baseline\n",
        "    F0 = percentile_filter(trace, baseline_percentile, size=window_samples)\n",
        "    \n",
        "    # Compute ΔF/F\n",
        "    dff = (trace - F0) / (F0 + 1e-10)\n",
        "    \n",
        "    # Inverted for Voltron (negative indicator)\n",
        "    dff_inverted = -dff\n",
        "    \n",
        "    return dff, dff_inverted\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4LpF8enkAAi"
      },
      "source": [
        "#### Behavioral Kernel Fitting (GLM for spike prediction)\n",
        "```python\n",
        "def fit_spike_glm_kernels(spike_times, behavior_data, fps=300,\n",
        "                          history_sec=1.0, l2_reg=0.01):\n",
        "    \"\"\"\n",
        "    Fit GLM kernels to predict spikes from behavioral variables.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    P(spike at time t) = Binomial(w_s^T * S_t + w_v^T * V_t - w_sp^T * SP_t)\n",
        "    \n",
        "    Where:\n",
        "    - S_t: swim vigor history\n",
        "    - V_t: visual input history  \n",
        "    - SP_t: recent spike history\n",
        "    - w_s, w_v, w_sp: learned kernels\n",
        "    \n",
        "    Args:\n",
        "        spike_times: array of spike times in seconds\n",
        "        behavior_data: dict with 'swim_vigor', 'visual_input' arrays\n",
        "        fps: frame rate\n",
        "        history_sec: history window for kernels (1 second in paper)\n",
        "        l2_reg: L2 regularization strength\n",
        "    \n",
        "    Returns:\n",
        "        kernels: dict with fitted w_s, w_v, w_sp\n",
        "        model: fitted sklearn LogisticRegression\n",
        "        performance: explained variance on validation set\n",
        "    \"\"\"\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    \n",
        "    n_history = int(history_sec * fps)  # 300 time points for 1 sec at 300 Hz\n",
        "    \n",
        "    swim_vigor = behavior_data.get('swim_vigor', np.zeros(10000))\n",
        "    visual_input = behavior_data.get('visual_input', np.zeros(10000))\n",
        "    n_frames = len(swim_vigor)\n",
        "    \n",
        "    # Create spike raster\n",
        "    spike_raster = np.zeros(n_frames)\n",
        "    spike_frames = (np.array(spike_times) * fps).astype(int)\n",
        "    spike_frames = spike_frames[(spike_frames >= 0) & (spike_frames < n_frames)]\n",
        "    spike_raster[spike_frames] = 1\n",
        "    \n",
        "    # Build design matrix\n",
        "    # Use sqrt of swim vigor (as in paper)\n",
        "    swim_vigor_sqrt = np.sqrt(np.abs(swim_vigor)) * np.sign(swim_vigor)\n",
        "    \n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for t in range(n_history, n_frames):\n",
        "        # Swim vigor history\n",
        "        s_t = swim_vigor_sqrt[t-n_history:t]\n",
        "        # Visual input history\n",
        "        v_t = visual_input[t-n_history:t]\n",
        "        # Spike history\n",
        "        sp_t = spike_raster[t-n_history:t]\n",
        "        \n",
        "        features = np.concatenate([s_t, v_t, sp_t])\n",
        "        X.append(features)\n",
        "        y.append(spike_raster[t])\n",
        "    \n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Balance classes (sample around spike events)\n",
        "    spike_idx = np.where(y == 1)[0]\n",
        "    no_spike_idx = np.where(y == 0)[0]\n",
        "    \n",
        "    # Sample to balance ~50/50\n",
        "    n_samples = min(len(spike_idx) * 2, len(no_spike_idx))\n",
        "    balanced_no_spike = np.random.choice(no_spike_idx, n_samples, replace=False)\n",
        "    balanced_idx = np.concatenate([spike_idx, balanced_no_spike])\n",
        "    np.random.shuffle(balanced_idx)\n",
        "    \n",
        "    X_balanced = X[balanced_idx]\n",
        "    y_balanced = y[balanced_idx]\n",
        "    \n",
        "    # Fit logistic regression with L2 regularization\n",
        "    model = LogisticRegression(penalty='l2', C=1/l2_reg, max_iter=1000)\n",
        "    model.fit(X_balanced, y_balanced)\n",
        "    \n",
        "    # Extract kernels\n",
        "    coef = model.coef_[0]\n",
        "    kernels = {\n",
        "        'swim_vigor': coef[:n_history],\n",
        "        'visual_input': coef[n_history:2*n_history],\n",
        "        'spike_history': coef[2*n_history:]\n",
        "    }\n",
        "    \n",
        "    # Cross-validation performance\n",
        "    cv_scores = cross_val_score(model, X_balanced, y_balanced, cv=5, scoring='roc_auc')\n",
        "    \n",
        "    return kernels, model, {'cv_auc': cv_scores.mean(), 'cv_std': cv_scores.std()}\n",
        "\n",
        "def plot_behavioral_kernels(kernels, fps=300):\n",
        "    \"\"\"Plot fitted behavioral kernels.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
        "    \n",
        "    time_ms = np.arange(len(kernels['swim_vigor'])) * 1000 / fps - len(kernels['swim_vigor']) * 1000 / fps\n",
        "    \n",
        "    axes[0].plot(time_ms, kernels['swim_vigor'])\n",
        "    axes[0].set_xlabel('Time before spike (ms)')\n",
        "    axes[0].set_ylabel('Weight')\n",
        "    axes[0].set_title('Swim Vigor Kernel')\n",
        "    axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    axes[1].plot(time_ms, kernels['visual_input'])\n",
        "    axes[1].set_xlabel('Time before spike (ms)')\n",
        "    axes[1].set_title('Visual Input Kernel')\n",
        "    axes[1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    axes[2].plot(time_ms, -kernels['spike_history'])  # Negative because it's subtracted\n",
        "    axes[2].set_xlabel('Time before spike (ms)')\n",
        "    axes[2].set_title('Spike History Kernel (refractory)')\n",
        "    axes[2].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "```\n",
        "\n",
        "#### Population Coding Analysis (sparse LDA)\n",
        "```python\n",
        "def compute_population_coding(neural_activity, conditions, l2_gamma=0.5):\n",
        "    \"\"\"\n",
        "    Compute population coding direction using sparse linear discriminant analysis.\n",
        "    \n",
        "    From Kawashima et al.:\n",
        "    l = argmin_l -(l^T (r_high - r_low))^2 / (l^T Σ_r l)\n",
        "    \n",
        "    With regularized covariance: Σ_r = (1-γ) * cov(r) + γ * I\n",
        "    \n",
        "    Args:\n",
        "        neural_activity: (n_neurons, n_timepoints) firing rates\n",
        "        conditions: array of condition labels (e.g., 'high' or 'low')\n",
        "        l2_gamma: regularization parameter [0, 1]\n",
        "    \n",
        "    Returns:\n",
        "        coding_direction: (n_neurons,) vector\n",
        "        explained_variance: fraction of variance explained\n",
        "    \"\"\"\n",
        "    unique_conditions = np.unique(conditions)\n",
        "    if len(unique_conditions) != 2:\n",
        "        raise ValueError(\"Exactly 2 conditions required\")\n",
        "    \n",
        "    cond1, cond2 = unique_conditions\n",
        "    \n",
        "    # Mean activity per condition\n",
        "    r_cond1 = neural_activity[:, conditions == cond1].mean(axis=1)\n",
        "    r_cond2 = neural_activity[:, conditions == cond2].mean(axis=1)\n",
        "    \n",
        "    # Regularized covariance\n",
        "    r_centered = neural_activity - neural_activity.mean(axis=1, keepdims=True)\n",
        "    cov_r = np.cov(r_centered)\n",
        "    n_neurons = cov_r.shape[0]\n",
        "    \n",
        "    sigma_r = (1 - l2_gamma) * cov_r + l2_gamma * np.eye(n_neurons)\n",
        "    \n",
        "    # Solve for coding direction (Fisher LDA)\n",
        "    # l = Σ_r^-1 (r_high - r_low)\n",
        "    diff = r_cond2 - r_cond1\n",
        "    coding_direction = np.linalg.solve(sigma_r, diff)\n",
        "    \n",
        "    # Normalize\n",
        "    coding_direction /= np.linalg.norm(coding_direction)\n",
        "    \n",
        "    # Compute explained variance\n",
        "    projected = coding_direction @ neural_activity\n",
        "    proj_cond1 = projected[conditions == cond1]\n",
        "    proj_cond2 = projected[conditions == cond2]\n",
        "    \n",
        "    between_var = (proj_cond1.mean() - proj_cond2.mean())**2\n",
        "    within_var = proj_cond1.var() + proj_cond2.var()\n",
        "    explained_variance = between_var / (between_var + within_var + 1e-10)\n",
        "    \n",
        "    return coding_direction, explained_variance\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agSuNAvTkAAi"
      },
      "source": [
        "---\n",
        "# 10. Running the Benchmark\n",
        "\n",
        "```python\n",
        "def run_benchmark(pipeline_fn, data_paths, verbose=True):\n",
        "    \"\"\"\n",
        "    Run benchmark on multiple datasets.\n",
        "    \n",
        "    Args:\n",
        "        pipeline_fn: function(video, fps) -> results dict\n",
        "        data_paths: list of paths to benchmark data\n",
        "    \n",
        "    Returns:\n",
        "        mean_score: float\n",
        "        all_scores: list of (dataset, score, details) tuples\n",
        "    \"\"\"\n",
        "    all_scores = []\n",
        "    \n",
        "    for path in data_paths:\n",
        "        if verbose:\n",
        "            print(f\"\\nProcessing: {path}\")\n",
        "        \n",
        "        # Load data\n",
        "        video, fps, ground_truth = load_benchmark_data(path)\n",
        "        \n",
        "        # Run pipeline\n",
        "        results = pipeline_fn(video, fps)\n",
        "        \n",
        "        # Evaluate\n",
        "        score, details = compute_benchmark_score(\n",
        "            results, video, fps, ground_truth\n",
        "        )\n",
        "        \n",
        "        all_scores.append((path, score, details))\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"  Score: {score:.4f}\")\n",
        "            for k, v in details.items():\n",
        "                print(f\"    {k}: {v:.4f}\")\n",
        "    \n",
        "    mean_score = np.mean([s[1] for s in all_scores])\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n=== FINAL SCORE: {mean_score:.4f} ===\")\n",
        "    \n",
        "    return mean_score, all_scores\n",
        "\n",
        "# Example usage:\n",
        "# score, details = run_benchmark(baseline_pipeline, ['data/fish1', 'data/fish2'])\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}